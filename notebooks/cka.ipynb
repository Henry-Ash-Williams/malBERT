{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8b5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "N_SAMPLES = 5\n",
    "FORWARD_SIZE = 256\n",
    "\n",
    "models = {\n",
    "    \"base\": {\n",
    "        \"base\": \"e96b8h5a\",\n",
    "        \"pretrained\": \"e1tosi4k\",\n",
    "        \"frozen\": \"t4o7wvla\",\n",
    "    },\n",
    "    \"ob\": {\n",
    "        \"base\": \"swcod025\",\n",
    "        \"pretrained\": \"xzlusbyu\",\n",
    "        \"frozen\": \"aqzn99cx\",\n",
    "    },\n",
    "    \"pob\": {\n",
    "        \"base\": \"wozkyaa6\",\n",
    "        \"pretrained\": \"daf0h543\",\n",
    "        \"frozen\": \"47y92682\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d58ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook(name, base):\n",
    "    def base_hook(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            base_activations[name].append(output)\n",
    "\n",
    "    def pretrained_hook(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            pretrained_activations[name].append(output)\n",
    "\n",
    "    if base:\n",
    "        return base_hook\n",
    "    else:\n",
    "        return pretrained_hook\n",
    "\n",
    "\n",
    "for (name_b, module_b), (name_p, module_p) in zip(\n",
    "    base_model.named_modules(), pretrained_model.named_modules()\n",
    "):\n",
    "    module_b.register_forward_hook(get_hook(name_b, base=True))\n",
    "    module_p.register_forward_hook(get_hook(name_p, base=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8648f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_CKA(X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n",
    "    def center_gram(K: torch.Tensor) -> torch.Tensor:\n",
    "        n = K.size(0)\n",
    "        H = torch.eye(n, device=K.device) - torch.ones(n, n, device=K.device) / n\n",
    "        return H @ K @ H\n",
    "\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    Y = Y - Y.mean(dim=0, keepdim=True)\n",
    "\n",
    "    K = X @ X.T\n",
    "    L = Y @ Y.T\n",
    "\n",
    "    Kc = center_gram(K)\n",
    "    Lc = center_gram(L)\n",
    "\n",
    "    hsic = (Kc * Lc).sum()\n",
    "    norm_x = (Kc * Kc).sum().sqrt()\n",
    "    norm_y = (Lc * Lc).sum().sqrt()\n",
    "\n",
    "    return hsic / (norm_x * norm_y + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26506c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5 Computing RSA w/ CKA: 100%|██████████| 11664/11664 [00:20<00:00, 575.36it/s]\n",
      "2/5 Computing RSA w/ CKA: 100%|██████████| 11664/11664 [00:24<00:00, 485.34it/s]\n",
      "3/5 Computing RSA w/ CKA: 100%|██████████| 11664/11664 [00:20<00:00, 561.29it/s]\n",
      "4/5 Computing RSA w/ CKA: 100%|██████████| 11664/11664 [00:21<00:00, 544.89it/s]\n",
      "5/5 Computing RSA w/ CKA: 100%|█████████▉| 11657/11664 [00:20<00:00, 705.13it/s]"
     ]
    }
   ],
   "source": [
    "sims = []\n",
    "\n",
    "base_activations = defaultdict(list)\n",
    "pretrained_activations = defaultdict(list)\n",
    "\n",
    "for a, sample in enumerate(dataset[\"test\"].select(range(N_SAMPLES))):\n",
    "    torch.mps.empty_cache()\n",
    "    input = tokenize(base_model, sample, tokenizer)\n",
    "\n",
    "    pretrained_model(\n",
    "        input_ids=input[\"input_ids\"][:FORWARD_SIZE].to(\"mps\"),\n",
    "        attention_mask=input[\"attention_mask\"][:FORWARD_SIZE].to(\"mps\"),\n",
    "    )\n",
    "\n",
    "    base_model(\n",
    "        input_ids=input[\"input_ids\"][:FORWARD_SIZE].to(\"mps\"),\n",
    "        attention_mask=input[\"attention_mask\"][:FORWARD_SIZE].to(\"mps\"),\n",
    "    )\n",
    "\n",
    "    sim_matrix = torch.zeros((len(pretrained_activations), len(pretrained_activations)))\n",
    "    loop = tqdm(\n",
    "        total=len(pretrained_activations) ** 2,\n",
    "        desc=f\"{a + 1}/{N_SAMPLES} Computing RSA w/ CKA\",\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    for i, (k_b, v_b) in enumerate(base_activations.items()):\n",
    "        base_act = v_b[-1].reshape(v_b[-1].shape[0], -1)\n",
    "\n",
    "        for j, (k_p, v_p) in enumerate(pretrained_activations.items()):\n",
    "            pretrained_act = v_p[-1].reshape(v_p[-1].shape[0], -1)\n",
    "            sim_matrix[i, j] = linear_CKA(base_act.detach(), pretrained_act.detach())\n",
    "            loop.update(1)\n",
    "\n",
    "    sims.append(sim_matrix)\n",
    "\n",
    "sim_matrix = torch.stack(sims).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65ee03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "customdata": [
          [
           "CKA: 0.923<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings.word_embeddings<br>(0, 0)",
           "CKA: -1.17e-14<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(0, 1)",
           "CKA: 0.00389<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings.position_embeddings<br>(0, 2)",
           "CKA: 0.94<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings.LayerNorm<br>(0, 3)",
           "CKA: 0.94<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings.dropout<br>(0, 4)",
           "CKA: 0.94<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.embeddings<br>(0, 5)",
           "CKA: 0.617<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(0, 6)",
           "CKA: 0.743<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(0, 7)",
           "CKA: 0.868<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(0, 8)",
           "CKA: 0.31<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(0, 9)",
           "CKA: 0.31<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(0, 10)",
           "CKA: 0.947<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(0, 11)",
           "CKA: 0.947<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(0, 12)",
           "CKA: 0.918<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(0, 13)",
           "CKA: 0.933<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(0, 14)",
           "CKA: 0.933<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(0, 15)",
           "CKA: 0.858<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(0, 16)",
           "CKA: 0.858<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(0, 17)",
           "CKA: 0.958<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(0, 18)",
           "CKA: 0.958<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.0.output<br>(0, 19)",
           "CKA: 0.834<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(0, 20)",
           "CKA: 0.758<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(0, 21)",
           "CKA: 0.852<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(0, 22)",
           "CKA: 0.384<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(0, 23)",
           "CKA: 0.384<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(0, 24)",
           "CKA: 0.932<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(0, 25)",
           "CKA: 0.932<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(0, 26)",
           "CKA: 0.926<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(0, 27)",
           "CKA: 0.943<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(0, 28)",
           "CKA: 0.943<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(0, 29)",
           "CKA: 0.88<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(0, 30)",
           "CKA: 0.88<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(0, 31)",
           "CKA: 0.931<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(0, 32)",
           "CKA: 0.931<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.1.output<br>(0, 33)",
           "CKA: 0.773<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(0, 34)",
           "CKA: 0.848<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(0, 35)",
           "CKA: 0.857<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(0, 36)",
           "CKA: 0.381<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(0, 37)",
           "CKA: 0.381<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(0, 38)",
           "CKA: 0.879<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(0, 39)",
           "CKA: 0.879<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(0, 40)",
           "CKA: 0.88<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(0, 41)",
           "CKA: 0.908<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(0, 42)",
           "CKA: 0.908<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(0, 43)",
           "CKA: 0.87<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(0, 44)",
           "CKA: 0.87<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(0, 45)",
           "CKA: 0.886<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(0, 46)",
           "CKA: 0.886<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.2.output<br>(0, 47)",
           "CKA: 0.717<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(0, 48)",
           "CKA: 0.775<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(0, 49)",
           "CKA: 0.839<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(0, 50)",
           "CKA: 0.396<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(0, 51)",
           "CKA: 0.396<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(0, 52)",
           "CKA: 0.804<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(0, 53)",
           "CKA: 0.804<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(0, 54)",
           "CKA: 0.733<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(0, 55)",
           "CKA: 0.763<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(0, 56)",
           "CKA: 0.763<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(0, 57)",
           "CKA: 0.748<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(0, 58)",
           "CKA: 0.748<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(0, 59)",
           "CKA: 0.8<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(0, 60)",
           "CKA: 0.8<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.3.output<br>(0, 61)",
           "CKA: 0.525<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(0, 62)",
           "CKA: 0.644<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(0, 63)",
           "CKA: 0.768<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(0, 64)",
           "CKA: 0.283<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(0, 65)",
           "CKA: 0.283<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(0, 66)",
           "CKA: 0.692<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(0, 67)",
           "CKA: 0.692<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(0, 68)",
           "CKA: 0.57<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(0, 69)",
           "CKA: 0.57<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(0, 70)",
           "CKA: 0.57<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(0, 71)",
           "CKA: 0.548<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(0, 72)",
           "CKA: 0.548<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(0, 73)",
           "CKA: 0.642<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(0, 74)",
           "CKA: 0.642<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.4.output<br>(0, 75)",
           "CKA: 0.426<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(0, 76)",
           "CKA: 0.521<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(0, 77)",
           "CKA: 0.508<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(0, 78)",
           "CKA: 0.249<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(0, 79)",
           "CKA: 0.249<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(0, 80)",
           "CKA: 0.51<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(0, 81)",
           "CKA: 0.51<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(0, 82)",
           "CKA: 0.392<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(0, 83)",
           "CKA: 0.377<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(0, 84)",
           "CKA: 0.377<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(0, 85)",
           "CKA: 0.303<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(0, 86)",
           "CKA: 0.303<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(0, 87)",
           "CKA: 0.459<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(0, 88)",
           "CKA: 0.459<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.5.output<br>(0, 89)",
           "CKA: 0.266<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(0, 90)",
           "CKA: 0.4<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(0, 91)",
           "CKA: 0.288<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(0, 92)",
           "CKA: 0.151<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(0, 93)",
           "CKA: 0.151<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(0, 94)",
           "CKA: 0.32<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(0, 95)",
           "CKA: 0.32<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(0, 96)",
           "CKA: 0.213<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(0, 97)",
           "CKA: 0.228<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(0, 98)",
           "CKA: 0.228<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(0, 99)",
           "CKA: 0.132<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(0, 100)",
           "CKA: 0.132<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(0, 101)",
           "CKA: 0.214<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(0, 102)",
           "CKA: 0.214<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: roberta.encoder.layer.6.output<br>(0, 103)",
           "CKA: 0.108<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: classifier.dropout<br>(0, 104)",
           "CKA: 0.111<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: classifier.dense<br>(0, 105)",
           "CKA: 0.077<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: classifier.out_proj<br>(0, 106)",
           "CKA: 0.077<br>Base: roberta.embeddings.word_embeddings<br>Pretrained: classifier<br>(0, 107)"
          ],
          [
           "CKA: -1.24e-13<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings.word_embeddings<br>(1, 0)",
           "CKA: 0.0<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(1, 1)",
           "CKA: -2.16e-17<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings.position_embeddings<br>(1, 2)",
           "CKA: -9.76e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings.LayerNorm<br>(1, 3)",
           "CKA: -9.76e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings.dropout<br>(1, 4)",
           "CKA: -9.76e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.embeddings<br>(1, 5)",
           "CKA: -1.03e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(1, 6)",
           "CKA: -1.29e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(1, 7)",
           "CKA: -2.72e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(1, 8)",
           "CKA: -1.54e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(1, 9)",
           "CKA: -1.54e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(1, 10)",
           "CKA: -4.41e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(1, 11)",
           "CKA: -4.41e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(1, 12)",
           "CKA: -3.73e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(1, 13)",
           "CKA: -1.13e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(1, 14)",
           "CKA: -1.13e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(1, 15)",
           "CKA: -9.38e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(1, 16)",
           "CKA: -9.38e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(1, 17)",
           "CKA: -4.64e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(1, 18)",
           "CKA: -4.64e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.0.output<br>(1, 19)",
           "CKA: -1.61e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(1, 20)",
           "CKA: -5.04e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(1, 21)",
           "CKA: -1.4e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(1, 22)",
           "CKA: -1.24e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(1, 23)",
           "CKA: -1.24e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(1, 24)",
           "CKA: -5.7e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(1, 25)",
           "CKA: -5.7e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(1, 26)",
           "CKA: -4.05e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(1, 27)",
           "CKA: -1.26e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(1, 28)",
           "CKA: -1.26e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(1, 29)",
           "CKA: -9.52e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(1, 30)",
           "CKA: -9.52e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(1, 31)",
           "CKA: -6.68e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(1, 32)",
           "CKA: -6.68e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.1.output<br>(1, 33)",
           "CKA: -2.78e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(1, 34)",
           "CKA: -3.34e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(1, 35)",
           "CKA: -2.34e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(1, 36)",
           "CKA: -6.88e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(1, 37)",
           "CKA: -6.88e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(1, 38)",
           "CKA: -7.43e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(1, 39)",
           "CKA: -7.43e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(1, 40)",
           "CKA: -5.82e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(1, 41)",
           "CKA: -1.82e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(1, 42)",
           "CKA: -1.82e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(1, 43)",
           "CKA: -9.84e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(1, 44)",
           "CKA: -9.84e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(1, 45)",
           "CKA: -7.9e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(1, 46)",
           "CKA: -7.9e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.2.output<br>(1, 47)",
           "CKA: -3.53e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(1, 48)",
           "CKA: -1.39e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(1, 49)",
           "CKA: -2.61e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(1, 50)",
           "CKA: -1.47e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(1, 51)",
           "CKA: -1.47e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(1, 52)",
           "CKA: -8.61e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(1, 53)",
           "CKA: -8.61e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(1, 54)",
           "CKA: -6.7e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(1, 55)",
           "CKA: -2.13e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(1, 56)",
           "CKA: -2.13e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(1, 57)",
           "CKA: -9.92e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(1, 58)",
           "CKA: -9.92e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(1, 59)",
           "CKA: -9.36e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(1, 60)",
           "CKA: -9.36e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.3.output<br>(1, 61)",
           "CKA: -4.74e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(1, 62)",
           "CKA: -3.48e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(1, 63)",
           "CKA: -2.76e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(1, 64)",
           "CKA: -7.41e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(1, 65)",
           "CKA: -7.41e-12<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(1, 66)",
           "CKA: -9.82e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(1, 67)",
           "CKA: -9.82e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(1, 68)",
           "CKA: -9.77e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(1, 69)",
           "CKA: -3.21e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(1, 70)",
           "CKA: -3.21e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(1, 71)",
           "CKA: -1.27e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(1, 72)",
           "CKA: -1.27e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(1, 73)",
           "CKA: -9.31e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(1, 74)",
           "CKA: -9.31e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.4.output<br>(1, 75)",
           "CKA: -2.16e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(1, 76)",
           "CKA: -2.26e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(1, 77)",
           "CKA: -2.69e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(1, 78)",
           "CKA: -1.6e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(1, 79)",
           "CKA: -1.6e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(1, 80)",
           "CKA: -1.08e-10<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(1, 81)",
           "CKA: -1.08e-10<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(1, 82)",
           "CKA: -1.1e-10<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(1, 83)",
           "CKA: -4.12e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(1, 84)",
           "CKA: -4.12e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(1, 85)",
           "CKA: -1.64e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(1, 86)",
           "CKA: -1.64e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(1, 87)",
           "CKA: -9.18e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(1, 88)",
           "CKA: -9.18e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.5.output<br>(1, 89)",
           "CKA: -2.37e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(1, 90)",
           "CKA: -2.02e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(1, 91)",
           "CKA: -3.51e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(1, 92)",
           "CKA: -2.58e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(1, 93)",
           "CKA: -2.58e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(1, 94)",
           "CKA: -7.24e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(1, 95)",
           "CKA: -7.24e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(1, 96)",
           "CKA: -1.46e-10<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(1, 97)",
           "CKA: -4.38e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(1, 98)",
           "CKA: -4.38e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(1, 99)",
           "CKA: -3.69e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(1, 100)",
           "CKA: -3.69e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(1, 101)",
           "CKA: -3e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(1, 102)",
           "CKA: -3e-11<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: roberta.encoder.layer.6.output<br>(1, 103)",
           "CKA: -1.27e-13<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: classifier.dropout<br>(1, 104)",
           "CKA: -1.51e-13<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: classifier.dense<br>(1, 105)",
           "CKA: -1.93e-14<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: classifier.out_proj<br>(1, 106)",
           "CKA: -1.93e-14<br>Base: roberta.embeddings.token_type_embeddings<br>Pretrained: classifier<br>(1, 107)"
          ],
          [
           "CKA: 0.0878<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings.word_embeddings<br>(2, 0)",
           "CKA: 6.22e-16<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(2, 1)",
           "CKA: 0.2<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings.position_embeddings<br>(2, 2)",
           "CKA: 0.0535<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings.LayerNorm<br>(2, 3)",
           "CKA: 0.0535<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings.dropout<br>(2, 4)",
           "CKA: 0.0535<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.embeddings<br>(2, 5)",
           "CKA: 0.119<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(2, 6)",
           "CKA: 0.0477<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(2, 7)",
           "CKA: 0.0434<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(2, 8)",
           "CKA: 0.0228<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(2, 9)",
           "CKA: 0.0228<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(2, 10)",
           "CKA: 0.0271<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(2, 11)",
           "CKA: 0.0271<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(2, 12)",
           "CKA: 0.0643<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(2, 13)",
           "CKA: 0.0921<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(2, 14)",
           "CKA: 0.0921<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(2, 15)",
           "CKA: 0.139<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(2, 16)",
           "CKA: 0.139<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(2, 17)",
           "CKA: 0.02<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(2, 18)",
           "CKA: 0.02<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.0.output<br>(2, 19)",
           "CKA: 0.0383<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(2, 20)",
           "CKA: 0.0238<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(2, 21)",
           "CKA: 0.0109<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(2, 22)",
           "CKA: 0.0103<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(2, 23)",
           "CKA: 0.0103<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(2, 24)",
           "CKA: 0.0179<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(2, 25)",
           "CKA: 0.0179<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(2, 26)",
           "CKA: 0.0252<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(2, 27)",
           "CKA: 0.0182<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(2, 28)",
           "CKA: 0.0182<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(2, 29)",
           "CKA: 0.0326<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(2, 30)",
           "CKA: 0.0326<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(2, 31)",
           "CKA: 0.0143<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(2, 32)",
           "CKA: 0.0143<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.1.output<br>(2, 33)",
           "CKA: 0.0224<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(2, 34)",
           "CKA: 0.0168<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(2, 35)",
           "CKA: 0.00857<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(2, 36)",
           "CKA: 0.0152<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(2, 37)",
           "CKA: 0.0152<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(2, 38)",
           "CKA: 0.0136<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(2, 39)",
           "CKA: 0.0136<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(2, 40)",
           "CKA: 0.0116<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(2, 41)",
           "CKA: 0.00722<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(2, 42)",
           "CKA: 0.00722<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(2, 43)",
           "CKA: 0.00671<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(2, 44)",
           "CKA: 0.00671<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(2, 45)",
           "CKA: 0.0114<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(2, 46)",
           "CKA: 0.0114<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.2.output<br>(2, 47)",
           "CKA: 0.00351<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(2, 48)",
           "CKA: 0.0161<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(2, 49)",
           "CKA: 0.004<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(2, 50)",
           "CKA: 0.00818<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(2, 51)",
           "CKA: 0.00818<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(2, 52)",
           "CKA: 0.0124<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(2, 53)",
           "CKA: 0.0124<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(2, 54)",
           "CKA: 0.00976<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(2, 55)",
           "CKA: 0.0067<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(2, 56)",
           "CKA: 0.0067<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(2, 57)",
           "CKA: 0.00561<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(2, 58)",
           "CKA: 0.00561<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(2, 59)",
           "CKA: 0.0113<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(2, 60)",
           "CKA: 0.0113<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.3.output<br>(2, 61)",
           "CKA: 0.00682<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(2, 62)",
           "CKA: 0.00779<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(2, 63)",
           "CKA: 0.0132<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(2, 64)",
           "CKA: 0.018<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(2, 65)",
           "CKA: 0.018<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(2, 66)",
           "CKA: 0.0117<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(2, 67)",
           "CKA: 0.0117<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(2, 68)",
           "CKA: 0.0114<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(2, 69)",
           "CKA: 0.00824<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(2, 70)",
           "CKA: 0.00824<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(2, 71)",
           "CKA: 0.00716<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(2, 72)",
           "CKA: 0.00716<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(2, 73)",
           "CKA: 0.0114<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(2, 74)",
           "CKA: 0.0114<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.4.output<br>(2, 75)",
           "CKA: 0.0163<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(2, 76)",
           "CKA: 0.0073<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(2, 77)",
           "CKA: 0.0112<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(2, 78)",
           "CKA: 0.012<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(2, 79)",
           "CKA: 0.012<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(2, 80)",
           "CKA: 0.0123<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(2, 81)",
           "CKA: 0.0123<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(2, 82)",
           "CKA: 0.0179<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(2, 83)",
           "CKA: 0.0154<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(2, 84)",
           "CKA: 0.0154<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(2, 85)",
           "CKA: 0.0177<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(2, 86)",
           "CKA: 0.0177<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(2, 87)",
           "CKA: 0.0158<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(2, 88)",
           "CKA: 0.0158<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.5.output<br>(2, 89)",
           "CKA: 0.0132<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(2, 90)",
           "CKA: 0.0144<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(2, 91)",
           "CKA: 0.0097<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(2, 92)",
           "CKA: 0.00529<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(2, 93)",
           "CKA: 0.00529<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(2, 94)",
           "CKA: 0.0144<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(2, 95)",
           "CKA: 0.0144<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(2, 96)",
           "CKA: 0.0137<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(2, 97)",
           "CKA: 0.012<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(2, 98)",
           "CKA: 0.012<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(2, 99)",
           "CKA: 0.00545<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(2, 100)",
           "CKA: 0.00545<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(2, 101)",
           "CKA: 0.008<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(2, 102)",
           "CKA: 0.008<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: roberta.encoder.layer.6.output<br>(2, 103)",
           "CKA: 0.00232<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: classifier.dropout<br>(2, 104)",
           "CKA: 0.00213<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: classifier.dense<br>(2, 105)",
           "CKA: 0.00069<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: classifier.out_proj<br>(2, 106)",
           "CKA: 0.00069<br>Base: roberta.embeddings.position_embeddings<br>Pretrained: classifier<br>(2, 107)"
          ],
          [
           "CKA: 0.925<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(3, 0)",
           "CKA: -1.01e-12<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(3, 1)",
           "CKA: 0.0103<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(3, 2)",
           "CKA: 0.941<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(3, 3)",
           "CKA: 0.941<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(3, 4)",
           "CKA: 0.941<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.embeddings<br>(3, 5)",
           "CKA: 0.62<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(3, 6)",
           "CKA: 0.744<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(3, 7)",
           "CKA: 0.868<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(3, 8)",
           "CKA: 0.311<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(3, 9)",
           "CKA: 0.311<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(3, 10)",
           "CKA: 0.948<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(3, 11)",
           "CKA: 0.948<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(3, 12)",
           "CKA: 0.92<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(3, 13)",
           "CKA: 0.936<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(3, 14)",
           "CKA: 0.936<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(3, 15)",
           "CKA: 0.863<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(3, 16)",
           "CKA: 0.863<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(3, 17)",
           "CKA: 0.958<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(3, 18)",
           "CKA: 0.958<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(3, 19)",
           "CKA: 0.836<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(3, 20)",
           "CKA: 0.759<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(3, 21)",
           "CKA: 0.853<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(3, 22)",
           "CKA: 0.384<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(3, 23)",
           "CKA: 0.384<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(3, 24)",
           "CKA: 0.932<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(3, 25)",
           "CKA: 0.932<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(3, 26)",
           "CKA: 0.927<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(3, 27)",
           "CKA: 0.943<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(3, 28)",
           "CKA: 0.943<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(3, 29)",
           "CKA: 0.882<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(3, 30)",
           "CKA: 0.882<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(3, 31)",
           "CKA: 0.931<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(3, 32)",
           "CKA: 0.931<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(3, 33)",
           "CKA: 0.774<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(3, 34)",
           "CKA: 0.848<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(3, 35)",
           "CKA: 0.857<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(3, 36)",
           "CKA: 0.381<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(3, 37)",
           "CKA: 0.381<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(3, 38)",
           "CKA: 0.879<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(3, 39)",
           "CKA: 0.879<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(3, 40)",
           "CKA: 0.881<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(3, 41)",
           "CKA: 0.908<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(3, 42)",
           "CKA: 0.908<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(3, 43)",
           "CKA: 0.871<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(3, 44)",
           "CKA: 0.871<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(3, 45)",
           "CKA: 0.886<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(3, 46)",
           "CKA: 0.886<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(3, 47)",
           "CKA: 0.717<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(3, 48)",
           "CKA: 0.775<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(3, 49)",
           "CKA: 0.84<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(3, 50)",
           "CKA: 0.396<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(3, 51)",
           "CKA: 0.396<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(3, 52)",
           "CKA: 0.804<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(3, 53)",
           "CKA: 0.804<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(3, 54)",
           "CKA: 0.733<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(3, 55)",
           "CKA: 0.763<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(3, 56)",
           "CKA: 0.763<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(3, 57)",
           "CKA: 0.748<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(3, 58)",
           "CKA: 0.748<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(3, 59)",
           "CKA: 0.8<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(3, 60)",
           "CKA: 0.8<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(3, 61)",
           "CKA: 0.525<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(3, 62)",
           "CKA: 0.644<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(3, 63)",
           "CKA: 0.768<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(3, 64)",
           "CKA: 0.283<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(3, 65)",
           "CKA: 0.283<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(3, 66)",
           "CKA: 0.692<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(3, 67)",
           "CKA: 0.692<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(3, 68)",
           "CKA: 0.57<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(3, 69)",
           "CKA: 0.57<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(3, 70)",
           "CKA: 0.57<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(3, 71)",
           "CKA: 0.548<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(3, 72)",
           "CKA: 0.548<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(3, 73)",
           "CKA: 0.642<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(3, 74)",
           "CKA: 0.642<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(3, 75)",
           "CKA: 0.427<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(3, 76)",
           "CKA: 0.521<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(3, 77)",
           "CKA: 0.508<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(3, 78)",
           "CKA: 0.249<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(3, 79)",
           "CKA: 0.249<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(3, 80)",
           "CKA: 0.51<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(3, 81)",
           "CKA: 0.51<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(3, 82)",
           "CKA: 0.392<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(3, 83)",
           "CKA: 0.377<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(3, 84)",
           "CKA: 0.377<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(3, 85)",
           "CKA: 0.304<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(3, 86)",
           "CKA: 0.304<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(3, 87)",
           "CKA: 0.459<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(3, 88)",
           "CKA: 0.459<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(3, 89)",
           "CKA: 0.266<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(3, 90)",
           "CKA: 0.401<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(3, 91)",
           "CKA: 0.289<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(3, 92)",
           "CKA: 0.151<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(3, 93)",
           "CKA: 0.151<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(3, 94)",
           "CKA: 0.32<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(3, 95)",
           "CKA: 0.32<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(3, 96)",
           "CKA: 0.214<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(3, 97)",
           "CKA: 0.228<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(3, 98)",
           "CKA: 0.228<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(3, 99)",
           "CKA: 0.132<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(3, 100)",
           "CKA: 0.132<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(3, 101)",
           "CKA: 0.215<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(3, 102)",
           "CKA: 0.215<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(3, 103)",
           "CKA: 0.108<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: classifier.dropout<br>(3, 104)",
           "CKA: 0.111<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: classifier.dense<br>(3, 105)",
           "CKA: 0.0771<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: classifier.out_proj<br>(3, 106)",
           "CKA: 0.0771<br>Base: roberta.embeddings.LayerNorm<br>Pretrained: classifier<br>(3, 107)"
          ],
          [
           "CKA: 0.925<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(4, 0)",
           "CKA: -1.01e-12<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(4, 1)",
           "CKA: 0.0103<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(4, 2)",
           "CKA: 0.941<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(4, 3)",
           "CKA: 0.941<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings.dropout<br>(4, 4)",
           "CKA: 0.941<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.embeddings<br>(4, 5)",
           "CKA: 0.62<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(4, 6)",
           "CKA: 0.744<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(4, 7)",
           "CKA: 0.868<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(4, 8)",
           "CKA: 0.311<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(4, 9)",
           "CKA: 0.311<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(4, 10)",
           "CKA: 0.948<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(4, 11)",
           "CKA: 0.948<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(4, 12)",
           "CKA: 0.92<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(4, 13)",
           "CKA: 0.936<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(4, 14)",
           "CKA: 0.936<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(4, 15)",
           "CKA: 0.863<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(4, 16)",
           "CKA: 0.863<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(4, 17)",
           "CKA: 0.958<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(4, 18)",
           "CKA: 0.958<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(4, 19)",
           "CKA: 0.836<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(4, 20)",
           "CKA: 0.759<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(4, 21)",
           "CKA: 0.853<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(4, 22)",
           "CKA: 0.384<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(4, 23)",
           "CKA: 0.384<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(4, 24)",
           "CKA: 0.932<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(4, 25)",
           "CKA: 0.932<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(4, 26)",
           "CKA: 0.927<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(4, 27)",
           "CKA: 0.943<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(4, 28)",
           "CKA: 0.943<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(4, 29)",
           "CKA: 0.882<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(4, 30)",
           "CKA: 0.882<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(4, 31)",
           "CKA: 0.931<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(4, 32)",
           "CKA: 0.931<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(4, 33)",
           "CKA: 0.774<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(4, 34)",
           "CKA: 0.848<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(4, 35)",
           "CKA: 0.857<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(4, 36)",
           "CKA: 0.381<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(4, 37)",
           "CKA: 0.381<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(4, 38)",
           "CKA: 0.879<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(4, 39)",
           "CKA: 0.879<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(4, 40)",
           "CKA: 0.881<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(4, 41)",
           "CKA: 0.908<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(4, 42)",
           "CKA: 0.908<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(4, 43)",
           "CKA: 0.871<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(4, 44)",
           "CKA: 0.871<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(4, 45)",
           "CKA: 0.886<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(4, 46)",
           "CKA: 0.886<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(4, 47)",
           "CKA: 0.717<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(4, 48)",
           "CKA: 0.775<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(4, 49)",
           "CKA: 0.84<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(4, 50)",
           "CKA: 0.396<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(4, 51)",
           "CKA: 0.396<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(4, 52)",
           "CKA: 0.804<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(4, 53)",
           "CKA: 0.804<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(4, 54)",
           "CKA: 0.733<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(4, 55)",
           "CKA: 0.763<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(4, 56)",
           "CKA: 0.763<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(4, 57)",
           "CKA: 0.748<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(4, 58)",
           "CKA: 0.748<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(4, 59)",
           "CKA: 0.8<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(4, 60)",
           "CKA: 0.8<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(4, 61)",
           "CKA: 0.525<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(4, 62)",
           "CKA: 0.644<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(4, 63)",
           "CKA: 0.768<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(4, 64)",
           "CKA: 0.283<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(4, 65)",
           "CKA: 0.283<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(4, 66)",
           "CKA: 0.692<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(4, 67)",
           "CKA: 0.692<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(4, 68)",
           "CKA: 0.57<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(4, 69)",
           "CKA: 0.57<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(4, 70)",
           "CKA: 0.57<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(4, 71)",
           "CKA: 0.548<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(4, 72)",
           "CKA: 0.548<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(4, 73)",
           "CKA: 0.642<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(4, 74)",
           "CKA: 0.642<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(4, 75)",
           "CKA: 0.427<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(4, 76)",
           "CKA: 0.521<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(4, 77)",
           "CKA: 0.508<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(4, 78)",
           "CKA: 0.249<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(4, 79)",
           "CKA: 0.249<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(4, 80)",
           "CKA: 0.51<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(4, 81)",
           "CKA: 0.51<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(4, 82)",
           "CKA: 0.392<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(4, 83)",
           "CKA: 0.377<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(4, 84)",
           "CKA: 0.377<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(4, 85)",
           "CKA: 0.304<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(4, 86)",
           "CKA: 0.304<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(4, 87)",
           "CKA: 0.459<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(4, 88)",
           "CKA: 0.459<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(4, 89)",
           "CKA: 0.266<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(4, 90)",
           "CKA: 0.401<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(4, 91)",
           "CKA: 0.289<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(4, 92)",
           "CKA: 0.151<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(4, 93)",
           "CKA: 0.151<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(4, 94)",
           "CKA: 0.32<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(4, 95)",
           "CKA: 0.32<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(4, 96)",
           "CKA: 0.214<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(4, 97)",
           "CKA: 0.228<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(4, 98)",
           "CKA: 0.228<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(4, 99)",
           "CKA: 0.132<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(4, 100)",
           "CKA: 0.132<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(4, 101)",
           "CKA: 0.215<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(4, 102)",
           "CKA: 0.215<br>Base: roberta.embeddings.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(4, 103)",
           "CKA: 0.108<br>Base: roberta.embeddings.dropout<br>Pretrained: classifier.dropout<br>(4, 104)",
           "CKA: 0.111<br>Base: roberta.embeddings.dropout<br>Pretrained: classifier.dense<br>(4, 105)",
           "CKA: 0.0771<br>Base: roberta.embeddings.dropout<br>Pretrained: classifier.out_proj<br>(4, 106)",
           "CKA: 0.0771<br>Base: roberta.embeddings.dropout<br>Pretrained: classifier<br>(4, 107)"
          ],
          [
           "CKA: 0.925<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings.word_embeddings<br>(5, 0)",
           "CKA: -1.01e-12<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(5, 1)",
           "CKA: 0.0103<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings.position_embeddings<br>(5, 2)",
           "CKA: 0.941<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings.LayerNorm<br>(5, 3)",
           "CKA: 0.941<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings.dropout<br>(5, 4)",
           "CKA: 0.941<br>Base: roberta.embeddings<br>Pretrained: roberta.embeddings<br>(5, 5)",
           "CKA: 0.62<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(5, 6)",
           "CKA: 0.744<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(5, 7)",
           "CKA: 0.868<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(5, 8)",
           "CKA: 0.311<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(5, 9)",
           "CKA: 0.311<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(5, 10)",
           "CKA: 0.948<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(5, 11)",
           "CKA: 0.948<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(5, 12)",
           "CKA: 0.92<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(5, 13)",
           "CKA: 0.936<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(5, 14)",
           "CKA: 0.936<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(5, 15)",
           "CKA: 0.863<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(5, 16)",
           "CKA: 0.863<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(5, 17)",
           "CKA: 0.958<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(5, 18)",
           "CKA: 0.958<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.0.output<br>(5, 19)",
           "CKA: 0.836<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(5, 20)",
           "CKA: 0.759<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(5, 21)",
           "CKA: 0.853<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(5, 22)",
           "CKA: 0.384<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(5, 23)",
           "CKA: 0.384<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(5, 24)",
           "CKA: 0.932<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(5, 25)",
           "CKA: 0.932<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(5, 26)",
           "CKA: 0.927<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(5, 27)",
           "CKA: 0.943<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(5, 28)",
           "CKA: 0.943<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(5, 29)",
           "CKA: 0.882<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(5, 30)",
           "CKA: 0.882<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(5, 31)",
           "CKA: 0.931<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(5, 32)",
           "CKA: 0.931<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.1.output<br>(5, 33)",
           "CKA: 0.774<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(5, 34)",
           "CKA: 0.848<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(5, 35)",
           "CKA: 0.857<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(5, 36)",
           "CKA: 0.381<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(5, 37)",
           "CKA: 0.381<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(5, 38)",
           "CKA: 0.879<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(5, 39)",
           "CKA: 0.879<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(5, 40)",
           "CKA: 0.881<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(5, 41)",
           "CKA: 0.908<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(5, 42)",
           "CKA: 0.908<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(5, 43)",
           "CKA: 0.871<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(5, 44)",
           "CKA: 0.871<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(5, 45)",
           "CKA: 0.886<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(5, 46)",
           "CKA: 0.886<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.2.output<br>(5, 47)",
           "CKA: 0.717<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(5, 48)",
           "CKA: 0.775<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(5, 49)",
           "CKA: 0.84<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(5, 50)",
           "CKA: 0.396<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(5, 51)",
           "CKA: 0.396<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(5, 52)",
           "CKA: 0.804<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(5, 53)",
           "CKA: 0.804<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(5, 54)",
           "CKA: 0.733<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(5, 55)",
           "CKA: 0.763<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(5, 56)",
           "CKA: 0.763<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(5, 57)",
           "CKA: 0.748<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(5, 58)",
           "CKA: 0.748<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(5, 59)",
           "CKA: 0.8<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(5, 60)",
           "CKA: 0.8<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.3.output<br>(5, 61)",
           "CKA: 0.525<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(5, 62)",
           "CKA: 0.644<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(5, 63)",
           "CKA: 0.768<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(5, 64)",
           "CKA: 0.283<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(5, 65)",
           "CKA: 0.283<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(5, 66)",
           "CKA: 0.692<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(5, 67)",
           "CKA: 0.692<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(5, 68)",
           "CKA: 0.57<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(5, 69)",
           "CKA: 0.57<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(5, 70)",
           "CKA: 0.57<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(5, 71)",
           "CKA: 0.548<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(5, 72)",
           "CKA: 0.548<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(5, 73)",
           "CKA: 0.642<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(5, 74)",
           "CKA: 0.642<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.4.output<br>(5, 75)",
           "CKA: 0.427<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(5, 76)",
           "CKA: 0.521<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(5, 77)",
           "CKA: 0.508<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(5, 78)",
           "CKA: 0.249<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(5, 79)",
           "CKA: 0.249<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(5, 80)",
           "CKA: 0.51<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(5, 81)",
           "CKA: 0.51<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(5, 82)",
           "CKA: 0.392<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(5, 83)",
           "CKA: 0.377<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(5, 84)",
           "CKA: 0.377<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(5, 85)",
           "CKA: 0.304<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(5, 86)",
           "CKA: 0.304<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(5, 87)",
           "CKA: 0.459<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(5, 88)",
           "CKA: 0.459<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.5.output<br>(5, 89)",
           "CKA: 0.266<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(5, 90)",
           "CKA: 0.401<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(5, 91)",
           "CKA: 0.289<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(5, 92)",
           "CKA: 0.151<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(5, 93)",
           "CKA: 0.151<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(5, 94)",
           "CKA: 0.32<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(5, 95)",
           "CKA: 0.32<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(5, 96)",
           "CKA: 0.214<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(5, 97)",
           "CKA: 0.228<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(5, 98)",
           "CKA: 0.228<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(5, 99)",
           "CKA: 0.132<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(5, 100)",
           "CKA: 0.132<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(5, 101)",
           "CKA: 0.215<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(5, 102)",
           "CKA: 0.215<br>Base: roberta.embeddings<br>Pretrained: roberta.encoder.layer.6.output<br>(5, 103)",
           "CKA: 0.108<br>Base: roberta.embeddings<br>Pretrained: classifier.dropout<br>(5, 104)",
           "CKA: 0.111<br>Base: roberta.embeddings<br>Pretrained: classifier.dense<br>(5, 105)",
           "CKA: 0.0771<br>Base: roberta.embeddings<br>Pretrained: classifier.out_proj<br>(5, 106)",
           "CKA: 0.0771<br>Base: roberta.embeddings<br>Pretrained: classifier<br>(5, 107)"
          ],
          [
           "CKA: 0.635<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(6, 0)",
           "CKA: -3.22e-13<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(6, 1)",
           "CKA: 0.0118<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(6, 2)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(6, 3)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(6, 4)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.embeddings<br>(6, 5)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(6, 6)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(6, 7)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(6, 8)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(6, 9)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(6, 10)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(6, 11)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(6, 12)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(6, 13)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(6, 14)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(6, 15)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(6, 16)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(6, 17)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(6, 18)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(6, 19)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(6, 20)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(6, 21)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(6, 22)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(6, 23)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(6, 24)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(6, 25)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(6, 26)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(6, 27)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(6, 28)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(6, 29)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(6, 30)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(6, 31)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(6, 32)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(6, 33)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(6, 34)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(6, 35)",
           "CKA: 0.624<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(6, 36)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(6, 37)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(6, 38)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(6, 39)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(6, 40)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(6, 41)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(6, 42)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(6, 43)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(6, 44)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(6, 45)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(6, 46)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(6, 47)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(6, 48)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(6, 49)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(6, 50)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(6, 51)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(6, 52)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(6, 53)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(6, 54)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(6, 55)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(6, 56)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(6, 57)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(6, 58)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(6, 59)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(6, 60)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(6, 61)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(6, 62)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(6, 63)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(6, 64)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(6, 65)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(6, 66)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(6, 67)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(6, 68)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(6, 69)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(6, 70)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(6, 71)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(6, 72)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(6, 73)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(6, 74)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(6, 75)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(6, 76)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(6, 77)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(6, 78)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(6, 79)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(6, 80)",
           "CKA: 0.361<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(6, 81)",
           "CKA: 0.361<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(6, 82)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(6, 83)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(6, 84)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(6, 85)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(6, 86)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(6, 87)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(6, 88)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(6, 89)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(6, 90)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(6, 91)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(6, 92)",
           "CKA: 0.103<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(6, 93)",
           "CKA: 0.103<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(6, 94)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(6, 95)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(6, 96)",
           "CKA: 0.138<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(6, 97)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(6, 98)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(6, 99)",
           "CKA: 0.083<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(6, 100)",
           "CKA: 0.083<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(6, 101)",
           "CKA: 0.141<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(6, 102)",
           "CKA: 0.141<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(6, 103)",
           "CKA: 0.0734<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: classifier.dropout<br>(6, 104)",
           "CKA: 0.0763<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: classifier.dense<br>(6, 105)",
           "CKA: 0.0521<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: classifier.out_proj<br>(6, 106)",
           "CKA: 0.0521<br>Base: roberta.encoder.layer.0.attention.self.query<br>Pretrained: classifier<br>(6, 107)"
          ],
          [
           "CKA: 0.699<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(7, 0)",
           "CKA: -1.83e-13<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(7, 1)",
           "CKA: 0.00538<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(7, 2)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(7, 3)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(7, 4)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.embeddings<br>(7, 5)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(7, 6)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(7, 7)",
           "CKA: 0.662<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(7, 8)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(7, 9)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(7, 10)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(7, 11)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(7, 12)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(7, 13)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(7, 14)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(7, 15)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(7, 16)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(7, 17)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(7, 18)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(7, 19)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(7, 20)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(7, 21)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(7, 22)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(7, 23)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(7, 24)",
           "CKA: 0.642<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(7, 25)",
           "CKA: 0.642<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(7, 26)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(7, 27)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(7, 28)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(7, 29)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(7, 30)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(7, 31)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(7, 32)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(7, 33)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(7, 34)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(7, 35)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(7, 36)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(7, 37)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(7, 38)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(7, 39)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(7, 40)",
           "CKA: 0.591<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(7, 41)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(7, 42)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(7, 43)",
           "CKA: 0.542<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(7, 44)",
           "CKA: 0.542<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(7, 45)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(7, 46)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(7, 47)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(7, 48)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(7, 49)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(7, 50)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(7, 51)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(7, 52)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(7, 53)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(7, 54)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(7, 55)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(7, 56)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(7, 57)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(7, 58)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(7, 59)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(7, 60)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(7, 61)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(7, 62)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(7, 63)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(7, 64)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(7, 65)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(7, 66)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(7, 67)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(7, 68)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(7, 69)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(7, 70)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(7, 71)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(7, 72)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(7, 73)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(7, 74)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(7, 75)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(7, 76)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(7, 77)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(7, 78)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(7, 79)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(7, 80)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(7, 81)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(7, 82)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(7, 83)",
           "CKA: 0.262<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(7, 84)",
           "CKA: 0.262<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(7, 85)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(7, 86)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(7, 87)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(7, 88)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(7, 89)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(7, 90)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(7, 91)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(7, 92)",
           "CKA: 0.102<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(7, 93)",
           "CKA: 0.102<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(7, 94)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(7, 95)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(7, 96)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(7, 97)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(7, 98)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(7, 99)",
           "CKA: 0.0841<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(7, 100)",
           "CKA: 0.0841<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(7, 101)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(7, 102)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(7, 103)",
           "CKA: 0.0677<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: classifier.dropout<br>(7, 104)",
           "CKA: 0.0703<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: classifier.dense<br>(7, 105)",
           "CKA: 0.0478<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: classifier.out_proj<br>(7, 106)",
           "CKA: 0.0478<br>Base: roberta.encoder.layer.0.attention.self.key<br>Pretrained: classifier<br>(7, 107)"
          ],
          [
           "CKA: 0.86<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(8, 0)",
           "CKA: -1.22e-13<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(8, 1)",
           "CKA: 0.00713<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(8, 2)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(8, 3)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(8, 4)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.embeddings<br>(8, 5)",
           "CKA: 0.567<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(8, 6)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(8, 7)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(8, 8)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(8, 9)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(8, 10)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(8, 11)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(8, 12)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(8, 13)",
           "CKA: 0.883<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(8, 14)",
           "CKA: 0.883<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(8, 15)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(8, 16)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(8, 17)",
           "CKA: 0.9<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(8, 18)",
           "CKA: 0.9<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(8, 19)",
           "CKA: 0.804<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(8, 20)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(8, 21)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(8, 22)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(8, 23)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(8, 24)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(8, 25)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(8, 26)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(8, 27)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(8, 28)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(8, 29)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(8, 30)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(8, 31)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(8, 32)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(8, 33)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(8, 34)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(8, 35)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(8, 36)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(8, 37)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(8, 38)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(8, 39)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(8, 40)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(8, 41)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(8, 42)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(8, 43)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(8, 44)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(8, 45)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(8, 46)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(8, 47)",
           "CKA: 0.688<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(8, 48)",
           "CKA: 0.758<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(8, 49)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(8, 50)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(8, 51)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(8, 52)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(8, 53)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(8, 54)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(8, 55)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(8, 56)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(8, 57)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(8, 58)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(8, 59)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(8, 60)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(8, 61)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(8, 62)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(8, 63)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(8, 64)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(8, 65)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(8, 66)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(8, 67)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(8, 68)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(8, 69)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(8, 70)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(8, 71)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(8, 72)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(8, 73)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(8, 74)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(8, 75)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(8, 76)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(8, 77)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(8, 78)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(8, 79)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(8, 80)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(8, 81)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(8, 82)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(8, 83)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(8, 84)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(8, 85)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(8, 86)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(8, 87)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(8, 88)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(8, 89)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(8, 90)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(8, 91)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(8, 92)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(8, 93)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(8, 94)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(8, 95)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(8, 96)",
           "CKA: 0.202<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(8, 97)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(8, 98)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(8, 99)",
           "CKA: 0.123<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(8, 100)",
           "CKA: 0.123<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(8, 101)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(8, 102)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(8, 103)",
           "CKA: 0.103<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: classifier.dropout<br>(8, 104)",
           "CKA: 0.106<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: classifier.dense<br>(8, 105)",
           "CKA: 0.0743<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: classifier.out_proj<br>(8, 106)",
           "CKA: 0.0743<br>Base: roberta.encoder.layer.0.attention.self.value<br>Pretrained: classifier<br>(8, 107)"
          ],
          [
           "CKA: 0.396<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(9, 0)",
           "CKA: -2.45e-14<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(9, 1)",
           "CKA: 0.00884<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(9, 2)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(9, 3)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(9, 4)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.embeddings<br>(9, 5)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(9, 6)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(9, 7)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(9, 8)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(9, 9)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(9, 10)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(9, 11)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(9, 12)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(9, 13)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(9, 14)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(9, 15)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(9, 16)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(9, 17)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(9, 18)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(9, 19)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(9, 20)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(9, 21)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(9, 22)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(9, 23)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(9, 24)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(9, 25)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(9, 26)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(9, 27)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(9, 28)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(9, 29)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(9, 30)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(9, 31)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(9, 32)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(9, 33)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(9, 34)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(9, 35)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(9, 36)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(9, 37)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(9, 38)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(9, 39)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(9, 40)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(9, 41)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(9, 42)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(9, 43)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(9, 44)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(9, 45)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(9, 46)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(9, 47)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(9, 48)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(9, 49)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(9, 50)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(9, 51)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(9, 52)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(9, 53)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(9, 54)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(9, 55)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(9, 56)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(9, 57)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(9, 58)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(9, 59)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(9, 60)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(9, 61)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(9, 62)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(9, 63)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(9, 64)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(9, 65)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(9, 66)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(9, 67)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(9, 68)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(9, 69)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(9, 70)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(9, 71)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(9, 72)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(9, 73)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(9, 74)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(9, 75)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(9, 76)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(9, 77)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(9, 78)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(9, 79)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(9, 80)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(9, 81)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(9, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(9, 83)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(9, 84)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(9, 85)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(9, 86)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(9, 87)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(9, 88)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(9, 89)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(9, 90)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(9, 91)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(9, 92)",
           "CKA: 0.181<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(9, 93)",
           "CKA: 0.181<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(9, 94)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(9, 95)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(9, 96)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(9, 97)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(9, 98)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(9, 99)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(9, 100)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(9, 101)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(9, 102)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(9, 103)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: classifier.dropout<br>(9, 104)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: classifier.dense<br>(9, 105)",
           "CKA: 0.0763<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: classifier.out_proj<br>(9, 106)",
           "CKA: 0.0763<br>Base: roberta.encoder.layer.0.attention.output.dense<br>Pretrained: classifier<br>(9, 107)"
          ],
          [
           "CKA: 0.396<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(10, 0)",
           "CKA: -2.45e-14<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(10, 1)",
           "CKA: 0.00884<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(10, 2)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(10, 3)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(10, 4)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(10, 5)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(10, 6)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(10, 7)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(10, 8)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(10, 9)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(10, 10)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(10, 11)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(10, 12)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(10, 13)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(10, 14)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(10, 15)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(10, 16)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(10, 17)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(10, 18)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(10, 19)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(10, 20)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(10, 21)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(10, 22)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(10, 23)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(10, 24)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(10, 25)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(10, 26)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(10, 27)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(10, 28)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(10, 29)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(10, 30)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(10, 31)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(10, 32)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(10, 33)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(10, 34)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(10, 35)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(10, 36)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(10, 37)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(10, 38)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(10, 39)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(10, 40)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(10, 41)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(10, 42)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(10, 43)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(10, 44)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(10, 45)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(10, 46)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(10, 47)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(10, 48)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(10, 49)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(10, 50)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(10, 51)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(10, 52)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(10, 53)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(10, 54)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(10, 55)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(10, 56)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(10, 57)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(10, 58)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(10, 59)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(10, 60)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(10, 61)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(10, 62)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(10, 63)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(10, 64)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(10, 65)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(10, 66)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(10, 67)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(10, 68)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(10, 69)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(10, 70)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(10, 71)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(10, 72)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(10, 73)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(10, 74)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(10, 75)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(10, 76)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(10, 77)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(10, 78)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(10, 79)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(10, 80)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(10, 81)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(10, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(10, 83)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(10, 84)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(10, 85)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(10, 86)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(10, 87)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(10, 88)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(10, 89)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(10, 90)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(10, 91)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(10, 92)",
           "CKA: 0.181<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(10, 93)",
           "CKA: 0.181<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(10, 94)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(10, 95)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(10, 96)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(10, 97)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(10, 98)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(10, 99)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(10, 100)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(10, 101)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(10, 102)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(10, 103)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: classifier.dropout<br>(10, 104)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: classifier.dense<br>(10, 105)",
           "CKA: 0.0763<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(10, 106)",
           "CKA: 0.0763<br>Base: roberta.encoder.layer.0.attention.output.dropout<br>Pretrained: classifier<br>(10, 107)"
          ],
          [
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(11, 0)",
           "CKA: -1.17e-11<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(11, 1)",
           "CKA: 0.015<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(11, 2)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(11, 3)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(11, 4)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(11, 5)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(11, 6)",
           "CKA: 0.751<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(11, 7)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(11, 8)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(11, 9)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(11, 10)",
           "CKA: 0.947<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(11, 11)",
           "CKA: 0.947<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(11, 12)",
           "CKA: 0.92<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(11, 13)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(11, 14)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(11, 15)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(11, 16)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(11, 17)",
           "CKA: 0.958<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(11, 18)",
           "CKA: 0.958<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(11, 19)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(11, 20)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(11, 21)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(11, 22)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(11, 23)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(11, 24)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(11, 25)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(11, 26)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(11, 27)",
           "CKA: 0.946<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(11, 28)",
           "CKA: 0.946<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(11, 29)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(11, 30)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(11, 31)",
           "CKA: 0.932<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(11, 32)",
           "CKA: 0.932<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(11, 33)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(11, 34)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(11, 35)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(11, 36)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(11, 37)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(11, 38)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(11, 39)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(11, 40)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(11, 41)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(11, 42)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(11, 43)",
           "CKA: 0.876<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(11, 44)",
           "CKA: 0.876<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(11, 45)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(11, 46)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(11, 47)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(11, 48)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(11, 49)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(11, 50)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(11, 51)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(11, 52)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(11, 53)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(11, 54)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(11, 55)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(11, 56)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(11, 57)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(11, 58)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(11, 59)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(11, 60)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(11, 61)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(11, 62)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(11, 63)",
           "CKA: 0.77<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(11, 64)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(11, 65)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(11, 66)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(11, 67)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(11, 68)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(11, 69)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(11, 70)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(11, 71)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(11, 72)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(11, 73)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(11, 74)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(11, 75)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(11, 76)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(11, 77)",
           "CKA: 0.508<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(11, 78)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(11, 79)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(11, 80)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(11, 81)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(11, 82)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(11, 83)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(11, 84)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(11, 85)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(11, 86)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(11, 87)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(11, 88)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(11, 89)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(11, 90)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(11, 91)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(11, 92)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(11, 93)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(11, 94)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(11, 95)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(11, 96)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(11, 97)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(11, 98)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(11, 99)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(11, 100)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(11, 101)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(11, 102)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(11, 103)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(11, 104)",
           "CKA: 0.112<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(11, 105)",
           "CKA: 0.0787<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(11, 106)",
           "CKA: 0.0787<br>Base: roberta.encoder.layer.0.attention.output.LayerNorm<br>Pretrained: classifier<br>(11, 107)"
          ],
          [
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(12, 0)",
           "CKA: -1.17e-11<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(12, 1)",
           "CKA: 0.015<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(12, 2)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(12, 3)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(12, 4)",
           "CKA: 0.94<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.embeddings<br>(12, 5)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(12, 6)",
           "CKA: 0.751<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(12, 7)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(12, 8)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(12, 9)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(12, 10)",
           "CKA: 0.947<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(12, 11)",
           "CKA: 0.947<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(12, 12)",
           "CKA: 0.92<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(12, 13)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(12, 14)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(12, 15)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(12, 16)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(12, 17)",
           "CKA: 0.958<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(12, 18)",
           "CKA: 0.958<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(12, 19)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(12, 20)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(12, 21)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(12, 22)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(12, 23)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(12, 24)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(12, 25)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(12, 26)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(12, 27)",
           "CKA: 0.946<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(12, 28)",
           "CKA: 0.946<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(12, 29)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(12, 30)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(12, 31)",
           "CKA: 0.932<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(12, 32)",
           "CKA: 0.932<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(12, 33)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(12, 34)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(12, 35)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(12, 36)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(12, 37)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(12, 38)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(12, 39)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(12, 40)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(12, 41)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(12, 42)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(12, 43)",
           "CKA: 0.876<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(12, 44)",
           "CKA: 0.876<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(12, 45)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(12, 46)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(12, 47)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(12, 48)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(12, 49)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(12, 50)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(12, 51)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(12, 52)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(12, 53)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(12, 54)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(12, 55)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(12, 56)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(12, 57)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(12, 58)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(12, 59)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(12, 60)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(12, 61)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(12, 62)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(12, 63)",
           "CKA: 0.77<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(12, 64)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(12, 65)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(12, 66)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(12, 67)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(12, 68)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(12, 69)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(12, 70)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(12, 71)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(12, 72)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(12, 73)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(12, 74)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(12, 75)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(12, 76)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(12, 77)",
           "CKA: 0.508<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(12, 78)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(12, 79)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(12, 80)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(12, 81)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(12, 82)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(12, 83)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(12, 84)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(12, 85)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(12, 86)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(12, 87)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(12, 88)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(12, 89)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(12, 90)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(12, 91)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(12, 92)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(12, 93)",
           "CKA: 0.154<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(12, 94)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(12, 95)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(12, 96)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(12, 97)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(12, 98)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(12, 99)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(12, 100)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(12, 101)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(12, 102)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(12, 103)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: classifier.dropout<br>(12, 104)",
           "CKA: 0.112<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: classifier.dense<br>(12, 105)",
           "CKA: 0.0787<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: classifier.out_proj<br>(12, 106)",
           "CKA: 0.0787<br>Base: roberta.encoder.layer.0.attention.output<br>Pretrained: classifier<br>(12, 107)"
          ],
          [
           "CKA: 0.89<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(13, 0)",
           "CKA: -7.6e-12<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(13, 1)",
           "CKA: 0.0106<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(13, 2)",
           "CKA: 0.907<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(13, 3)",
           "CKA: 0.907<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(13, 4)",
           "CKA: 0.907<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.embeddings<br>(13, 5)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(13, 6)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(13, 7)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(13, 8)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(13, 9)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(13, 10)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(13, 11)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(13, 12)",
           "CKA: 0.894<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(13, 13)",
           "CKA: 0.918<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(13, 14)",
           "CKA: 0.918<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(13, 15)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(13, 16)",
           "CKA: 0.856<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(13, 17)",
           "CKA: 0.936<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(13, 18)",
           "CKA: 0.936<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(13, 19)",
           "CKA: 0.817<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(13, 20)",
           "CKA: 0.77<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(13, 21)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(13, 22)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(13, 23)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(13, 24)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(13, 25)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(13, 26)",
           "CKA: 0.914<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(13, 27)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(13, 28)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(13, 29)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(13, 30)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(13, 31)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(13, 32)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(13, 33)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(13, 34)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(13, 35)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(13, 36)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(13, 37)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(13, 38)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(13, 39)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(13, 40)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(13, 41)",
           "CKA: 0.903<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(13, 42)",
           "CKA: 0.903<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(13, 43)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(13, 44)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(13, 45)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(13, 46)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(13, 47)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(13, 48)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(13, 49)",
           "CKA: 0.847<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(13, 50)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(13, 51)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(13, 52)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(13, 53)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(13, 54)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(13, 55)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(13, 56)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(13, 57)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(13, 58)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(13, 59)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(13, 60)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(13, 61)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(13, 62)",
           "CKA: 0.645<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(13, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(13, 64)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(13, 65)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(13, 66)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(13, 67)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(13, 68)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(13, 69)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(13, 70)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(13, 71)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(13, 72)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(13, 73)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(13, 74)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(13, 75)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(13, 76)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(13, 77)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(13, 78)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(13, 79)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(13, 80)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(13, 81)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(13, 82)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(13, 83)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(13, 84)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(13, 85)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(13, 86)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(13, 87)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(13, 88)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(13, 89)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(13, 90)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(13, 91)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(13, 92)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(13, 93)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(13, 94)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(13, 95)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(13, 96)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(13, 97)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(13, 98)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(13, 99)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(13, 100)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(13, 101)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(13, 102)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(13, 103)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: classifier.dropout<br>(13, 104)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: classifier.dense<br>(13, 105)",
           "CKA: 0.0771<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: classifier.out_proj<br>(13, 106)",
           "CKA: 0.0771<br>Base: roberta.encoder.layer.0.intermediate.dense<br>Pretrained: classifier<br>(13, 107)"
          ],
          [
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(14, 0)",
           "CKA: -2.11e-12<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(14, 1)",
           "CKA: 0.00993<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(14, 2)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(14, 3)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(14, 4)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(14, 5)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(14, 6)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(14, 7)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(14, 8)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(14, 9)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(14, 10)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(14, 11)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(14, 12)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(14, 13)",
           "CKA: 0.922<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(14, 14)",
           "CKA: 0.922<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(14, 15)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(14, 16)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(14, 17)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(14, 18)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(14, 19)",
           "CKA: 0.821<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(14, 20)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(14, 21)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(14, 22)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(14, 23)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(14, 24)",
           "CKA: 0.91<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(14, 25)",
           "CKA: 0.91<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(14, 26)",
           "CKA: 0.916<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(14, 27)",
           "CKA: 0.937<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(14, 28)",
           "CKA: 0.937<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(14, 29)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(14, 30)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(14, 31)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(14, 32)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(14, 33)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(14, 34)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(14, 35)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(14, 36)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(14, 37)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(14, 38)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(14, 39)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(14, 40)",
           "CKA: 0.874<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(14, 41)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(14, 42)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(14, 43)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(14, 44)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(14, 45)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(14, 46)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(14, 47)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(14, 48)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(14, 49)",
           "CKA: 0.853<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(14, 50)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(14, 51)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(14, 52)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(14, 53)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(14, 54)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(14, 55)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(14, 56)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(14, 57)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(14, 58)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(14, 59)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(14, 60)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(14, 61)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(14, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(14, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(14, 64)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(14, 65)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(14, 66)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(14, 67)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(14, 68)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(14, 69)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(14, 70)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(14, 71)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(14, 72)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(14, 73)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(14, 74)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(14, 75)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(14, 76)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(14, 77)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(14, 78)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(14, 79)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(14, 80)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(14, 81)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(14, 82)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(14, 83)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(14, 84)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(14, 85)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(14, 86)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(14, 87)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(14, 88)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(14, 89)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(14, 90)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(14, 91)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(14, 92)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(14, 93)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(14, 94)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(14, 95)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(14, 96)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(14, 97)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(14, 98)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(14, 99)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(14, 100)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(14, 101)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(14, 102)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(14, 103)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(14, 104)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(14, 105)",
           "CKA: 0.077<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(14, 106)",
           "CKA: 0.077<br>Base: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(14, 107)"
          ],
          [
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(15, 0)",
           "CKA: -2.11e-12<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(15, 1)",
           "CKA: 0.00993<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(15, 2)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(15, 3)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(15, 4)",
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.embeddings<br>(15, 5)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(15, 6)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(15, 7)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(15, 8)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(15, 9)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(15, 10)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(15, 11)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(15, 12)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(15, 13)",
           "CKA: 0.922<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(15, 14)",
           "CKA: 0.922<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(15, 15)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(15, 16)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(15, 17)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(15, 18)",
           "CKA: 0.938<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(15, 19)",
           "CKA: 0.821<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(15, 20)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(15, 21)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(15, 22)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(15, 23)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(15, 24)",
           "CKA: 0.91<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(15, 25)",
           "CKA: 0.91<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(15, 26)",
           "CKA: 0.916<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(15, 27)",
           "CKA: 0.937<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(15, 28)",
           "CKA: 0.937<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(15, 29)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(15, 30)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(15, 31)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(15, 32)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(15, 33)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(15, 34)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(15, 35)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(15, 36)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(15, 37)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(15, 38)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(15, 39)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(15, 40)",
           "CKA: 0.874<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(15, 41)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(15, 42)",
           "CKA: 0.908<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(15, 43)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(15, 44)",
           "CKA: 0.892<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(15, 45)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(15, 46)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(15, 47)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(15, 48)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(15, 49)",
           "CKA: 0.853<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(15, 50)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(15, 51)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(15, 52)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(15, 53)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(15, 54)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(15, 55)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(15, 56)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(15, 57)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(15, 58)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(15, 59)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(15, 60)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(15, 61)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(15, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(15, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(15, 64)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(15, 65)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(15, 66)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(15, 67)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(15, 68)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(15, 69)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(15, 70)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(15, 71)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(15, 72)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(15, 73)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(15, 74)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(15, 75)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(15, 76)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(15, 77)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(15, 78)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(15, 79)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(15, 80)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(15, 81)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(15, 82)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(15, 83)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(15, 84)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(15, 85)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(15, 86)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(15, 87)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(15, 88)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(15, 89)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(15, 90)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(15, 91)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(15, 92)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(15, 93)",
           "CKA: 0.15<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(15, 94)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(15, 95)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(15, 96)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(15, 97)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(15, 98)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(15, 99)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(15, 100)",
           "CKA: 0.131<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(15, 101)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(15, 102)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(15, 103)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: classifier.dropout<br>(15, 104)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: classifier.dense<br>(15, 105)",
           "CKA: 0.077<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: classifier.out_proj<br>(15, 106)",
           "CKA: 0.077<br>Base: roberta.encoder.layer.0.intermediate<br>Pretrained: classifier<br>(15, 107)"
          ],
          [
           "CKA: 0.797<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(16, 0)",
           "CKA: -9.53e-13<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(16, 1)",
           "CKA: 0.00509<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(16, 2)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(16, 3)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(16, 4)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.embeddings<br>(16, 5)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(16, 6)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(16, 7)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(16, 8)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(16, 9)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(16, 10)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(16, 11)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(16, 12)",
           "CKA: 0.819<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(16, 13)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(16, 14)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(16, 15)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(16, 16)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(16, 17)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(16, 18)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(16, 19)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(16, 20)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(16, 21)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(16, 22)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(16, 23)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(16, 24)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(16, 25)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(16, 26)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(16, 27)",
           "CKA: 0.88<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(16, 28)",
           "CKA: 0.88<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(16, 29)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(16, 30)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(16, 31)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(16, 32)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(16, 33)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(16, 34)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(16, 35)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(16, 36)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(16, 37)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(16, 38)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(16, 39)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(16, 40)",
           "CKA: 0.817<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(16, 41)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(16, 42)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(16, 43)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(16, 44)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(16, 45)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(16, 46)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(16, 47)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(16, 48)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(16, 49)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(16, 50)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(16, 51)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(16, 52)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(16, 53)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(16, 54)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(16, 55)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(16, 56)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(16, 57)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(16, 58)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(16, 59)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(16, 60)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(16, 61)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(16, 62)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(16, 63)",
           "CKA: 0.739<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(16, 64)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(16, 65)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(16, 66)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(16, 67)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(16, 68)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(16, 69)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(16, 70)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(16, 71)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(16, 72)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(16, 73)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(16, 74)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(16, 75)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(16, 76)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(16, 77)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(16, 78)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(16, 79)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(16, 80)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(16, 81)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(16, 82)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(16, 83)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(16, 84)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(16, 85)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(16, 86)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(16, 87)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(16, 88)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(16, 89)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(16, 90)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(16, 91)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(16, 92)",
           "CKA: 0.132<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(16, 93)",
           "CKA: 0.132<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(16, 94)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(16, 95)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(16, 96)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(16, 97)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(16, 98)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(16, 99)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(16, 100)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(16, 101)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(16, 102)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(16, 103)",
           "CKA: 0.0974<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: classifier.dropout<br>(16, 104)",
           "CKA: 0.0983<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: classifier.dense<br>(16, 105)",
           "CKA: 0.0671<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: classifier.out_proj<br>(16, 106)",
           "CKA: 0.0671<br>Base: roberta.encoder.layer.0.output.dense<br>Pretrained: classifier<br>(16, 107)"
          ],
          [
           "CKA: 0.797<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(17, 0)",
           "CKA: -9.53e-13<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(17, 1)",
           "CKA: 0.00509<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(17, 2)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(17, 3)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(17, 4)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.embeddings<br>(17, 5)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(17, 6)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(17, 7)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(17, 8)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(17, 9)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(17, 10)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(17, 11)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(17, 12)",
           "CKA: 0.819<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(17, 13)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(17, 14)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(17, 15)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(17, 16)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(17, 17)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(17, 18)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(17, 19)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(17, 20)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(17, 21)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(17, 22)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(17, 23)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(17, 24)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(17, 25)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(17, 26)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(17, 27)",
           "CKA: 0.88<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(17, 28)",
           "CKA: 0.88<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(17, 29)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(17, 30)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(17, 31)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(17, 32)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(17, 33)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(17, 34)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(17, 35)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(17, 36)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(17, 37)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(17, 38)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(17, 39)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(17, 40)",
           "CKA: 0.817<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(17, 41)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(17, 42)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(17, 43)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(17, 44)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(17, 45)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(17, 46)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(17, 47)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(17, 48)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(17, 49)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(17, 50)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(17, 51)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(17, 52)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(17, 53)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(17, 54)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(17, 55)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(17, 56)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(17, 57)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(17, 58)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(17, 59)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(17, 60)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(17, 61)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(17, 62)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(17, 63)",
           "CKA: 0.739<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(17, 64)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(17, 65)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(17, 66)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(17, 67)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(17, 68)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(17, 69)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(17, 70)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(17, 71)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(17, 72)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(17, 73)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(17, 74)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(17, 75)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(17, 76)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(17, 77)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(17, 78)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(17, 79)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(17, 80)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(17, 81)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(17, 82)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(17, 83)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(17, 84)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(17, 85)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(17, 86)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(17, 87)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(17, 88)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(17, 89)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(17, 90)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(17, 91)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(17, 92)",
           "CKA: 0.132<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(17, 93)",
           "CKA: 0.132<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(17, 94)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(17, 95)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(17, 96)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(17, 97)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(17, 98)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(17, 99)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(17, 100)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(17, 101)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(17, 102)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(17, 103)",
           "CKA: 0.0974<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: classifier.dropout<br>(17, 104)",
           "CKA: 0.0983<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: classifier.dense<br>(17, 105)",
           "CKA: 0.0671<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: classifier.out_proj<br>(17, 106)",
           "CKA: 0.0671<br>Base: roberta.encoder.layer.0.output.dropout<br>Pretrained: classifier<br>(17, 107)"
          ],
          [
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(18, 0)",
           "CKA: -1.59e-11<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(18, 1)",
           "CKA: 0.0165<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(18, 2)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(18, 3)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(18, 4)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(18, 5)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(18, 6)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(18, 7)",
           "CKA: 0.863<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(18, 8)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(18, 9)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(18, 10)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(18, 11)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(18, 12)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(18, 13)",
           "CKA: 0.934<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(18, 14)",
           "CKA: 0.934<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(18, 15)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(18, 16)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(18, 17)",
           "CKA: 0.95<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(18, 18)",
           "CKA: 0.95<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(18, 19)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(18, 20)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(18, 21)",
           "CKA: 0.853<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(18, 22)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(18, 23)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(18, 24)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(18, 25)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(18, 26)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(18, 27)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(18, 28)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(18, 29)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(18, 30)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(18, 31)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(18, 32)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(18, 33)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(18, 34)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(18, 35)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(18, 36)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(18, 37)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(18, 38)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(18, 39)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(18, 40)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(18, 41)",
           "CKA: 0.912<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(18, 42)",
           "CKA: 0.912<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(18, 43)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(18, 44)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(18, 45)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(18, 46)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(18, 47)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(18, 48)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(18, 49)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(18, 50)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(18, 51)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(18, 52)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(18, 53)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(18, 54)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(18, 55)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(18, 56)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(18, 57)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(18, 58)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(18, 59)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(18, 60)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(18, 61)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(18, 62)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(18, 63)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(18, 64)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(18, 65)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(18, 66)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(18, 67)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(18, 68)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(18, 69)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(18, 70)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(18, 71)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(18, 72)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(18, 73)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(18, 74)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(18, 75)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(18, 76)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(18, 77)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(18, 78)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(18, 79)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(18, 80)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(18, 81)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(18, 82)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(18, 83)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(18, 84)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(18, 85)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(18, 86)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(18, 87)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(18, 88)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(18, 89)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(18, 90)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(18, 91)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(18, 92)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(18, 93)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(18, 94)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(18, 95)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(18, 96)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(18, 97)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(18, 98)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(18, 99)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(18, 100)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(18, 101)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(18, 102)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(18, 103)",
           "CKA: 0.109<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: classifier.dropout<br>(18, 104)",
           "CKA: 0.111<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: classifier.dense<br>(18, 105)",
           "CKA: 0.0776<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(18, 106)",
           "CKA: 0.0776<br>Base: roberta.encoder.layer.0.output.LayerNorm<br>Pretrained: classifier<br>(18, 107)"
          ],
          [
           "CKA: 0.909<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(19, 0)",
           "CKA: -1.59e-11<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(19, 1)",
           "CKA: 0.0165<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(19, 2)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(19, 3)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings.dropout<br>(19, 4)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.embeddings<br>(19, 5)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(19, 6)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(19, 7)",
           "CKA: 0.863<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(19, 8)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(19, 9)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(19, 10)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(19, 11)",
           "CKA: 0.933<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(19, 12)",
           "CKA: 0.911<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(19, 13)",
           "CKA: 0.934<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(19, 14)",
           "CKA: 0.934<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(19, 15)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(19, 16)",
           "CKA: 0.872<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(19, 17)",
           "CKA: 0.95<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(19, 18)",
           "CKA: 0.95<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.0.output<br>(19, 19)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(19, 20)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(19, 21)",
           "CKA: 0.853<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(19, 22)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(19, 23)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(19, 24)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(19, 25)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(19, 26)",
           "CKA: 0.924<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(19, 27)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(19, 28)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(19, 29)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(19, 30)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(19, 31)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(19, 32)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.1.output<br>(19, 33)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(19, 34)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(19, 35)",
           "CKA: 0.85<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(19, 36)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(19, 37)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(19, 38)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(19, 39)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(19, 40)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(19, 41)",
           "CKA: 0.912<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(19, 42)",
           "CKA: 0.912<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(19, 43)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(19, 44)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(19, 45)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(19, 46)",
           "CKA: 0.882<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.2.output<br>(19, 47)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(19, 48)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(19, 49)",
           "CKA: 0.855<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(19, 50)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(19, 51)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(19, 52)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(19, 53)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(19, 54)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(19, 55)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(19, 56)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(19, 57)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(19, 58)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(19, 59)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(19, 60)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.3.output<br>(19, 61)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(19, 62)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(19, 63)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(19, 64)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(19, 65)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(19, 66)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(19, 67)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(19, 68)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(19, 69)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(19, 70)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(19, 71)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(19, 72)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(19, 73)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(19, 74)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.4.output<br>(19, 75)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(19, 76)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(19, 77)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(19, 78)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(19, 79)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(19, 80)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(19, 81)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(19, 82)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(19, 83)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(19, 84)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(19, 85)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(19, 86)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(19, 87)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(19, 88)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.5.output<br>(19, 89)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(19, 90)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(19, 91)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(19, 92)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(19, 93)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(19, 94)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(19, 95)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(19, 96)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(19, 97)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(19, 98)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(19, 99)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(19, 100)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(19, 101)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(19, 102)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.0.output<br>Pretrained: roberta.encoder.layer.6.output<br>(19, 103)",
           "CKA: 0.109<br>Base: roberta.encoder.layer.0.output<br>Pretrained: classifier.dropout<br>(19, 104)",
           "CKA: 0.111<br>Base: roberta.encoder.layer.0.output<br>Pretrained: classifier.dense<br>(19, 105)",
           "CKA: 0.0776<br>Base: roberta.encoder.layer.0.output<br>Pretrained: classifier.out_proj<br>(19, 106)",
           "CKA: 0.0776<br>Base: roberta.encoder.layer.0.output<br>Pretrained: classifier<br>(19, 107)"
          ],
          [
           "CKA: 0.795<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(20, 0)",
           "CKA: -4.72e-12<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(20, 1)",
           "CKA: 0.0268<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(20, 2)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(20, 3)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(20, 4)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.embeddings<br>(20, 5)",
           "CKA: 0.583<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(20, 6)",
           "CKA: 0.758<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(20, 7)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(20, 8)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(20, 9)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(20, 10)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(20, 11)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(20, 12)",
           "CKA: 0.808<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(20, 13)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(20, 14)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(20, 15)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(20, 16)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(20, 17)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(20, 18)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(20, 19)",
           "CKA: 0.777<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(20, 20)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(20, 21)",
           "CKA: 0.829<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(20, 22)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(20, 23)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(20, 24)",
           "CKA: 0.821<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(20, 25)",
           "CKA: 0.821<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(20, 26)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(20, 27)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(20, 28)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(20, 29)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(20, 30)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(20, 31)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(20, 32)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(20, 33)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(20, 34)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(20, 35)",
           "CKA: 0.77<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(20, 36)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(20, 37)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(20, 38)",
           "CKA: 0.787<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(20, 39)",
           "CKA: 0.787<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(20, 40)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(20, 41)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(20, 42)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(20, 43)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(20, 44)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(20, 45)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(20, 46)",
           "CKA: 0.798<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(20, 47)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(20, 48)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(20, 49)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(20, 50)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(20, 51)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(20, 52)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(20, 53)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(20, 54)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(20, 55)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(20, 56)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(20, 57)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(20, 58)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(20, 59)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(20, 60)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(20, 61)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(20, 62)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(20, 63)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(20, 64)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(20, 65)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(20, 66)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(20, 67)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(20, 68)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(20, 69)",
           "CKA: 0.565<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(20, 70)",
           "CKA: 0.565<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(20, 71)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(20, 72)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(20, 73)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(20, 74)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(20, 75)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(20, 76)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(20, 77)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(20, 78)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(20, 79)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(20, 80)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(20, 81)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(20, 82)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(20, 83)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(20, 84)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(20, 85)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(20, 86)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(20, 87)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(20, 88)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(20, 89)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(20, 90)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(20, 91)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(20, 92)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(20, 93)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(20, 94)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(20, 95)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(20, 96)",
           "CKA: 0.205<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(20, 97)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(20, 98)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(20, 99)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(20, 100)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(20, 101)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(20, 102)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(20, 103)",
           "CKA: 0.107<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: classifier.dropout<br>(20, 104)",
           "CKA: 0.109<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: classifier.dense<br>(20, 105)",
           "CKA: 0.0772<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: classifier.out_proj<br>(20, 106)",
           "CKA: 0.0772<br>Base: roberta.encoder.layer.1.attention.self.query<br>Pretrained: classifier<br>(20, 107)"
          ],
          [
           "CKA: 0.767<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(21, 0)",
           "CKA: -6.95e-12<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(21, 1)",
           "CKA: 0.00366<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(21, 2)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(21, 3)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(21, 4)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.embeddings<br>(21, 5)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(21, 6)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(21, 7)",
           "CKA: 0.797<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(21, 8)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(21, 9)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(21, 10)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(21, 11)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(21, 12)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(21, 13)",
           "CKA: 0.84<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(21, 14)",
           "CKA: 0.84<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(21, 15)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(21, 16)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(21, 17)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(21, 18)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(21, 19)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(21, 20)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(21, 21)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(21, 22)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(21, 23)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(21, 24)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(21, 25)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(21, 26)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(21, 27)",
           "CKA: 0.863<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(21, 28)",
           "CKA: 0.863<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(21, 29)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(21, 30)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(21, 31)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(21, 32)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(21, 33)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(21, 34)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(21, 35)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(21, 36)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(21, 37)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(21, 38)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(21, 39)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(21, 40)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(21, 41)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(21, 42)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(21, 43)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(21, 44)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(21, 45)",
           "CKA: 0.772<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(21, 46)",
           "CKA: 0.772<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(21, 47)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(21, 48)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(21, 49)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(21, 50)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(21, 51)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(21, 52)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(21, 53)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(21, 54)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(21, 55)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(21, 56)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(21, 57)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(21, 58)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(21, 59)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(21, 60)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(21, 61)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(21, 62)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(21, 63)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(21, 64)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(21, 65)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(21, 66)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(21, 67)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(21, 68)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(21, 69)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(21, 70)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(21, 71)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(21, 72)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(21, 73)",
           "CKA: 0.566<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(21, 74)",
           "CKA: 0.566<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(21, 75)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(21, 76)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(21, 77)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(21, 78)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(21, 79)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(21, 80)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(21, 81)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(21, 82)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(21, 83)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(21, 84)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(21, 85)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(21, 86)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(21, 87)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(21, 88)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(21, 89)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(21, 90)",
           "CKA: 0.361<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(21, 91)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(21, 92)",
           "CKA: 0.12<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(21, 93)",
           "CKA: 0.12<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(21, 94)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(21, 95)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(21, 96)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(21, 97)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(21, 98)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(21, 99)",
           "CKA: 0.101<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(21, 100)",
           "CKA: 0.101<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(21, 101)",
           "CKA: 0.173<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(21, 102)",
           "CKA: 0.173<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(21, 103)",
           "CKA: 0.0884<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: classifier.dropout<br>(21, 104)",
           "CKA: 0.0892<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: classifier.dense<br>(21, 105)",
           "CKA: 0.0594<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: classifier.out_proj<br>(21, 106)",
           "CKA: 0.0594<br>Base: roberta.encoder.layer.1.attention.self.key<br>Pretrained: classifier<br>(21, 107)"
          ],
          [
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(22, 0)",
           "CKA: -2.57e-12<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(22, 1)",
           "CKA: 0.00904<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(22, 2)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(22, 3)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(22, 4)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.embeddings<br>(22, 5)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(22, 6)",
           "CKA: 0.696<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(22, 7)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(22, 8)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(22, 9)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(22, 10)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(22, 11)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(22, 12)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(22, 13)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(22, 14)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(22, 15)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(22, 16)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(22, 17)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(22, 18)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(22, 19)",
           "CKA: 0.77<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(22, 20)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(22, 21)",
           "CKA: 0.821<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(22, 22)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(22, 23)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(22, 24)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(22, 25)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(22, 26)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(22, 27)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(22, 28)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(22, 29)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(22, 30)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(22, 31)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(22, 32)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(22, 33)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(22, 34)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(22, 35)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(22, 36)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(22, 37)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(22, 38)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(22, 39)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(22, 40)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(22, 41)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(22, 42)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(22, 43)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(22, 44)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(22, 45)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(22, 46)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(22, 47)",
           "CKA: 0.666<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(22, 48)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(22, 49)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(22, 50)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(22, 51)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(22, 52)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(22, 53)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(22, 54)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(22, 55)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(22, 56)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(22, 57)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(22, 58)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(22, 59)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(22, 60)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(22, 61)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(22, 62)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(22, 63)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(22, 64)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(22, 65)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(22, 66)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(22, 67)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(22, 68)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(22, 69)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(22, 70)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(22, 71)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(22, 72)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(22, 73)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(22, 74)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(22, 75)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(22, 76)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(22, 77)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(22, 78)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(22, 79)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(22, 80)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(22, 81)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(22, 82)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(22, 83)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(22, 84)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(22, 85)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(22, 86)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(22, 87)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(22, 88)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(22, 89)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(22, 90)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(22, 91)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(22, 92)",
           "CKA: 0.137<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(22, 93)",
           "CKA: 0.137<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(22, 94)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(22, 95)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(22, 96)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(22, 97)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(22, 98)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(22, 99)",
           "CKA: 0.122<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(22, 100)",
           "CKA: 0.122<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(22, 101)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(22, 102)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(22, 103)",
           "CKA: 0.102<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: classifier.dropout<br>(22, 104)",
           "CKA: 0.103<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: classifier.dense<br>(22, 105)",
           "CKA: 0.0705<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: classifier.out_proj<br>(22, 106)",
           "CKA: 0.0705<br>Base: roberta.encoder.layer.1.attention.self.value<br>Pretrained: classifier<br>(22, 107)"
          ],
          [
           "CKA: 0.366<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(23, 0)",
           "CKA: -5.02e-13<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(23, 1)",
           "CKA: 0.0105<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(23, 2)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(23, 3)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(23, 4)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.embeddings<br>(23, 5)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(23, 6)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(23, 7)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(23, 8)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(23, 9)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(23, 10)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(23, 11)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(23, 12)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(23, 13)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(23, 14)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(23, 15)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(23, 16)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(23, 17)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(23, 18)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(23, 19)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(23, 20)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(23, 21)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(23, 22)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(23, 23)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(23, 24)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(23, 25)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(23, 26)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(23, 27)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(23, 28)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(23, 29)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(23, 30)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(23, 31)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(23, 32)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(23, 33)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(23, 34)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(23, 35)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(23, 36)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(23, 37)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(23, 38)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(23, 39)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(23, 40)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(23, 41)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(23, 42)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(23, 43)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(23, 44)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(23, 45)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(23, 46)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(23, 47)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(23, 48)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(23, 49)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(23, 50)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(23, 51)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(23, 52)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(23, 53)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(23, 54)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(23, 55)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(23, 56)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(23, 57)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(23, 58)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(23, 59)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(23, 60)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(23, 61)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(23, 62)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(23, 63)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(23, 64)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(23, 65)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(23, 66)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(23, 67)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(23, 68)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(23, 69)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(23, 70)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(23, 71)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(23, 72)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(23, 73)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(23, 74)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(23, 75)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(23, 76)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(23, 77)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(23, 78)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(23, 79)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(23, 80)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(23, 81)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(23, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(23, 83)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(23, 84)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(23, 85)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(23, 86)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(23, 87)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(23, 88)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(23, 89)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(23, 90)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(23, 91)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(23, 92)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(23, 93)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(23, 94)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(23, 95)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(23, 96)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(23, 97)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(23, 98)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(23, 99)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(23, 100)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(23, 101)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(23, 102)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(23, 103)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: classifier.dropout<br>(23, 104)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: classifier.dense<br>(23, 105)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: classifier.out_proj<br>(23, 106)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.1.attention.output.dense<br>Pretrained: classifier<br>(23, 107)"
          ],
          [
           "CKA: 0.366<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(24, 0)",
           "CKA: -5.02e-13<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(24, 1)",
           "CKA: 0.0105<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(24, 2)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(24, 3)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(24, 4)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(24, 5)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(24, 6)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(24, 7)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(24, 8)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(24, 9)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(24, 10)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(24, 11)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(24, 12)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(24, 13)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(24, 14)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(24, 15)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(24, 16)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(24, 17)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(24, 18)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(24, 19)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(24, 20)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(24, 21)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(24, 22)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(24, 23)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(24, 24)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(24, 25)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(24, 26)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(24, 27)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(24, 28)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(24, 29)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(24, 30)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(24, 31)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(24, 32)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(24, 33)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(24, 34)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(24, 35)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(24, 36)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(24, 37)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(24, 38)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(24, 39)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(24, 40)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(24, 41)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(24, 42)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(24, 43)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(24, 44)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(24, 45)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(24, 46)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(24, 47)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(24, 48)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(24, 49)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(24, 50)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(24, 51)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(24, 52)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(24, 53)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(24, 54)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(24, 55)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(24, 56)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(24, 57)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(24, 58)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(24, 59)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(24, 60)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(24, 61)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(24, 62)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(24, 63)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(24, 64)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(24, 65)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(24, 66)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(24, 67)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(24, 68)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(24, 69)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(24, 70)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(24, 71)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(24, 72)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(24, 73)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(24, 74)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(24, 75)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(24, 76)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(24, 77)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(24, 78)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(24, 79)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(24, 80)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(24, 81)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(24, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(24, 83)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(24, 84)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(24, 85)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(24, 86)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(24, 87)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(24, 88)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(24, 89)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(24, 90)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(24, 91)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(24, 92)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(24, 93)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(24, 94)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(24, 95)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(24, 96)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(24, 97)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(24, 98)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(24, 99)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(24, 100)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(24, 101)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(24, 102)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(24, 103)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: classifier.dropout<br>(24, 104)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: classifier.dense<br>(24, 105)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(24, 106)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.1.attention.output.dropout<br>Pretrained: classifier<br>(24, 107)"
          ],
          [
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(25, 0)",
           "CKA: -1.71e-11<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(25, 1)",
           "CKA: 0.0161<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(25, 2)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(25, 3)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(25, 4)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(25, 5)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(25, 6)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(25, 7)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(25, 8)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(25, 9)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(25, 10)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(25, 11)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(25, 12)",
           "CKA: 0.906<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(25, 13)",
           "CKA: 0.927<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(25, 14)",
           "CKA: 0.927<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(25, 15)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(25, 16)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(25, 17)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(25, 18)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(25, 19)",
           "CKA: 0.829<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(25, 20)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(25, 21)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(25, 22)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(25, 23)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(25, 24)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(25, 25)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(25, 26)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(25, 27)",
           "CKA: 0.943<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(25, 28)",
           "CKA: 0.943<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(25, 29)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(25, 30)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(25, 31)",
           "CKA: 0.929<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(25, 32)",
           "CKA: 0.929<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(25, 33)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(25, 34)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(25, 35)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(25, 36)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(25, 37)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(25, 38)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(25, 39)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(25, 40)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(25, 41)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(25, 42)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(25, 43)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(25, 44)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(25, 45)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(25, 46)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(25, 47)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(25, 48)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(25, 49)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(25, 50)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(25, 51)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(25, 52)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(25, 53)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(25, 54)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(25, 55)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(25, 56)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(25, 57)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(25, 58)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(25, 59)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(25, 60)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(25, 61)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(25, 62)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(25, 63)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(25, 64)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(25, 65)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(25, 66)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(25, 67)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(25, 68)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(25, 69)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(25, 70)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(25, 71)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(25, 72)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(25, 73)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(25, 74)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(25, 75)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(25, 76)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(25, 77)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(25, 78)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(25, 79)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(25, 80)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(25, 81)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(25, 82)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(25, 83)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(25, 84)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(25, 85)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(25, 86)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(25, 87)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(25, 88)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(25, 89)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(25, 90)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(25, 91)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(25, 92)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(25, 93)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(25, 94)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(25, 95)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(25, 96)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(25, 97)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(25, 98)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(25, 99)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(25, 100)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(25, 101)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(25, 102)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(25, 103)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(25, 104)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(25, 105)",
           "CKA: 0.0921<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(25, 106)",
           "CKA: 0.0921<br>Base: roberta.encoder.layer.1.attention.output.LayerNorm<br>Pretrained: classifier<br>(25, 107)"
          ],
          [
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(26, 0)",
           "CKA: -1.71e-11<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(26, 1)",
           "CKA: 0.0161<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(26, 2)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(26, 3)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(26, 4)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.embeddings<br>(26, 5)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(26, 6)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(26, 7)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(26, 8)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(26, 9)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(26, 10)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(26, 11)",
           "CKA: 0.928<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(26, 12)",
           "CKA: 0.906<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(26, 13)",
           "CKA: 0.927<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(26, 14)",
           "CKA: 0.927<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(26, 15)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(26, 16)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(26, 17)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(26, 18)",
           "CKA: 0.944<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(26, 19)",
           "CKA: 0.829<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(26, 20)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(26, 21)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(26, 22)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(26, 23)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(26, 24)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(26, 25)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(26, 26)",
           "CKA: 0.926<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(26, 27)",
           "CKA: 0.943<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(26, 28)",
           "CKA: 0.943<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(26, 29)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(26, 30)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(26, 31)",
           "CKA: 0.929<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(26, 32)",
           "CKA: 0.929<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(26, 33)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(26, 34)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(26, 35)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(26, 36)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(26, 37)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(26, 38)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(26, 39)",
           "CKA: 0.881<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(26, 40)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(26, 41)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(26, 42)",
           "CKA: 0.917<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(26, 43)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(26, 44)",
           "CKA: 0.885<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(26, 45)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(26, 46)",
           "CKA: 0.89<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(26, 47)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(26, 48)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(26, 49)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(26, 50)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(26, 51)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(26, 52)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(26, 53)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(26, 54)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(26, 55)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(26, 56)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(26, 57)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(26, 58)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(26, 59)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(26, 60)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(26, 61)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(26, 62)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(26, 63)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(26, 64)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(26, 65)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(26, 66)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(26, 67)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(26, 68)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(26, 69)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(26, 70)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(26, 71)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(26, 72)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(26, 73)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(26, 74)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(26, 75)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(26, 76)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(26, 77)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(26, 78)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(26, 79)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(26, 80)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(26, 81)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(26, 82)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(26, 83)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(26, 84)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(26, 85)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(26, 86)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(26, 87)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(26, 88)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(26, 89)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(26, 90)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(26, 91)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(26, 92)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(26, 93)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(26, 94)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(26, 95)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(26, 96)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(26, 97)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(26, 98)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(26, 99)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(26, 100)",
           "CKA: 0.151<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(26, 101)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(26, 102)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(26, 103)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: classifier.dropout<br>(26, 104)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: classifier.dense<br>(26, 105)",
           "CKA: 0.0921<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: classifier.out_proj<br>(26, 106)",
           "CKA: 0.0921<br>Base: roberta.encoder.layer.1.attention.output<br>Pretrained: classifier<br>(26, 107)"
          ],
          [
           "CKA: 0.85<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(27, 0)",
           "CKA: -1.23e-11<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(27, 1)",
           "CKA: 0.00929<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(27, 2)",
           "CKA: 0.866<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(27, 3)",
           "CKA: 0.866<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(27, 4)",
           "CKA: 0.866<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.embeddings<br>(27, 5)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(27, 6)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(27, 7)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(27, 8)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(27, 9)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(27, 10)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(27, 11)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(27, 12)",
           "CKA: 0.863<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(27, 13)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(27, 14)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(27, 15)",
           "CKA: 0.84<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(27, 16)",
           "CKA: 0.84<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(27, 17)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(27, 18)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(27, 19)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(27, 20)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(27, 21)",
           "CKA: 0.822<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(27, 22)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(27, 23)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(27, 24)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(27, 25)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(27, 26)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(27, 27)",
           "CKA: 0.913<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(27, 28)",
           "CKA: 0.913<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(27, 29)",
           "CKA: 0.868<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(27, 30)",
           "CKA: 0.868<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(27, 31)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(27, 32)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(27, 33)",
           "CKA: 0.738<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(27, 34)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(27, 35)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(27, 36)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(27, 37)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(27, 38)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(27, 39)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(27, 40)",
           "CKA: 0.86<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(27, 41)",
           "CKA: 0.896<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(27, 42)",
           "CKA: 0.896<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(27, 43)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(27, 44)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(27, 45)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(27, 46)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(27, 47)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(27, 48)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(27, 49)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(27, 50)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(27, 51)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(27, 52)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(27, 53)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(27, 54)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(27, 55)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(27, 56)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(27, 57)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(27, 58)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(27, 59)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(27, 60)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(27, 61)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(27, 62)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(27, 63)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(27, 64)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(27, 65)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(27, 66)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(27, 67)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(27, 68)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(27, 69)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(27, 70)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(27, 71)",
           "CKA: 0.558<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(27, 72)",
           "CKA: 0.558<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(27, 73)",
           "CKA: 0.625<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(27, 74)",
           "CKA: 0.625<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(27, 75)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(27, 76)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(27, 77)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(27, 78)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(27, 79)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(27, 80)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(27, 81)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(27, 82)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(27, 83)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(27, 84)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(27, 85)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(27, 86)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(27, 87)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(27, 88)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(27, 89)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(27, 90)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(27, 91)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(27, 92)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(27, 93)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(27, 94)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(27, 95)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(27, 96)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(27, 97)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(27, 98)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(27, 99)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(27, 100)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(27, 101)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(27, 102)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(27, 103)",
           "CKA: 0.12<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: classifier.dropout<br>(27, 104)",
           "CKA: 0.123<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: classifier.dense<br>(27, 105)",
           "CKA: 0.0873<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: classifier.out_proj<br>(27, 106)",
           "CKA: 0.0873<br>Base: roberta.encoder.layer.1.intermediate.dense<br>Pretrained: classifier<br>(27, 107)"
          ],
          [
           "CKA: 0.848<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(28, 0)",
           "CKA: -3.79e-12<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(28, 1)",
           "CKA: 0.0078<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(28, 2)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(28, 3)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(28, 4)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(28, 5)",
           "CKA: 0.597<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(28, 6)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(28, 7)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(28, 8)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(28, 9)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(28, 10)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(28, 11)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(28, 12)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(28, 13)",
           "CKA: 0.893<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(28, 14)",
           "CKA: 0.893<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(28, 15)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(28, 16)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(28, 17)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(28, 18)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(28, 19)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(28, 20)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(28, 21)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(28, 22)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(28, 23)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(28, 24)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(28, 25)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(28, 26)",
           "CKA: 0.888<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(28, 27)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(28, 28)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(28, 29)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(28, 30)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(28, 31)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(28, 32)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(28, 33)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(28, 34)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(28, 35)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(28, 36)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(28, 37)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(28, 38)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(28, 39)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(28, 40)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(28, 41)",
           "CKA: 0.902<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(28, 42)",
           "CKA: 0.902<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(28, 43)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(28, 44)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(28, 45)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(28, 46)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(28, 47)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(28, 48)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(28, 49)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(28, 50)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(28, 51)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(28, 52)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(28, 53)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(28, 54)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(28, 55)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(28, 56)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(28, 57)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(28, 58)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(28, 59)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(28, 60)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(28, 61)",
           "CKA: 0.493<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(28, 62)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(28, 63)",
           "CKA: 0.749<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(28, 64)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(28, 65)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(28, 66)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(28, 67)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(28, 68)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(28, 69)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(28, 70)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(28, 71)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(28, 72)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(28, 73)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(28, 74)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(28, 75)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(28, 76)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(28, 77)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(28, 78)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(28, 79)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(28, 80)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(28, 81)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(28, 82)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(28, 83)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(28, 84)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(28, 85)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(28, 86)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(28, 87)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(28, 88)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(28, 89)",
           "CKA: 0.264<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(28, 90)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(28, 91)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(28, 92)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(28, 93)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(28, 94)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(28, 95)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(28, 96)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(28, 97)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(28, 98)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(28, 99)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(28, 100)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(28, 101)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(28, 102)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(28, 103)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(28, 104)",
           "CKA: 0.118<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(28, 105)",
           "CKA: 0.0834<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(28, 106)",
           "CKA: 0.0834<br>Base: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(28, 107)"
          ],
          [
           "CKA: 0.848<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(29, 0)",
           "CKA: -3.79e-12<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(29, 1)",
           "CKA: 0.0078<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(29, 2)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(29, 3)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(29, 4)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.embeddings<br>(29, 5)",
           "CKA: 0.597<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(29, 6)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(29, 7)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(29, 8)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(29, 9)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(29, 10)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(29, 11)",
           "CKA: 0.873<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(29, 12)",
           "CKA: 0.864<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(29, 13)",
           "CKA: 0.893<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(29, 14)",
           "CKA: 0.893<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(29, 15)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(29, 16)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(29, 17)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(29, 18)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(29, 19)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(29, 20)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(29, 21)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(29, 22)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(29, 23)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(29, 24)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(29, 25)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(29, 26)",
           "CKA: 0.888<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(29, 27)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(29, 28)",
           "CKA: 0.915<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(29, 29)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(29, 30)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(29, 31)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(29, 32)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(29, 33)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(29, 34)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(29, 35)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(29, 36)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(29, 37)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(29, 38)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(29, 39)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(29, 40)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(29, 41)",
           "CKA: 0.902<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(29, 42)",
           "CKA: 0.902<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(29, 43)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(29, 44)",
           "CKA: 0.884<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(29, 45)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(29, 46)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(29, 47)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(29, 48)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(29, 49)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(29, 50)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(29, 51)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(29, 52)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(29, 53)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(29, 54)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(29, 55)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(29, 56)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(29, 57)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(29, 58)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(29, 59)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(29, 60)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(29, 61)",
           "CKA: 0.493<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(29, 62)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(29, 63)",
           "CKA: 0.749<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(29, 64)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(29, 65)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(29, 66)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(29, 67)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(29, 68)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(29, 69)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(29, 70)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(29, 71)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(29, 72)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(29, 73)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(29, 74)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(29, 75)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(29, 76)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(29, 77)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(29, 78)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(29, 79)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(29, 80)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(29, 81)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(29, 82)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(29, 83)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(29, 84)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(29, 85)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(29, 86)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(29, 87)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(29, 88)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(29, 89)",
           "CKA: 0.264<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(29, 90)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(29, 91)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(29, 92)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(29, 93)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(29, 94)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(29, 95)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(29, 96)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(29, 97)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(29, 98)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(29, 99)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(29, 100)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(29, 101)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(29, 102)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(29, 103)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: classifier.dropout<br>(29, 104)",
           "CKA: 0.118<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: classifier.dense<br>(29, 105)",
           "CKA: 0.0834<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: classifier.out_proj<br>(29, 106)",
           "CKA: 0.0834<br>Base: roberta.encoder.layer.1.intermediate<br>Pretrained: classifier<br>(29, 107)"
          ],
          [
           "CKA: 0.708<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(30, 0)",
           "CKA: -2.67e-12<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(30, 1)",
           "CKA: 0.00301<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(30, 2)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(30, 3)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(30, 4)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.embeddings<br>(30, 5)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(30, 6)",
           "CKA: 0.725<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(30, 7)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(30, 8)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(30, 9)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(30, 10)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(30, 11)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(30, 12)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(30, 13)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(30, 14)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(30, 15)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(30, 16)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(30, 17)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(30, 18)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(30, 19)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(30, 20)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(30, 21)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(30, 22)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(30, 23)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(30, 24)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(30, 25)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(30, 26)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(30, 27)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(30, 28)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(30, 29)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(30, 30)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(30, 31)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(30, 32)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(30, 33)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(30, 34)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(30, 35)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(30, 36)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(30, 37)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(30, 38)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(30, 39)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(30, 40)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(30, 41)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(30, 42)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(30, 43)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(30, 44)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(30, 45)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(30, 46)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(30, 47)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(30, 48)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(30, 49)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(30, 50)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(30, 51)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(30, 52)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(30, 53)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(30, 54)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(30, 55)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(30, 56)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(30, 57)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(30, 58)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(30, 59)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(30, 60)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(30, 61)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(30, 62)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(30, 63)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(30, 64)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(30, 65)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(30, 66)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(30, 67)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(30, 68)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(30, 69)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(30, 70)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(30, 71)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(30, 72)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(30, 73)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(30, 74)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(30, 75)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(30, 76)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(30, 77)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(30, 78)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(30, 79)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(30, 80)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(30, 81)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(30, 82)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(30, 83)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(30, 84)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(30, 85)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(30, 86)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(30, 87)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(30, 88)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(30, 89)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(30, 90)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(30, 91)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(30, 92)",
           "CKA: 0.127<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(30, 93)",
           "CKA: 0.127<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(30, 94)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(30, 95)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(30, 96)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(30, 97)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(30, 98)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(30, 99)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(30, 100)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(30, 101)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(30, 102)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(30, 103)",
           "CKA: 0.098<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: classifier.dropout<br>(30, 104)",
           "CKA: 0.0997<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: classifier.dense<br>(30, 105)",
           "CKA: 0.0687<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: classifier.out_proj<br>(30, 106)",
           "CKA: 0.0687<br>Base: roberta.encoder.layer.1.output.dense<br>Pretrained: classifier<br>(30, 107)"
          ],
          [
           "CKA: 0.708<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(31, 0)",
           "CKA: -2.67e-12<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(31, 1)",
           "CKA: 0.00301<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(31, 2)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(31, 3)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(31, 4)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.embeddings<br>(31, 5)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(31, 6)",
           "CKA: 0.725<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(31, 7)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(31, 8)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(31, 9)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(31, 10)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(31, 11)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(31, 12)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(31, 13)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(31, 14)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(31, 15)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(31, 16)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(31, 17)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(31, 18)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(31, 19)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(31, 20)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(31, 21)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(31, 22)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(31, 23)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(31, 24)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(31, 25)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(31, 26)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(31, 27)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(31, 28)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(31, 29)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(31, 30)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(31, 31)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(31, 32)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(31, 33)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(31, 34)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(31, 35)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(31, 36)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(31, 37)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(31, 38)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(31, 39)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(31, 40)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(31, 41)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(31, 42)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(31, 43)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(31, 44)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(31, 45)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(31, 46)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(31, 47)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(31, 48)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(31, 49)",
           "CKA: 0.826<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(31, 50)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(31, 51)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(31, 52)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(31, 53)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(31, 54)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(31, 55)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(31, 56)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(31, 57)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(31, 58)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(31, 59)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(31, 60)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(31, 61)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(31, 62)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(31, 63)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(31, 64)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(31, 65)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(31, 66)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(31, 67)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(31, 68)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(31, 69)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(31, 70)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(31, 71)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(31, 72)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(31, 73)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(31, 74)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(31, 75)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(31, 76)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(31, 77)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(31, 78)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(31, 79)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(31, 80)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(31, 81)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(31, 82)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(31, 83)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(31, 84)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(31, 85)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(31, 86)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(31, 87)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(31, 88)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(31, 89)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(31, 90)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(31, 91)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(31, 92)",
           "CKA: 0.127<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(31, 93)",
           "CKA: 0.127<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(31, 94)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(31, 95)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(31, 96)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(31, 97)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(31, 98)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(31, 99)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(31, 100)",
           "CKA: 0.108<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(31, 101)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(31, 102)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(31, 103)",
           "CKA: 0.098<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: classifier.dropout<br>(31, 104)",
           "CKA: 0.0997<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: classifier.dense<br>(31, 105)",
           "CKA: 0.0687<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: classifier.out_proj<br>(31, 106)",
           "CKA: 0.0687<br>Base: roberta.encoder.layer.1.output.dropout<br>Pretrained: classifier<br>(31, 107)"
          ],
          [
           "CKA: 0.87<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(32, 0)",
           "CKA: -2.13e-11<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(32, 1)",
           "CKA: 0.0141<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(32, 2)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(32, 3)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(32, 4)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(32, 5)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(32, 6)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(32, 7)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(32, 8)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(32, 9)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(32, 10)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(32, 11)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(32, 12)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(32, 13)",
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(32, 14)",
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(32, 15)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(32, 16)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(32, 17)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(32, 18)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(32, 19)",
           "CKA: 0.81<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(32, 20)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(32, 21)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(32, 22)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(32, 23)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(32, 24)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(32, 25)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(32, 26)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(32, 27)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(32, 28)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(32, 29)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(32, 30)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(32, 31)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(32, 32)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(32, 33)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(32, 34)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(32, 35)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(32, 36)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(32, 37)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(32, 38)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(32, 39)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(32, 40)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(32, 41)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(32, 42)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(32, 43)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(32, 44)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(32, 45)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(32, 46)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(32, 47)",
           "CKA: 0.714<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(32, 48)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(32, 49)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(32, 50)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(32, 51)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(32, 52)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(32, 53)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(32, 54)",
           "CKA: 0.737<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(32, 55)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(32, 56)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(32, 57)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(32, 58)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(32, 59)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(32, 60)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(32, 61)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(32, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(32, 63)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(32, 64)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(32, 65)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(32, 66)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(32, 67)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(32, 68)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(32, 69)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(32, 70)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(32, 71)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(32, 72)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(32, 73)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(32, 74)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(32, 75)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(32, 76)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(32, 77)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(32, 78)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(32, 79)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(32, 80)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(32, 81)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(32, 82)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(32, 83)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(32, 84)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(32, 85)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(32, 86)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(32, 87)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(32, 88)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(32, 89)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(32, 90)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(32, 91)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(32, 92)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(32, 93)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(32, 94)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(32, 95)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(32, 96)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(32, 97)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(32, 98)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(32, 99)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(32, 100)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(32, 101)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(32, 102)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(32, 103)",
           "CKA: 0.122<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: classifier.dropout<br>(32, 104)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: classifier.dense<br>(32, 105)",
           "CKA: 0.0898<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(32, 106)",
           "CKA: 0.0898<br>Base: roberta.encoder.layer.1.output.LayerNorm<br>Pretrained: classifier<br>(32, 107)"
          ],
          [
           "CKA: 0.87<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(33, 0)",
           "CKA: -2.13e-11<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(33, 1)",
           "CKA: 0.0141<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(33, 2)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(33, 3)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings.dropout<br>(33, 4)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.embeddings<br>(33, 5)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(33, 6)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(33, 7)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(33, 8)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(33, 9)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(33, 10)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(33, 11)",
           "CKA: 0.897<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(33, 12)",
           "CKA: 0.878<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(33, 13)",
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(33, 14)",
           "CKA: 0.904<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(33, 15)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(33, 16)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(33, 17)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(33, 18)",
           "CKA: 0.919<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.0.output<br>(33, 19)",
           "CKA: 0.81<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(33, 20)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(33, 21)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(33, 22)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(33, 23)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(33, 24)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(33, 25)",
           "CKA: 0.899<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(33, 26)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(33, 27)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(33, 28)",
           "CKA: 0.923<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(33, 29)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(33, 30)",
           "CKA: 0.879<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(33, 31)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(33, 32)",
           "CKA: 0.905<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.1.output<br>(33, 33)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(33, 34)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(33, 35)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(33, 36)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(33, 37)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(33, 38)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(33, 39)",
           "CKA: 0.857<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(33, 40)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(33, 41)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(33, 42)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(33, 43)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(33, 44)",
           "CKA: 0.875<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(33, 45)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(33, 46)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.2.output<br>(33, 47)",
           "CKA: 0.714<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(33, 48)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(33, 49)",
           "CKA: 0.852<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(33, 50)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(33, 51)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(33, 52)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(33, 53)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(33, 54)",
           "CKA: 0.737<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(33, 55)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(33, 56)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(33, 57)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(33, 58)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(33, 59)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(33, 60)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.3.output<br>(33, 61)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(33, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(33, 63)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(33, 64)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(33, 65)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(33, 66)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(33, 67)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(33, 68)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(33, 69)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(33, 70)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(33, 71)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(33, 72)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(33, 73)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(33, 74)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.4.output<br>(33, 75)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(33, 76)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(33, 77)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(33, 78)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(33, 79)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(33, 80)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(33, 81)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(33, 82)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(33, 83)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(33, 84)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(33, 85)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(33, 86)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(33, 87)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(33, 88)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.5.output<br>(33, 89)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(33, 90)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(33, 91)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(33, 92)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(33, 93)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(33, 94)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(33, 95)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(33, 96)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(33, 97)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(33, 98)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(33, 99)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(33, 100)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(33, 101)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(33, 102)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.1.output<br>Pretrained: roberta.encoder.layer.6.output<br>(33, 103)",
           "CKA: 0.122<br>Base: roberta.encoder.layer.1.output<br>Pretrained: classifier.dropout<br>(33, 104)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.1.output<br>Pretrained: classifier.dense<br>(33, 105)",
           "CKA: 0.0898<br>Base: roberta.encoder.layer.1.output<br>Pretrained: classifier.out_proj<br>(33, 106)",
           "CKA: 0.0898<br>Base: roberta.encoder.layer.1.output<br>Pretrained: classifier<br>(33, 107)"
          ],
          [
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(34, 0)",
           "CKA: -7.96e-12<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(34, 1)",
           "CKA: 0.0252<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(34, 2)",
           "CKA: 0.716<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(34, 3)",
           "CKA: 0.716<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(34, 4)",
           "CKA: 0.716<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.embeddings<br>(34, 5)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(34, 6)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(34, 7)",
           "CKA: 0.696<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(34, 8)",
           "CKA: 0.264<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(34, 9)",
           "CKA: 0.264<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(34, 10)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(34, 11)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(34, 12)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(34, 13)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(34, 14)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(34, 15)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(34, 16)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(34, 17)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(34, 18)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(34, 19)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(34, 20)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(34, 21)",
           "CKA: 0.738<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(34, 22)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(34, 23)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(34, 24)",
           "CKA: 0.741<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(34, 25)",
           "CKA: 0.741<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(34, 26)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(34, 27)",
           "CKA: 0.782<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(34, 28)",
           "CKA: 0.782<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(34, 29)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(34, 30)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(34, 31)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(34, 32)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(34, 33)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(34, 34)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(34, 35)",
           "CKA: 0.67<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(34, 36)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(34, 37)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(34, 38)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(34, 39)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(34, 40)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(34, 41)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(34, 42)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(34, 43)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(34, 44)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(34, 45)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(34, 46)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(34, 47)",
           "CKA: 0.598<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(34, 48)",
           "CKA: 0.636<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(34, 49)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(34, 50)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(34, 51)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(34, 52)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(34, 53)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(34, 54)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(34, 55)",
           "CKA: 0.66<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(34, 56)",
           "CKA: 0.66<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(34, 57)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(34, 58)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(34, 59)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(34, 60)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(34, 61)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(34, 62)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(34, 63)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(34, 64)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(34, 65)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(34, 66)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(34, 67)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(34, 68)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(34, 69)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(34, 70)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(34, 71)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(34, 72)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(34, 73)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(34, 74)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(34, 75)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(34, 76)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(34, 77)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(34, 78)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(34, 79)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(34, 80)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(34, 81)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(34, 82)",
           "CKA: 0.361<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(34, 83)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(34, 84)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(34, 85)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(34, 86)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(34, 87)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(34, 88)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(34, 89)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(34, 90)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(34, 91)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(34, 92)",
           "CKA: 0.138<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(34, 93)",
           "CKA: 0.138<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(34, 94)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(34, 95)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(34, 96)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(34, 97)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(34, 98)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(34, 99)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(34, 100)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(34, 101)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(34, 102)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(34, 103)",
           "CKA: 0.102<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: classifier.dropout<br>(34, 104)",
           "CKA: 0.104<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: classifier.dense<br>(34, 105)",
           "CKA: 0.0717<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: classifier.out_proj<br>(34, 106)",
           "CKA: 0.0717<br>Base: roberta.encoder.layer.2.attention.self.query<br>Pretrained: classifier<br>(34, 107)"
          ],
          [
           "CKA: 0.758<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(35, 0)",
           "CKA: -7.03e-12<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(35, 1)",
           "CKA: 0.00524<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(35, 2)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(35, 3)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(35, 4)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.embeddings<br>(35, 5)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(35, 6)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(35, 7)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(35, 8)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(35, 9)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(35, 10)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(35, 11)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(35, 12)",
           "CKA: 0.786<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(35, 13)",
           "CKA: 0.829<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(35, 14)",
           "CKA: 0.829<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(35, 15)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(35, 16)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(35, 17)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(35, 18)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(35, 19)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(35, 20)",
           "CKA: 0.76<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(35, 21)",
           "CKA: 0.766<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(35, 22)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(35, 23)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(35, 24)",
           "CKA: 0.797<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(35, 25)",
           "CKA: 0.797<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(35, 26)",
           "CKA: 0.819<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(35, 27)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(35, 28)",
           "CKA: 0.851<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(35, 29)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(35, 30)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(35, 31)",
           "CKA: 0.81<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(35, 32)",
           "CKA: 0.81<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(35, 33)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(35, 34)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(35, 35)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(35, 36)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(35, 37)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(35, 38)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(35, 39)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(35, 40)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(35, 41)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(35, 42)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(35, 43)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(35, 44)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(35, 45)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(35, 46)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(35, 47)",
           "CKA: 0.642<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(35, 48)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(35, 49)",
           "CKA: 0.819<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(35, 50)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(35, 51)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(35, 52)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(35, 53)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(35, 54)",
           "CKA: 0.662<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(35, 55)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(35, 56)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(35, 57)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(35, 58)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(35, 59)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(35, 60)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(35, 61)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(35, 62)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(35, 63)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(35, 64)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(35, 65)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(35, 66)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(35, 67)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(35, 68)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(35, 69)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(35, 70)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(35, 71)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(35, 72)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(35, 73)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(35, 74)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(35, 75)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(35, 76)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(35, 77)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(35, 78)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(35, 79)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(35, 80)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(35, 81)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(35, 82)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(35, 83)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(35, 84)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(35, 85)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(35, 86)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(35, 87)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(35, 88)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(35, 89)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(35, 90)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(35, 91)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(35, 92)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(35, 93)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(35, 94)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(35, 95)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(35, 96)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(35, 97)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(35, 98)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(35, 99)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(35, 100)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(35, 101)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(35, 102)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(35, 103)",
           "CKA: 0.1<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: classifier.dropout<br>(35, 104)",
           "CKA: 0.103<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: classifier.dense<br>(35, 105)",
           "CKA: 0.0704<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: classifier.out_proj<br>(35, 106)",
           "CKA: 0.0704<br>Base: roberta.encoder.layer.2.attention.self.key<br>Pretrained: classifier<br>(35, 107)"
          ],
          [
           "CKA: 0.733<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(36, 0)",
           "CKA: -5.81e-12<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(36, 1)",
           "CKA: 0.00378<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(36, 2)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(36, 3)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(36, 4)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.embeddings<br>(36, 5)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(36, 6)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(36, 7)",
           "CKA: 0.744<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(36, 8)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(36, 9)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(36, 10)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(36, 11)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(36, 12)",
           "CKA: 0.768<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(36, 13)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(36, 14)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(36, 15)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(36, 16)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(36, 17)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(36, 18)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(36, 19)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(36, 20)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(36, 21)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(36, 22)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(36, 23)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(36, 24)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(36, 25)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(36, 26)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(36, 27)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(36, 28)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(36, 29)",
           "CKA: 0.804<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(36, 30)",
           "CKA: 0.804<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(36, 31)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(36, 32)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(36, 33)",
           "CKA: 0.67<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(36, 34)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(36, 35)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(36, 36)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(36, 37)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(36, 38)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(36, 39)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(36, 40)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(36, 41)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(36, 42)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(36, 43)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(36, 44)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(36, 45)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(36, 46)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(36, 47)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(36, 48)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(36, 49)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(36, 50)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(36, 51)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(36, 52)",
           "CKA: 0.688<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(36, 53)",
           "CKA: 0.688<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(36, 54)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(36, 55)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(36, 56)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(36, 57)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(36, 58)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(36, 59)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(36, 60)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(36, 61)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(36, 62)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(36, 63)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(36, 64)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(36, 65)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(36, 66)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(36, 67)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(36, 68)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(36, 69)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(36, 70)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(36, 71)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(36, 72)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(36, 73)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(36, 74)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(36, 75)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(36, 76)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(36, 77)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(36, 78)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(36, 79)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(36, 80)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(36, 81)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(36, 82)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(36, 83)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(36, 84)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(36, 85)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(36, 86)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(36, 87)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(36, 88)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(36, 89)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(36, 90)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(36, 91)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(36, 92)",
           "CKA: 0.14<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(36, 93)",
           "CKA: 0.14<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(36, 94)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(36, 95)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(36, 96)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(36, 97)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(36, 98)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(36, 99)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(36, 100)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(36, 101)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(36, 102)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(36, 103)",
           "CKA: 0.106<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: classifier.dropout<br>(36, 104)",
           "CKA: 0.109<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: classifier.dense<br>(36, 105)",
           "CKA: 0.0753<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: classifier.out_proj<br>(36, 106)",
           "CKA: 0.0753<br>Base: roberta.encoder.layer.2.attention.self.value<br>Pretrained: classifier<br>(36, 107)"
          ],
          [
           "CKA: 0.276<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(37, 0)",
           "CKA: -9.54e-13<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(37, 1)",
           "CKA: 0.00292<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(37, 2)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(37, 3)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(37, 4)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.embeddings<br>(37, 5)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(37, 6)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(37, 7)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(37, 8)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(37, 9)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(37, 10)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(37, 11)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(37, 12)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(37, 13)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(37, 14)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(37, 15)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(37, 16)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(37, 17)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(37, 18)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(37, 19)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(37, 20)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(37, 21)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(37, 22)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(37, 23)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(37, 24)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(37, 25)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(37, 26)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(37, 27)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(37, 28)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(37, 29)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(37, 30)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(37, 31)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(37, 32)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(37, 33)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(37, 34)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(37, 35)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(37, 36)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(37, 37)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(37, 38)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(37, 39)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(37, 40)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(37, 41)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(37, 42)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(37, 43)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(37, 44)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(37, 45)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(37, 46)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(37, 47)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(37, 48)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(37, 49)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(37, 50)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(37, 51)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(37, 52)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(37, 53)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(37, 54)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(37, 55)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(37, 56)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(37, 57)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(37, 58)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(37, 59)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(37, 60)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(37, 61)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(37, 62)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(37, 63)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(37, 64)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(37, 65)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(37, 66)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(37, 67)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(37, 68)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(37, 69)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(37, 70)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(37, 71)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(37, 72)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(37, 73)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(37, 74)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(37, 75)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(37, 76)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(37, 77)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(37, 78)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(37, 79)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(37, 80)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(37, 81)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(37, 82)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(37, 83)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(37, 84)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(37, 85)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(37, 86)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(37, 87)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(37, 88)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(37, 89)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(37, 90)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(37, 91)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(37, 92)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(37, 93)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(37, 94)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(37, 95)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(37, 96)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(37, 97)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(37, 98)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(37, 99)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(37, 100)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(37, 101)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(37, 102)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(37, 103)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: classifier.dropout<br>(37, 104)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: classifier.dense<br>(37, 105)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: classifier.out_proj<br>(37, 106)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.2.attention.output.dense<br>Pretrained: classifier<br>(37, 107)"
          ],
          [
           "CKA: 0.276<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(38, 0)",
           "CKA: -9.54e-13<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(38, 1)",
           "CKA: 0.00292<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(38, 2)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(38, 3)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(38, 4)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(38, 5)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(38, 6)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(38, 7)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(38, 8)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(38, 9)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(38, 10)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(38, 11)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(38, 12)",
           "CKA: 0.284<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(38, 13)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(38, 14)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(38, 15)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(38, 16)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(38, 17)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(38, 18)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(38, 19)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(38, 20)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(38, 21)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(38, 22)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(38, 23)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(38, 24)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(38, 25)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(38, 26)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(38, 27)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(38, 28)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(38, 29)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(38, 30)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(38, 31)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(38, 32)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(38, 33)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(38, 34)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(38, 35)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(38, 36)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(38, 37)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(38, 38)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(38, 39)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(38, 40)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(38, 41)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(38, 42)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(38, 43)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(38, 44)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(38, 45)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(38, 46)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(38, 47)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(38, 48)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(38, 49)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(38, 50)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(38, 51)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(38, 52)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(38, 53)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(38, 54)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(38, 55)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(38, 56)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(38, 57)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(38, 58)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(38, 59)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(38, 60)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(38, 61)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(38, 62)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(38, 63)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(38, 64)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(38, 65)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(38, 66)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(38, 67)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(38, 68)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(38, 69)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(38, 70)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(38, 71)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(38, 72)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(38, 73)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(38, 74)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(38, 75)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(38, 76)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(38, 77)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(38, 78)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(38, 79)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(38, 80)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(38, 81)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(38, 82)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(38, 83)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(38, 84)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(38, 85)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(38, 86)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(38, 87)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(38, 88)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(38, 89)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(38, 90)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(38, 91)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(38, 92)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(38, 93)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(38, 94)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(38, 95)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(38, 96)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(38, 97)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(38, 98)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(38, 99)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(38, 100)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(38, 101)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(38, 102)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(38, 103)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: classifier.dropout<br>(38, 104)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: classifier.dense<br>(38, 105)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(38, 106)",
           "CKA: 0.125<br>Base: roberta.encoder.layer.2.attention.output.dropout<br>Pretrained: classifier<br>(38, 107)"
          ],
          [
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(39, 0)",
           "CKA: -2.36e-11<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(39, 1)",
           "CKA: 0.0137<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(39, 2)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(39, 3)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(39, 4)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(39, 5)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(39, 6)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(39, 7)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(39, 8)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(39, 9)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(39, 10)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(39, 11)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(39, 12)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(39, 13)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(39, 14)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(39, 15)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(39, 16)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(39, 17)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(39, 18)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(39, 19)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(39, 20)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(39, 21)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(39, 22)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(39, 23)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(39, 24)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(39, 25)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(39, 26)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(39, 27)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(39, 28)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(39, 29)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(39, 30)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(39, 31)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(39, 32)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(39, 33)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(39, 34)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(39, 35)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(39, 36)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(39, 37)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(39, 38)",
           "CKA: 0.86<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(39, 39)",
           "CKA: 0.86<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(39, 40)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(39, 41)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(39, 42)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(39, 43)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(39, 44)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(39, 45)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(39, 46)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(39, 47)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(39, 48)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(39, 49)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(39, 50)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(39, 51)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(39, 52)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(39, 53)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(39, 54)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(39, 55)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(39, 56)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(39, 57)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(39, 58)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(39, 59)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(39, 60)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(39, 61)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(39, 62)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(39, 63)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(39, 64)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(39, 65)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(39, 66)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(39, 67)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(39, 68)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(39, 69)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(39, 70)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(39, 71)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(39, 72)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(39, 73)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(39, 74)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(39, 75)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(39, 76)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(39, 77)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(39, 78)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(39, 79)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(39, 80)",
           "CKA: 0.56<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(39, 81)",
           "CKA: 0.56<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(39, 82)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(39, 83)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(39, 84)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(39, 85)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(39, 86)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(39, 87)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(39, 88)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(39, 89)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(39, 90)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(39, 91)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(39, 92)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(39, 93)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(39, 94)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(39, 95)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(39, 96)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(39, 97)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(39, 98)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(39, 99)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(39, 100)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(39, 101)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(39, 102)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(39, 103)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(39, 104)",
           "CKA: 0.169<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(39, 105)",
           "CKA: 0.121<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(39, 106)",
           "CKA: 0.121<br>Base: roberta.encoder.layer.2.attention.output.LayerNorm<br>Pretrained: classifier<br>(39, 107)"
          ],
          [
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(40, 0)",
           "CKA: -2.36e-11<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(40, 1)",
           "CKA: 0.0137<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(40, 2)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(40, 3)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(40, 4)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.embeddings<br>(40, 5)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(40, 6)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(40, 7)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(40, 8)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(40, 9)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(40, 10)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(40, 11)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(40, 12)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(40, 13)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(40, 14)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(40, 15)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(40, 16)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(40, 17)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(40, 18)",
           "CKA: 0.887<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(40, 19)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(40, 20)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(40, 21)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(40, 22)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(40, 23)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(40, 24)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(40, 25)",
           "CKA: 0.886<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(40, 26)",
           "CKA: 0.889<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(40, 27)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(40, 28)",
           "CKA: 0.901<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(40, 29)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(40, 30)",
           "CKA: 0.858<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(40, 31)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(40, 32)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(40, 33)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(40, 34)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(40, 35)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(40, 36)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(40, 37)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(40, 38)",
           "CKA: 0.86<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(40, 39)",
           "CKA: 0.86<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(40, 40)",
           "CKA: 0.87<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(40, 41)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(40, 42)",
           "CKA: 0.891<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(40, 43)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(40, 44)",
           "CKA: 0.859<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(40, 45)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(40, 46)",
           "CKA: 0.869<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(40, 47)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(40, 48)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(40, 49)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(40, 50)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(40, 51)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(40, 52)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(40, 53)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(40, 54)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(40, 55)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(40, 56)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(40, 57)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(40, 58)",
           "CKA: 0.748<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(40, 59)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(40, 60)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(40, 61)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(40, 62)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(40, 63)",
           "CKA: 0.795<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(40, 64)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(40, 65)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(40, 66)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(40, 67)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(40, 68)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(40, 69)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(40, 70)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(40, 71)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(40, 72)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(40, 73)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(40, 74)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(40, 75)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(40, 76)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(40, 77)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(40, 78)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(40, 79)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(40, 80)",
           "CKA: 0.56<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(40, 81)",
           "CKA: 0.56<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(40, 82)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(40, 83)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(40, 84)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(40, 85)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(40, 86)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(40, 87)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(40, 88)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(40, 89)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(40, 90)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(40, 91)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(40, 92)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(40, 93)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(40, 94)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(40, 95)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(40, 96)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(40, 97)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(40, 98)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(40, 99)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(40, 100)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(40, 101)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(40, 102)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(40, 103)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: classifier.dropout<br>(40, 104)",
           "CKA: 0.169<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: classifier.dense<br>(40, 105)",
           "CKA: 0.121<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: classifier.out_proj<br>(40, 106)",
           "CKA: 0.121<br>Base: roberta.encoder.layer.2.attention.output<br>Pretrained: classifier<br>(40, 107)"
          ],
          [
           "CKA: 0.776<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(41, 0)",
           "CKA: -1.87e-11<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(41, 1)",
           "CKA: 0.00942<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(41, 2)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(41, 3)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(41, 4)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.embeddings<br>(41, 5)",
           "CKA: 0.509<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(41, 6)",
           "CKA: 0.66<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(41, 7)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(41, 8)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(41, 9)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(41, 10)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(41, 11)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(41, 12)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(41, 13)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(41, 14)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(41, 15)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(41, 16)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(41, 17)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(41, 18)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(41, 19)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(41, 20)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(41, 21)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(41, 22)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(41, 23)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(41, 24)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(41, 25)",
           "CKA: 0.832<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(41, 26)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(41, 27)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(41, 28)",
           "CKA: 0.844<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(41, 29)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(41, 30)",
           "CKA: 0.807<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(41, 31)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(41, 32)",
           "CKA: 0.837<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(41, 33)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(41, 34)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(41, 35)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(41, 36)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(41, 37)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(41, 38)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(41, 39)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(41, 40)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(41, 41)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(41, 42)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(41, 43)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(41, 44)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(41, 45)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(41, 46)",
           "CKA: 0.824<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(41, 47)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(41, 48)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(41, 49)",
           "CKA: 0.777<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(41, 50)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(41, 51)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(41, 52)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(41, 53)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(41, 54)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(41, 55)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(41, 56)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(41, 57)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(41, 58)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(41, 59)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(41, 60)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(41, 61)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(41, 62)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(41, 63)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(41, 64)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(41, 65)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(41, 66)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(41, 67)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(41, 68)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(41, 69)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(41, 70)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(41, 71)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(41, 72)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(41, 73)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(41, 74)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(41, 75)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(41, 76)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(41, 77)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(41, 78)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(41, 79)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(41, 80)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(41, 81)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(41, 82)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(41, 83)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(41, 84)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(41, 85)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(41, 86)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(41, 87)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(41, 88)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(41, 89)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(41, 90)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(41, 91)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(41, 92)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(41, 93)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(41, 94)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(41, 95)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(41, 96)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(41, 97)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(41, 98)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(41, 99)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(41, 100)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(41, 101)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(41, 102)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(41, 103)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: classifier.dropout<br>(41, 104)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: classifier.dense<br>(41, 105)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: classifier.out_proj<br>(41, 106)",
           "CKA: 0.133<br>Base: roberta.encoder.layer.2.intermediate.dense<br>Pretrained: classifier<br>(41, 107)"
          ],
          [
           "CKA: 0.784<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(42, 0)",
           "CKA: -5.12e-12<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(42, 1)",
           "CKA: 0.00912<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(42, 2)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(42, 3)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(42, 4)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(42, 5)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(42, 6)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(42, 7)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(42, 8)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(42, 9)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(42, 10)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(42, 11)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(42, 12)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(42, 13)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(42, 14)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(42, 15)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(42, 16)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(42, 17)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(42, 18)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(42, 19)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(42, 20)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(42, 21)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(42, 22)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(42, 23)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(42, 24)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(42, 25)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(42, 26)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(42, 27)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(42, 28)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(42, 29)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(42, 30)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(42, 31)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(42, 32)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(42, 33)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(42, 34)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(42, 35)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(42, 36)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(42, 37)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(42, 38)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(42, 39)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(42, 40)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(42, 41)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(42, 42)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(42, 43)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(42, 44)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(42, 45)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(42, 46)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(42, 47)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(42, 48)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(42, 49)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(42, 50)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(42, 51)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(42, 52)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(42, 53)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(42, 54)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(42, 55)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(42, 56)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(42, 57)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(42, 58)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(42, 59)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(42, 60)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(42, 61)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(42, 62)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(42, 63)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(42, 64)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(42, 65)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(42, 66)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(42, 67)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(42, 68)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(42, 69)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(42, 70)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(42, 71)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(42, 72)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(42, 73)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(42, 74)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(42, 75)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(42, 76)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(42, 77)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(42, 78)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(42, 79)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(42, 80)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(42, 81)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(42, 82)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(42, 83)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(42, 84)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(42, 85)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(42, 86)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(42, 87)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(42, 88)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(42, 89)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(42, 90)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(42, 91)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(42, 92)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(42, 93)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(42, 94)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(42, 95)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(42, 96)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(42, 97)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(42, 98)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(42, 99)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(42, 100)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(42, 101)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(42, 102)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(42, 103)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(42, 104)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(42, 105)",
           "CKA: 0.13<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(42, 106)",
           "CKA: 0.13<br>Base: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(42, 107)"
          ],
          [
           "CKA: 0.784<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(43, 0)",
           "CKA: -5.12e-12<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(43, 1)",
           "CKA: 0.00912<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(43, 2)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(43, 3)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(43, 4)",
           "CKA: 0.802<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.embeddings<br>(43, 5)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(43, 6)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(43, 7)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(43, 8)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(43, 9)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(43, 10)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(43, 11)",
           "CKA: 0.815<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(43, 12)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(43, 13)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(43, 14)",
           "CKA: 0.812<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(43, 15)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(43, 16)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(43, 17)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(43, 18)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(43, 19)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(43, 20)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(43, 21)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(43, 22)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(43, 23)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(43, 24)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(43, 25)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(43, 26)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(43, 27)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(43, 28)",
           "CKA: 0.848<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(43, 29)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(43, 30)",
           "CKA: 0.811<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(43, 31)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(43, 32)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(43, 33)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(43, 34)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(43, 35)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(43, 36)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(43, 37)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(43, 38)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(43, 39)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(43, 40)",
           "CKA: 0.827<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(43, 41)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(43, 42)",
           "CKA: 0.845<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(43, 43)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(43, 44)",
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(43, 45)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(43, 46)",
           "CKA: 0.828<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(43, 47)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(43, 48)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(43, 49)",
           "CKA: 0.778<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(43, 50)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(43, 51)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(43, 52)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(43, 53)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(43, 54)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(43, 55)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(43, 56)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(43, 57)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(43, 58)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(43, 59)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(43, 60)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(43, 61)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(43, 62)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(43, 63)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(43, 64)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(43, 65)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(43, 66)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(43, 67)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(43, 68)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(43, 69)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(43, 70)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(43, 71)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(43, 72)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(43, 73)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(43, 74)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(43, 75)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(43, 76)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(43, 77)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(43, 78)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(43, 79)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(43, 80)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(43, 81)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(43, 82)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(43, 83)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(43, 84)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(43, 85)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(43, 86)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(43, 87)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(43, 88)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(43, 89)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(43, 90)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(43, 91)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(43, 92)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(43, 93)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(43, 94)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(43, 95)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(43, 96)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(43, 97)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(43, 98)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(43, 99)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(43, 100)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(43, 101)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(43, 102)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(43, 103)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: classifier.dropout<br>(43, 104)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: classifier.dense<br>(43, 105)",
           "CKA: 0.13<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: classifier.out_proj<br>(43, 106)",
           "CKA: 0.13<br>Base: roberta.encoder.layer.2.intermediate<br>Pretrained: classifier<br>(43, 107)"
          ],
          [
           "CKA: 0.64<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(44, 0)",
           "CKA: -3.46e-12<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(44, 1)",
           "CKA: 0.00493<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(44, 2)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(44, 3)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(44, 4)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.embeddings<br>(44, 5)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(44, 6)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(44, 7)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(44, 8)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(44, 9)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(44, 10)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(44, 11)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(44, 12)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(44, 13)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(44, 14)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(44, 15)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(44, 16)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(44, 17)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(44, 18)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(44, 19)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(44, 20)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(44, 21)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(44, 22)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(44, 23)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(44, 24)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(44, 25)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(44, 26)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(44, 27)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(44, 28)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(44, 29)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(44, 30)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(44, 31)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(44, 32)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(44, 33)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(44, 34)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(44, 35)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(44, 36)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(44, 37)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(44, 38)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(44, 39)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(44, 40)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(44, 41)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(44, 42)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(44, 43)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(44, 44)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(44, 45)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(44, 46)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(44, 47)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(44, 48)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(44, 49)",
           "CKA: 0.675<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(44, 50)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(44, 51)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(44, 52)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(44, 53)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(44, 54)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(44, 55)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(44, 56)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(44, 57)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(44, 58)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(44, 59)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(44, 60)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(44, 61)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(44, 62)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(44, 63)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(44, 64)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(44, 65)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(44, 66)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(44, 67)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(44, 68)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(44, 69)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(44, 70)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(44, 71)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(44, 72)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(44, 73)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(44, 74)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(44, 75)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(44, 76)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(44, 77)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(44, 78)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(44, 79)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(44, 80)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(44, 81)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(44, 82)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(44, 83)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(44, 84)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(44, 85)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(44, 86)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(44, 87)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(44, 88)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(44, 89)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(44, 90)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(44, 91)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(44, 92)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(44, 93)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(44, 94)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(44, 95)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(44, 96)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(44, 97)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(44, 98)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(44, 99)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(44, 100)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(44, 101)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(44, 102)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(44, 103)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: classifier.dropout<br>(44, 104)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: classifier.dense<br>(44, 105)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: classifier.out_proj<br>(44, 106)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.2.output.dense<br>Pretrained: classifier<br>(44, 107)"
          ],
          [
           "CKA: 0.64<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(45, 0)",
           "CKA: -3.46e-12<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(45, 1)",
           "CKA: 0.00493<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(45, 2)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(45, 3)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(45, 4)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.embeddings<br>(45, 5)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(45, 6)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(45, 7)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(45, 8)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(45, 9)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(45, 10)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(45, 11)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(45, 12)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(45, 13)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(45, 14)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(45, 15)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(45, 16)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(45, 17)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(45, 18)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(45, 19)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(45, 20)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(45, 21)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(45, 22)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(45, 23)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(45, 24)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(45, 25)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(45, 26)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(45, 27)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(45, 28)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(45, 29)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(45, 30)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(45, 31)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(45, 32)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(45, 33)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(45, 34)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(45, 35)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(45, 36)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(45, 37)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(45, 38)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(45, 39)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(45, 40)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(45, 41)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(45, 42)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(45, 43)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(45, 44)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(45, 45)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(45, 46)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(45, 47)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(45, 48)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(45, 49)",
           "CKA: 0.675<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(45, 50)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(45, 51)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(45, 52)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(45, 53)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(45, 54)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(45, 55)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(45, 56)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(45, 57)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(45, 58)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(45, 59)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(45, 60)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(45, 61)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(45, 62)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(45, 63)",
           "CKA: 0.603<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(45, 64)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(45, 65)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(45, 66)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(45, 67)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(45, 68)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(45, 69)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(45, 70)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(45, 71)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(45, 72)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(45, 73)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(45, 74)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(45, 75)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(45, 76)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(45, 77)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(45, 78)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(45, 79)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(45, 80)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(45, 81)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(45, 82)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(45, 83)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(45, 84)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(45, 85)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(45, 86)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(45, 87)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(45, 88)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(45, 89)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(45, 90)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(45, 91)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(45, 92)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(45, 93)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(45, 94)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(45, 95)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(45, 96)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(45, 97)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(45, 98)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(45, 99)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(45, 100)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(45, 101)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(45, 102)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(45, 103)",
           "CKA: 0.152<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: classifier.dropout<br>(45, 104)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: classifier.dense<br>(45, 105)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: classifier.out_proj<br>(45, 106)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.2.output.dropout<br>Pretrained: classifier<br>(45, 107)"
          ],
          [
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(46, 0)",
           "CKA: -3e-11<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(46, 1)",
           "CKA: 0.0129<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(46, 2)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(46, 3)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(46, 4)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(46, 5)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(46, 6)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(46, 7)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(46, 8)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(46, 9)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(46, 10)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(46, 11)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(46, 12)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(46, 13)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(46, 14)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(46, 15)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(46, 16)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(46, 17)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(46, 18)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(46, 19)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(46, 20)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(46, 21)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(46, 22)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(46, 23)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(46, 24)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(46, 25)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(46, 26)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(46, 27)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(46, 28)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(46, 29)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(46, 30)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(46, 31)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(46, 32)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(46, 33)",
           "CKA: 0.741<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(46, 34)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(46, 35)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(46, 36)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(46, 37)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(46, 38)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(46, 39)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(46, 40)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(46, 41)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(46, 42)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(46, 43)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(46, 44)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(46, 45)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(46, 46)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(46, 47)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(46, 48)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(46, 49)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(46, 50)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(46, 51)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(46, 52)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(46, 53)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(46, 54)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(46, 55)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(46, 56)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(46, 57)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(46, 58)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(46, 59)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(46, 60)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(46, 61)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(46, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(46, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(46, 64)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(46, 65)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(46, 66)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(46, 67)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(46, 68)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(46, 69)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(46, 70)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(46, 71)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(46, 72)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(46, 73)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(46, 74)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(46, 75)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(46, 76)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(46, 77)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(46, 78)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(46, 79)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(46, 80)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(46, 81)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(46, 82)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(46, 83)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(46, 84)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(46, 85)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(46, 86)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(46, 87)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(46, 88)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(46, 89)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(46, 90)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(46, 91)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(46, 92)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(46, 93)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(46, 94)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(46, 95)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(46, 96)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(46, 97)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(46, 98)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(46, 99)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(46, 100)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(46, 101)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(46, 102)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(46, 103)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: classifier.dropout<br>(46, 104)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: classifier.dense<br>(46, 105)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(46, 106)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.output.LayerNorm<br>Pretrained: classifier<br>(46, 107)"
          ],
          [
           "CKA: 0.814<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(47, 0)",
           "CKA: -3e-11<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(47, 1)",
           "CKA: 0.0129<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(47, 2)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(47, 3)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings.dropout<br>(47, 4)",
           "CKA: 0.83<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.embeddings<br>(47, 5)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(47, 6)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(47, 7)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(47, 8)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(47, 9)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(47, 10)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(47, 11)",
           "CKA: 0.842<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(47, 12)",
           "CKA: 0.823<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(47, 13)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(47, 14)",
           "CKA: 0.846<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(47, 15)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(47, 16)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(47, 17)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(47, 18)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.0.output<br>(47, 19)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(47, 20)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(47, 21)",
           "CKA: 0.779<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(47, 22)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(47, 23)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(47, 24)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(47, 25)",
           "CKA: 0.862<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(47, 26)",
           "CKA: 0.865<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(47, 27)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(47, 28)",
           "CKA: 0.877<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(47, 29)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(47, 30)",
           "CKA: 0.835<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(47, 31)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(47, 32)",
           "CKA: 0.867<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.1.output<br>(47, 33)",
           "CKA: 0.741<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(47, 34)",
           "CKA: 0.791<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(47, 35)",
           "CKA: 0.794<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(47, 36)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(47, 37)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(47, 38)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(47, 39)",
           "CKA: 0.839<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(47, 40)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(47, 41)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(47, 42)",
           "CKA: 0.871<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(47, 43)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(47, 44)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(47, 45)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(47, 46)",
           "CKA: 0.849<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.2.output<br>(47, 47)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(47, 48)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(47, 49)",
           "CKA: 0.816<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(47, 50)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(47, 51)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(47, 52)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(47, 53)",
           "CKA: 0.783<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(47, 54)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(47, 55)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(47, 56)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(47, 57)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(47, 58)",
           "CKA: 0.727<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(47, 59)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(47, 60)",
           "CKA: 0.781<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.3.output<br>(47, 61)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(47, 62)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(47, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(47, 64)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(47, 65)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(47, 66)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(47, 67)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(47, 68)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(47, 69)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(47, 70)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(47, 71)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(47, 72)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(47, 73)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(47, 74)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.4.output<br>(47, 75)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(47, 76)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(47, 77)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(47, 78)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(47, 79)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(47, 80)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(47, 81)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(47, 82)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(47, 83)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(47, 84)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(47, 85)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(47, 86)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(47, 87)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(47, 88)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.5.output<br>(47, 89)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(47, 90)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(47, 91)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(47, 92)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(47, 93)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(47, 94)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(47, 95)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(47, 96)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(47, 97)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(47, 98)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(47, 99)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(47, 100)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(47, 101)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(47, 102)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.2.output<br>Pretrained: roberta.encoder.layer.6.output<br>(47, 103)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.2.output<br>Pretrained: classifier.dropout<br>(47, 104)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.2.output<br>Pretrained: classifier.dense<br>(47, 105)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.output<br>Pretrained: classifier.out_proj<br>(47, 106)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.2.output<br>Pretrained: classifier<br>(47, 107)"
          ],
          [
           "CKA: 0.664<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(48, 0)",
           "CKA: -1.02e-11<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(48, 1)",
           "CKA: 0.00495<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(48, 2)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(48, 3)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(48, 4)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.embeddings<br>(48, 5)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(48, 6)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(48, 7)",
           "CKA: 0.67<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(48, 8)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(48, 9)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(48, 10)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(48, 11)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(48, 12)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(48, 13)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(48, 14)",
           "CKA: 0.702<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(48, 15)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(48, 16)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(48, 17)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(48, 18)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(48, 19)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(48, 20)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(48, 21)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(48, 22)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(48, 23)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(48, 24)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(48, 25)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(48, 26)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(48, 27)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(48, 28)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(48, 29)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(48, 30)",
           "CKA: 0.698<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(48, 31)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(48, 32)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(48, 33)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(48, 34)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(48, 35)",
           "CKA: 0.67<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(48, 36)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(48, 37)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(48, 38)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(48, 39)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(48, 40)",
           "CKA: 0.749<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(48, 41)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(48, 42)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(48, 43)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(48, 44)",
           "CKA: 0.745<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(48, 45)",
           "CKA: 0.734<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(48, 46)",
           "CKA: 0.734<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(48, 47)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(48, 48)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(48, 49)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(48, 50)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(48, 51)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(48, 52)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(48, 53)",
           "CKA: 0.701<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(48, 54)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(48, 55)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(48, 56)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(48, 57)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(48, 58)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(48, 59)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(48, 60)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(48, 61)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(48, 62)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(48, 63)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(48, 64)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(48, 65)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(48, 66)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(48, 67)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(48, 68)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(48, 69)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(48, 70)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(48, 71)",
           "CKA: 0.583<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(48, 72)",
           "CKA: 0.583<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(48, 73)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(48, 74)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(48, 75)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(48, 76)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(48, 77)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(48, 78)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(48, 79)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(48, 80)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(48, 81)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(48, 82)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(48, 83)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(48, 84)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(48, 85)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(48, 86)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(48, 87)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(48, 88)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(48, 89)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(48, 90)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(48, 91)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(48, 92)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(48, 93)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(48, 94)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(48, 95)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(48, 96)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(48, 97)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(48, 98)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(48, 99)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(48, 100)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(48, 101)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(48, 102)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(48, 103)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: classifier.dropout<br>(48, 104)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: classifier.dense<br>(48, 105)",
           "CKA: 0.135<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: classifier.out_proj<br>(48, 106)",
           "CKA: 0.135<br>Base: roberta.encoder.layer.3.attention.self.query<br>Pretrained: classifier<br>(48, 107)"
          ],
          [
           "CKA: 0.645<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(49, 0)",
           "CKA: -9.74e-12<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(49, 1)",
           "CKA: 0.0051<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(49, 2)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(49, 3)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(49, 4)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.embeddings<br>(49, 5)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(49, 6)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(49, 7)",
           "CKA: 0.642<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(49, 8)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(49, 9)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(49, 10)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(49, 11)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(49, 12)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(49, 13)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(49, 14)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(49, 15)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(49, 16)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(49, 17)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(49, 18)",
           "CKA: 0.712<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(49, 19)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(49, 20)",
           "CKA: 0.597<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(49, 21)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(49, 22)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(49, 23)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(49, 24)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(49, 25)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(49, 26)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(49, 27)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(49, 28)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(49, 29)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(49, 30)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(49, 31)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(49, 32)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(49, 33)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(49, 34)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(49, 35)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(49, 36)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(49, 37)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(49, 38)",
           "CKA: 0.662<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(49, 39)",
           "CKA: 0.662<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(49, 40)",
           "CKA: 0.688<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(49, 41)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(49, 42)",
           "CKA: 0.73<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(49, 43)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(49, 44)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(49, 45)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(49, 46)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(49, 47)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(49, 48)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(49, 49)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(49, 50)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(49, 51)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(49, 52)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(49, 53)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(49, 54)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(49, 55)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(49, 56)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(49, 57)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(49, 58)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(49, 59)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(49, 60)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(49, 61)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(49, 62)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(49, 63)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(49, 64)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(49, 65)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(49, 66)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(49, 67)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(49, 68)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(49, 69)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(49, 70)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(49, 71)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(49, 72)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(49, 73)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(49, 74)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(49, 75)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(49, 76)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(49, 77)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(49, 78)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(49, 79)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(49, 80)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(49, 81)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(49, 82)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(49, 83)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(49, 84)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(49, 85)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(49, 86)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(49, 87)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(49, 88)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(49, 89)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(49, 90)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(49, 91)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(49, 92)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(49, 93)",
           "CKA: 0.116<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(49, 94)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(49, 95)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(49, 96)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(49, 97)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(49, 98)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(49, 99)",
           "CKA: 0.106<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(49, 100)",
           "CKA: 0.106<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(49, 101)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(49, 102)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(49, 103)",
           "CKA: 0.0893<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: classifier.dropout<br>(49, 104)",
           "CKA: 0.0916<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: classifier.dense<br>(49, 105)",
           "CKA: 0.0649<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: classifier.out_proj<br>(49, 106)",
           "CKA: 0.0649<br>Base: roberta.encoder.layer.3.attention.self.key<br>Pretrained: classifier<br>(49, 107)"
          ],
          [
           "CKA: 0.625<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(50, 0)",
           "CKA: -5.07e-12<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(50, 1)",
           "CKA: 0.00506<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(50, 2)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(50, 3)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(50, 4)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.embeddings<br>(50, 5)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(50, 6)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(50, 7)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(50, 8)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(50, 9)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(50, 10)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(50, 11)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(50, 12)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(50, 13)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(50, 14)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(50, 15)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(50, 16)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(50, 17)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(50, 18)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(50, 19)",
           "CKA: 0.648<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(50, 20)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(50, 21)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(50, 22)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(50, 23)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(50, 24)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(50, 25)",
           "CKA: 0.679<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(50, 26)",
           "CKA: 0.695<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(50, 27)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(50, 28)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(50, 29)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(50, 30)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(50, 31)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(50, 32)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(50, 33)",
           "CKA: 0.625<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(50, 34)",
           "CKA: 0.624<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(50, 35)",
           "CKA: 0.645<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(50, 36)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(50, 37)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(50, 38)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(50, 39)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(50, 40)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(50, 41)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(50, 42)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(50, 43)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(50, 44)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(50, 45)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(50, 46)",
           "CKA: 0.677<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(50, 47)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(50, 48)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(50, 49)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(50, 50)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(50, 51)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(50, 52)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(50, 53)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(50, 54)",
           "CKA: 0.591<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(50, 55)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(50, 56)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(50, 57)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(50, 58)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(50, 59)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(50, 60)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(50, 61)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(50, 62)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(50, 63)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(50, 64)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(50, 65)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(50, 66)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(50, 67)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(50, 68)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(50, 69)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(50, 70)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(50, 71)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(50, 72)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(50, 73)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(50, 74)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(50, 75)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(50, 76)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(50, 77)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(50, 78)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(50, 79)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(50, 80)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(50, 81)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(50, 82)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(50, 83)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(50, 84)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(50, 85)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(50, 86)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(50, 87)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(50, 88)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(50, 89)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(50, 90)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(50, 91)",
           "CKA: 0.306<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(50, 92)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(50, 93)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(50, 94)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(50, 95)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(50, 96)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(50, 97)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(50, 98)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(50, 99)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(50, 100)",
           "CKA: 0.195<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(50, 101)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(50, 102)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(50, 103)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: classifier.dropout<br>(50, 104)",
           "CKA: 0.173<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: classifier.dense<br>(50, 105)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: classifier.out_proj<br>(50, 106)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.3.attention.self.value<br>Pretrained: classifier<br>(50, 107)"
          ],
          [
           "CKA: 0.189<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(51, 0)",
           "CKA: -4.53e-12<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(51, 1)",
           "CKA: 0.0033<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(51, 2)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(51, 3)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(51, 4)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.embeddings<br>(51, 5)",
           "CKA: 0.101<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(51, 6)",
           "CKA: 0.142<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(51, 7)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(51, 8)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(51, 9)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(51, 10)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(51, 11)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(51, 12)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(51, 13)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(51, 14)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(51, 15)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(51, 16)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(51, 17)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(51, 18)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(51, 19)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(51, 20)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(51, 21)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(51, 22)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(51, 23)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(51, 24)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(51, 25)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(51, 26)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(51, 27)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(51, 28)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(51, 29)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(51, 30)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(51, 31)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(51, 32)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(51, 33)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(51, 34)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(51, 35)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(51, 36)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(51, 37)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(51, 38)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(51, 39)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(51, 40)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(51, 41)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(51, 42)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(51, 43)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(51, 44)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(51, 45)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(51, 46)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(51, 47)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(51, 48)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(51, 49)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(51, 50)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(51, 51)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(51, 52)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(51, 53)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(51, 54)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(51, 55)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(51, 56)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(51, 57)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(51, 58)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(51, 59)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(51, 60)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(51, 61)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(51, 62)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(51, 63)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(51, 64)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(51, 65)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(51, 66)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(51, 67)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(51, 68)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(51, 69)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(51, 70)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(51, 71)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(51, 72)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(51, 73)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(51, 74)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(51, 75)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(51, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(51, 77)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(51, 78)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(51, 79)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(51, 80)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(51, 81)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(51, 82)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(51, 83)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(51, 84)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(51, 85)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(51, 86)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(51, 87)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(51, 88)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(51, 89)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(51, 90)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(51, 91)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(51, 92)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(51, 93)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(51, 94)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(51, 95)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(51, 96)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(51, 97)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(51, 98)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(51, 99)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(51, 100)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(51, 101)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(51, 102)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(51, 103)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: classifier.dropout<br>(51, 104)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: classifier.dense<br>(51, 105)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: classifier.out_proj<br>(51, 106)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.3.attention.output.dense<br>Pretrained: classifier<br>(51, 107)"
          ],
          [
           "CKA: 0.189<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(52, 0)",
           "CKA: -4.53e-12<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(52, 1)",
           "CKA: 0.0033<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(52, 2)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(52, 3)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(52, 4)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(52, 5)",
           "CKA: 0.101<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(52, 6)",
           "CKA: 0.142<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(52, 7)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(52, 8)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(52, 9)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(52, 10)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(52, 11)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(52, 12)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(52, 13)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(52, 14)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(52, 15)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(52, 16)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(52, 17)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(52, 18)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(52, 19)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(52, 20)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(52, 21)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(52, 22)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(52, 23)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(52, 24)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(52, 25)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(52, 26)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(52, 27)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(52, 28)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(52, 29)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(52, 30)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(52, 31)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(52, 32)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(52, 33)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(52, 34)",
           "CKA: 0.24<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(52, 35)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(52, 36)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(52, 37)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(52, 38)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(52, 39)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(52, 40)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(52, 41)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(52, 42)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(52, 43)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(52, 44)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(52, 45)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(52, 46)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(52, 47)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(52, 48)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(52, 49)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(52, 50)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(52, 51)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(52, 52)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(52, 53)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(52, 54)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(52, 55)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(52, 56)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(52, 57)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(52, 58)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(52, 59)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(52, 60)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(52, 61)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(52, 62)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(52, 63)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(52, 64)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(52, 65)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(52, 66)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(52, 67)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(52, 68)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(52, 69)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(52, 70)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(52, 71)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(52, 72)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(52, 73)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(52, 74)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(52, 75)",
           "CKA: 0.385<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(52, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(52, 77)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(52, 78)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(52, 79)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(52, 80)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(52, 81)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(52, 82)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(52, 83)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(52, 84)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(52, 85)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(52, 86)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(52, 87)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(52, 88)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(52, 89)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(52, 90)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(52, 91)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(52, 92)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(52, 93)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(52, 94)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(52, 95)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(52, 96)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(52, 97)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(52, 98)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(52, 99)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(52, 100)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(52, 101)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(52, 102)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(52, 103)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: classifier.dropout<br>(52, 104)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: classifier.dense<br>(52, 105)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(52, 106)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.3.attention.output.dropout<br>Pretrained: classifier<br>(52, 107)"
          ],
          [
           "CKA: 0.777<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(53, 0)",
           "CKA: -3.32e-11<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(53, 1)",
           "CKA: 0.0123<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(53, 2)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(53, 3)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(53, 4)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(53, 5)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(53, 6)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(53, 7)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(53, 8)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(53, 9)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(53, 10)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(53, 11)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(53, 12)",
           "CKA: 0.785<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(53, 13)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(53, 14)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(53, 15)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(53, 16)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(53, 17)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(53, 18)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(53, 19)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(53, 20)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(53, 21)",
           "CKA: 0.737<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(53, 22)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(53, 23)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(53, 24)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(53, 25)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(53, 26)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(53, 27)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(53, 28)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(53, 29)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(53, 30)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(53, 31)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(53, 32)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(53, 33)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(53, 34)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(53, 35)",
           "CKA: 0.777<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(53, 36)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(53, 37)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(53, 38)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(53, 39)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(53, 40)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(53, 41)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(53, 42)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(53, 43)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(53, 44)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(53, 45)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(53, 46)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(53, 47)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(53, 48)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(53, 49)",
           "CKA: 0.785<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(53, 50)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(53, 51)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(53, 52)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(53, 53)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(53, 54)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(53, 55)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(53, 56)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(53, 57)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(53, 58)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(53, 59)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(53, 60)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(53, 61)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(53, 62)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(53, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(53, 64)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(53, 65)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(53, 66)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(53, 67)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(53, 68)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(53, 69)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(53, 70)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(53, 71)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(53, 72)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(53, 73)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(53, 74)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(53, 75)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(53, 76)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(53, 77)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(53, 78)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(53, 79)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(53, 80)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(53, 81)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(53, 82)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(53, 83)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(53, 84)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(53, 85)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(53, 86)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(53, 87)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(53, 88)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(53, 89)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(53, 90)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(53, 91)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(53, 92)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(53, 93)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(53, 94)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(53, 95)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(53, 96)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(53, 97)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(53, 98)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(53, 99)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(53, 100)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(53, 101)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(53, 102)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(53, 103)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(53, 104)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(53, 105)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(53, 106)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.3.attention.output.LayerNorm<br>Pretrained: classifier<br>(53, 107)"
          ],
          [
           "CKA: 0.777<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(54, 0)",
           "CKA: -3.32e-11<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(54, 1)",
           "CKA: 0.0123<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(54, 2)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(54, 3)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(54, 4)",
           "CKA: 0.793<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.embeddings<br>(54, 5)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(54, 6)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(54, 7)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(54, 8)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(54, 9)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(54, 10)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(54, 11)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(54, 12)",
           "CKA: 0.785<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(54, 13)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(54, 14)",
           "CKA: 0.801<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(54, 15)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(54, 16)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(54, 17)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(54, 18)",
           "CKA: 0.82<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(54, 19)",
           "CKA: 0.733<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(54, 20)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(54, 21)",
           "CKA: 0.737<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(54, 22)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(54, 23)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(54, 24)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(54, 25)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(54, 26)",
           "CKA: 0.834<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(54, 27)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(54, 28)",
           "CKA: 0.841<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(54, 29)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(54, 30)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(54, 31)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(54, 32)",
           "CKA: 0.838<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(54, 33)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(54, 34)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(54, 35)",
           "CKA: 0.777<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(54, 36)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(54, 37)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(54, 38)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(54, 39)",
           "CKA: 0.825<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(54, 40)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(54, 41)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(54, 42)",
           "CKA: 0.843<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(54, 43)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(54, 44)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(54, 45)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(54, 46)",
           "CKA: 0.831<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(54, 47)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(54, 48)",
           "CKA: 0.746<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(54, 49)",
           "CKA: 0.785<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(54, 50)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(54, 51)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(54, 52)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(54, 53)",
           "CKA: 0.789<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(54, 54)",
           "CKA: 0.742<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(54, 55)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(54, 56)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(54, 57)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(54, 58)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(54, 59)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(54, 60)",
           "CKA: 0.784<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(54, 61)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(54, 62)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(54, 63)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(54, 64)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(54, 65)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(54, 66)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(54, 67)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(54, 68)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(54, 69)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(54, 70)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(54, 71)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(54, 72)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(54, 73)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(54, 74)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(54, 75)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(54, 76)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(54, 77)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(54, 78)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(54, 79)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(54, 80)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(54, 81)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(54, 82)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(54, 83)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(54, 84)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(54, 85)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(54, 86)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(54, 87)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(54, 88)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(54, 89)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(54, 90)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(54, 91)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(54, 92)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(54, 93)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(54, 94)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(54, 95)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(54, 96)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(54, 97)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(54, 98)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(54, 99)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(54, 100)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(54, 101)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(54, 102)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(54, 103)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: classifier.dropout<br>(54, 104)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: classifier.dense<br>(54, 105)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: classifier.out_proj<br>(54, 106)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.3.attention.output<br>Pretrained: classifier<br>(54, 107)"
          ],
          [
           "CKA: 0.696<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(55, 0)",
           "CKA: -3.8e-11<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(55, 1)",
           "CKA: 0.00808<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(55, 2)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(55, 3)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(55, 4)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.embeddings<br>(55, 5)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(55, 6)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(55, 7)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(55, 8)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(55, 9)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(55, 10)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(55, 11)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(55, 12)",
           "CKA: 0.71<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(55, 13)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(55, 14)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(55, 15)",
           "CKA: 0.671<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(55, 16)",
           "CKA: 0.671<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(55, 17)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(55, 18)",
           "CKA: 0.736<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(55, 19)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(55, 20)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(55, 21)",
           "CKA: 0.672<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(55, 22)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(55, 23)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(55, 24)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(55, 25)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(55, 26)",
           "CKA: 0.758<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(55, 27)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(55, 28)",
           "CKA: 0.764<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(55, 29)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(55, 30)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(55, 31)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(55, 32)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(55, 33)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(55, 34)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(55, 35)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(55, 36)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(55, 37)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(55, 38)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(55, 39)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(55, 40)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(55, 41)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(55, 42)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(55, 43)",
           "CKA: 0.739<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(55, 44)",
           "CKA: 0.739<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(55, 45)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(55, 46)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(55, 47)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(55, 48)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(55, 49)",
           "CKA: 0.722<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(55, 50)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(55, 51)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(55, 52)",
           "CKA: 0.715<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(55, 53)",
           "CKA: 0.715<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(55, 54)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(55, 55)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(55, 56)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(55, 57)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(55, 58)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(55, 59)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(55, 60)",
           "CKA: 0.707<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(55, 61)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(55, 62)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(55, 63)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(55, 64)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(55, 65)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(55, 66)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(55, 67)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(55, 68)",
           "CKA: 0.591<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(55, 69)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(55, 70)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(55, 71)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(55, 72)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(55, 73)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(55, 74)",
           "CKA: 0.633<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(55, 75)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(55, 76)",
           "CKA: 0.54<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(55, 77)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(55, 78)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(55, 79)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(55, 80)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(55, 81)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(55, 82)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(55, 83)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(55, 84)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(55, 85)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(55, 86)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(55, 87)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(55, 88)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(55, 89)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(55, 90)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(55, 91)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(55, 92)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(55, 93)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(55, 94)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(55, 95)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(55, 96)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(55, 97)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(55, 98)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(55, 99)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(55, 100)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(55, 101)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(55, 102)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(55, 103)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: classifier.dropout<br>(55, 104)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: classifier.dense<br>(55, 105)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: classifier.out_proj<br>(55, 106)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.3.intermediate.dense<br>Pretrained: classifier<br>(55, 107)"
          ],
          [
           "CKA: 0.717<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(56, 0)",
           "CKA: -1.08e-11<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(56, 1)",
           "CKA: 0.00768<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(56, 2)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(56, 3)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(56, 4)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(56, 5)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(56, 6)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(56, 7)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(56, 8)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(56, 9)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(56, 10)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(56, 11)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(56, 12)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(56, 13)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(56, 14)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(56, 15)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(56, 16)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(56, 17)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(56, 18)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(56, 19)",
           "CKA: 0.671<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(56, 20)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(56, 21)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(56, 22)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(56, 23)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(56, 24)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(56, 25)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(56, 26)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(56, 27)",
           "CKA: 0.78<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(56, 28)",
           "CKA: 0.78<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(56, 29)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(56, 30)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(56, 31)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(56, 32)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(56, 33)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(56, 34)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(56, 35)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(56, 36)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(56, 37)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(56, 38)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(56, 39)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(56, 40)",
           "CKA: 0.772<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(56, 41)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(56, 42)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(56, 43)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(56, 44)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(56, 45)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(56, 46)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(56, 47)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(56, 48)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(56, 49)",
           "CKA: 0.738<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(56, 50)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(56, 51)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(56, 52)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(56, 53)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(56, 54)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(56, 55)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(56, 56)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(56, 57)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(56, 58)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(56, 59)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(56, 60)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(56, 61)",
           "CKA: 0.508<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(56, 62)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(56, 63)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(56, 64)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(56, 65)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(56, 66)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(56, 67)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(56, 68)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(56, 69)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(56, 70)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(56, 71)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(56, 72)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(56, 73)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(56, 74)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(56, 75)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(56, 76)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(56, 77)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(56, 78)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(56, 79)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(56, 80)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(56, 81)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(56, 82)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(56, 83)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(56, 84)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(56, 85)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(56, 86)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(56, 87)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(56, 88)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(56, 89)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(56, 90)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(56, 91)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(56, 92)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(56, 93)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(56, 94)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(56, 95)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(56, 96)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(56, 97)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(56, 98)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(56, 99)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(56, 100)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(56, 101)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(56, 102)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(56, 103)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(56, 104)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(56, 105)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(56, 106)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(56, 107)"
          ],
          [
           "CKA: 0.717<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(57, 0)",
           "CKA: -1.08e-11<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(57, 1)",
           "CKA: 0.00768<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(57, 2)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(57, 3)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(57, 4)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.embeddings<br>(57, 5)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(57, 6)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(57, 7)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(57, 8)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(57, 9)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(57, 10)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(57, 11)",
           "CKA: 0.743<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(57, 12)",
           "CKA: 0.729<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(57, 13)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(57, 14)",
           "CKA: 0.74<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(57, 15)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(57, 16)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(57, 17)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(57, 18)",
           "CKA: 0.757<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(57, 19)",
           "CKA: 0.671<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(57, 20)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(57, 21)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(57, 22)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(57, 23)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(57, 24)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(57, 25)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(57, 26)",
           "CKA: 0.773<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(57, 27)",
           "CKA: 0.78<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(57, 28)",
           "CKA: 0.78<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(57, 29)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(57, 30)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(57, 31)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(57, 32)",
           "CKA: 0.774<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(57, 33)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(57, 34)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(57, 35)",
           "CKA: 0.713<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(57, 36)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(57, 37)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(57, 38)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(57, 39)",
           "CKA: 0.767<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(57, 40)",
           "CKA: 0.772<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(57, 41)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(57, 42)",
           "CKA: 0.788<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(57, 43)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(57, 44)",
           "CKA: 0.752<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(57, 45)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(57, 46)",
           "CKA: 0.769<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(57, 47)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(57, 48)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(57, 49)",
           "CKA: 0.738<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(57, 50)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(57, 51)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(57, 52)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(57, 53)",
           "CKA: 0.719<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(57, 54)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(57, 55)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(57, 56)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(57, 57)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(57, 58)",
           "CKA: 0.641<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(57, 59)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(57, 60)",
           "CKA: 0.711<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(57, 61)",
           "CKA: 0.508<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(57, 62)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(57, 63)",
           "CKA: 0.685<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(57, 64)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(57, 65)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(57, 66)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(57, 67)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(57, 68)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(57, 69)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(57, 70)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(57, 71)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(57, 72)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(57, 73)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(57, 74)",
           "CKA: 0.627<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(57, 75)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(57, 76)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(57, 77)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(57, 78)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(57, 79)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(57, 80)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(57, 81)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(57, 82)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(57, 83)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(57, 84)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(57, 85)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(57, 86)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(57, 87)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(57, 88)",
           "CKA: 0.505<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(57, 89)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(57, 90)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(57, 91)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(57, 92)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(57, 93)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(57, 94)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(57, 95)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(57, 96)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(57, 97)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(57, 98)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(57, 99)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(57, 100)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(57, 101)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(57, 102)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(57, 103)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: classifier.dropout<br>(57, 104)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: classifier.dense<br>(57, 105)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: classifier.out_proj<br>(57, 106)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.3.intermediate<br>Pretrained: classifier<br>(57, 107)"
          ],
          [
           "CKA: 0.584<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(58, 0)",
           "CKA: -1.19e-11<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(58, 1)",
           "CKA: 0.00343<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(58, 2)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(58, 3)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(58, 4)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.embeddings<br>(58, 5)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(58, 6)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(58, 7)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(58, 8)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(58, 9)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(58, 10)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(58, 11)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(58, 12)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(58, 13)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(58, 14)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(58, 15)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(58, 16)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(58, 17)",
           "CKA: 0.61<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(58, 18)",
           "CKA: 0.61<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(58, 19)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(58, 20)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(58, 21)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(58, 22)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(58, 23)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(58, 24)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(58, 25)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(58, 26)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(58, 27)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(58, 28)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(58, 29)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(58, 30)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(58, 31)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(58, 32)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(58, 33)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(58, 34)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(58, 35)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(58, 36)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(58, 37)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(58, 38)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(58, 39)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(58, 40)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(58, 41)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(58, 42)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(58, 43)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(58, 44)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(58, 45)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(58, 46)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(58, 47)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(58, 48)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(58, 49)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(58, 50)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(58, 51)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(58, 52)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(58, 53)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(58, 54)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(58, 55)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(58, 56)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(58, 57)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(58, 58)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(58, 59)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(58, 60)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(58, 61)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(58, 62)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(58, 63)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(58, 64)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(58, 65)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(58, 66)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(58, 67)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(58, 68)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(58, 69)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(58, 70)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(58, 71)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(58, 72)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(58, 73)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(58, 74)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(58, 75)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(58, 76)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(58, 77)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(58, 78)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(58, 79)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(58, 80)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(58, 81)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(58, 82)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(58, 83)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(58, 84)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(58, 85)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(58, 86)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(58, 87)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(58, 88)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(58, 89)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(58, 90)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(58, 91)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(58, 92)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(58, 93)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(58, 94)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(58, 95)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(58, 96)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(58, 97)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(58, 98)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(58, 99)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(58, 100)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(58, 101)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(58, 102)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(58, 103)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: classifier.dropout<br>(58, 104)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: classifier.dense<br>(58, 105)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: classifier.out_proj<br>(58, 106)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.3.output.dense<br>Pretrained: classifier<br>(58, 107)"
          ],
          [
           "CKA: 0.584<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(59, 0)",
           "CKA: -1.19e-11<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(59, 1)",
           "CKA: 0.00343<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(59, 2)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(59, 3)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(59, 4)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.embeddings<br>(59, 5)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(59, 6)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(59, 7)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(59, 8)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(59, 9)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(59, 10)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(59, 11)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(59, 12)",
           "CKA: 0.601<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(59, 13)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(59, 14)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(59, 15)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(59, 16)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(59, 17)",
           "CKA: 0.61<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(59, 18)",
           "CKA: 0.61<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(59, 19)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(59, 20)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(59, 21)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(59, 22)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(59, 23)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(59, 24)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(59, 25)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(59, 26)",
           "CKA: 0.628<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(59, 27)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(59, 28)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(59, 29)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(59, 30)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(59, 31)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(59, 32)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(59, 33)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(59, 34)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(59, 35)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(59, 36)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(59, 37)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(59, 38)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(59, 39)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(59, 40)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(59, 41)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(59, 42)",
           "CKA: 0.653<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(59, 43)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(59, 44)",
           "CKA: 0.62<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(59, 45)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(59, 46)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(59, 47)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(59, 48)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(59, 49)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(59, 50)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(59, 51)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(59, 52)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(59, 53)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(59, 54)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(59, 55)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(59, 56)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(59, 57)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(59, 58)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(59, 59)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(59, 60)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(59, 61)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(59, 62)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(59, 63)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(59, 64)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(59, 65)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(59, 66)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(59, 67)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(59, 68)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(59, 69)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(59, 70)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(59, 71)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(59, 72)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(59, 73)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(59, 74)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(59, 75)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(59, 76)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(59, 77)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(59, 78)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(59, 79)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(59, 80)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(59, 81)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(59, 82)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(59, 83)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(59, 84)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(59, 85)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(59, 86)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(59, 87)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(59, 88)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(59, 89)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(59, 90)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(59, 91)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(59, 92)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(59, 93)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(59, 94)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(59, 95)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(59, 96)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(59, 97)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(59, 98)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(59, 99)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(59, 100)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(59, 101)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(59, 102)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(59, 103)",
           "CKA: 0.168<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: classifier.dropout<br>(59, 104)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: classifier.dense<br>(59, 105)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: classifier.out_proj<br>(59, 106)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.3.output.dropout<br>Pretrained: classifier<br>(59, 107)"
          ],
          [
           "CKA: 0.749<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(60, 0)",
           "CKA: -4.63e-11<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(60, 1)",
           "CKA: 0.0116<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(60, 2)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(60, 3)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(60, 4)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(60, 5)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(60, 6)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(60, 7)",
           "CKA: 0.715<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(60, 8)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(60, 9)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(60, 10)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(60, 11)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(60, 12)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(60, 13)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(60, 14)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(60, 15)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(60, 16)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(60, 17)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(60, 18)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(60, 19)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(60, 20)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(60, 21)",
           "CKA: 0.716<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(60, 22)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(60, 23)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(60, 24)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(60, 25)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(60, 26)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(60, 27)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(60, 28)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(60, 29)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(60, 30)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(60, 31)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(60, 32)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(60, 33)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(60, 34)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(60, 35)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(60, 36)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(60, 37)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(60, 38)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(60, 39)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(60, 40)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(60, 41)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(60, 42)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(60, 43)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(60, 44)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(60, 45)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(60, 46)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(60, 47)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(60, 48)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(60, 49)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(60, 50)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(60, 51)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(60, 52)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(60, 53)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(60, 54)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(60, 55)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(60, 56)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(60, 57)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(60, 58)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(60, 59)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(60, 60)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(60, 61)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(60, 62)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(60, 63)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(60, 64)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(60, 65)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(60, 66)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(60, 67)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(60, 68)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(60, 69)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(60, 70)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(60, 71)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(60, 72)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(60, 73)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(60, 74)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(60, 75)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(60, 76)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(60, 77)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(60, 78)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(60, 79)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(60, 80)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(60, 81)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(60, 82)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(60, 83)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(60, 84)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(60, 85)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(60, 86)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(60, 87)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(60, 88)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(60, 89)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(60, 90)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(60, 91)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(60, 92)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(60, 93)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(60, 94)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(60, 95)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(60, 96)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(60, 97)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(60, 98)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(60, 99)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(60, 100)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(60, 101)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(60, 102)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(60, 103)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: classifier.dropout<br>(60, 104)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: classifier.dense<br>(60, 105)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(60, 106)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.output.LayerNorm<br>Pretrained: classifier<br>(60, 107)"
          ],
          [
           "CKA: 0.749<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(61, 0)",
           "CKA: -4.63e-11<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(61, 1)",
           "CKA: 0.0116<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(61, 2)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(61, 3)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings.dropout<br>(61, 4)",
           "CKA: 0.765<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.embeddings<br>(61, 5)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(61, 6)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(61, 7)",
           "CKA: 0.715<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(61, 8)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(61, 9)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(61, 10)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(61, 11)",
           "CKA: 0.775<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(61, 12)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(61, 13)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(61, 14)",
           "CKA: 0.771<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(61, 15)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(61, 16)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(61, 17)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(61, 18)",
           "CKA: 0.79<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.0.output<br>(61, 19)",
           "CKA: 0.704<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(61, 20)",
           "CKA: 0.647<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(61, 21)",
           "CKA: 0.716<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(61, 22)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(61, 23)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(61, 24)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(61, 25)",
           "CKA: 0.805<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(61, 26)",
           "CKA: 0.803<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(61, 27)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(61, 28)",
           "CKA: 0.809<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(61, 29)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(61, 30)",
           "CKA: 0.763<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(61, 31)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(61, 32)",
           "CKA: 0.806<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.1.output<br>(61, 33)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(61, 34)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(61, 35)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(61, 36)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(61, 37)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(61, 38)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(61, 39)",
           "CKA: 0.796<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(61, 40)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(61, 41)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(61, 42)",
           "CKA: 0.813<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(61, 43)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(61, 44)",
           "CKA: 0.776<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(61, 45)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(61, 46)",
           "CKA: 0.8<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.2.output<br>(61, 47)",
           "CKA: 0.669<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(61, 48)",
           "CKA: 0.718<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(61, 49)",
           "CKA: 0.755<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(61, 50)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(61, 51)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(61, 52)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(61, 53)",
           "CKA: 0.753<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(61, 54)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(61, 55)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(61, 56)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(61, 57)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(61, 58)",
           "CKA: 0.682<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(61, 59)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(61, 60)",
           "CKA: 0.747<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.3.output<br>(61, 61)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(61, 62)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(61, 63)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(61, 64)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(61, 65)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(61, 66)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(61, 67)",
           "CKA: 0.69<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(61, 68)",
           "CKA: 0.614<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(61, 69)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(61, 70)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(61, 71)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(61, 72)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(61, 73)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(61, 74)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.4.output<br>(61, 75)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(61, 76)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(61, 77)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(61, 78)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(61, 79)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(61, 80)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(61, 81)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(61, 82)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(61, 83)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(61, 84)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(61, 85)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(61, 86)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(61, 87)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(61, 88)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.5.output<br>(61, 89)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(61, 90)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(61, 91)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(61, 92)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(61, 93)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(61, 94)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(61, 95)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(61, 96)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(61, 97)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(61, 98)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(61, 99)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(61, 100)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(61, 101)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(61, 102)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.3.output<br>Pretrained: roberta.encoder.layer.6.output<br>(61, 103)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.3.output<br>Pretrained: classifier.dropout<br>(61, 104)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.3.output<br>Pretrained: classifier.dense<br>(61, 105)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.output<br>Pretrained: classifier.out_proj<br>(61, 106)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.3.output<br>Pretrained: classifier<br>(61, 107)"
          ],
          [
           "CKA: 0.455<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(62, 0)",
           "CKA: -1.11e-11<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(62, 1)",
           "CKA: 0.00398<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(62, 2)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(62, 3)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(62, 4)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.embeddings<br>(62, 5)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(62, 6)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(62, 7)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(62, 8)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(62, 9)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(62, 10)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(62, 11)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(62, 12)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(62, 13)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(62, 14)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(62, 15)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(62, 16)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(62, 17)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(62, 18)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(62, 19)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(62, 20)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(62, 21)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(62, 22)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(62, 23)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(62, 24)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(62, 25)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(62, 26)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(62, 27)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(62, 28)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(62, 29)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(62, 30)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(62, 31)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(62, 32)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(62, 33)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(62, 34)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(62, 35)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(62, 36)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(62, 37)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(62, 38)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(62, 39)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(62, 40)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(62, 41)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(62, 42)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(62, 43)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(62, 44)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(62, 45)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(62, 46)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(62, 47)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(62, 48)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(62, 49)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(62, 50)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(62, 51)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(62, 52)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(62, 53)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(62, 54)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(62, 55)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(62, 56)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(62, 57)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(62, 58)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(62, 59)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(62, 60)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(62, 61)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(62, 62)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(62, 63)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(62, 64)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(62, 65)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(62, 66)",
           "CKA: 0.542<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(62, 67)",
           "CKA: 0.542<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(62, 68)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(62, 69)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(62, 70)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(62, 71)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(62, 72)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(62, 73)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(62, 74)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(62, 75)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(62, 76)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(62, 77)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(62, 78)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(62, 79)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(62, 80)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(62, 81)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(62, 82)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(62, 83)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(62, 84)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(62, 85)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(62, 86)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(62, 87)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(62, 88)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(62, 89)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(62, 90)",
           "CKA: 0.446<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(62, 91)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(62, 92)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(62, 93)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(62, 94)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(62, 95)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(62, 96)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(62, 97)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(62, 98)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(62, 99)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(62, 100)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(62, 101)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(62, 102)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(62, 103)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: classifier.dropout<br>(62, 104)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: classifier.dense<br>(62, 105)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: classifier.out_proj<br>(62, 106)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.4.attention.self.query<br>Pretrained: classifier<br>(62, 107)"
          ],
          [
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(63, 0)",
           "CKA: -1.82e-11<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(63, 1)",
           "CKA: 0.00716<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(63, 2)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(63, 3)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(63, 4)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.embeddings<br>(63, 5)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(63, 6)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(63, 7)",
           "CKA: 0.558<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(63, 8)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(63, 9)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(63, 10)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(63, 11)",
           "CKA: 0.575<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(63, 12)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(63, 13)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(63, 14)",
           "CKA: 0.605<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(63, 15)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(63, 16)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(63, 17)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(63, 18)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(63, 19)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(63, 20)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(63, 21)",
           "CKA: 0.597<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(63, 22)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(63, 23)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(63, 24)",
           "CKA: 0.594<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(63, 25)",
           "CKA: 0.594<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(63, 26)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(63, 27)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(63, 28)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(63, 29)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(63, 30)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(63, 31)",
           "CKA: 0.598<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(63, 32)",
           "CKA: 0.598<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(63, 33)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(63, 34)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(63, 35)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(63, 36)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(63, 37)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(63, 38)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(63, 39)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(63, 40)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(63, 41)",
           "CKA: 0.624<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(63, 42)",
           "CKA: 0.624<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(63, 43)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(63, 44)",
           "CKA: 0.619<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(63, 45)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(63, 46)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(63, 47)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(63, 48)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(63, 49)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(63, 50)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(63, 51)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(63, 52)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(63, 53)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(63, 54)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(63, 55)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(63, 56)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(63, 57)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(63, 58)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(63, 59)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(63, 60)",
           "CKA: 0.53<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(63, 61)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(63, 62)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(63, 63)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(63, 64)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(63, 65)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(63, 66)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(63, 67)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(63, 68)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(63, 69)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(63, 70)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(63, 71)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(63, 72)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(63, 73)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(63, 74)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(63, 75)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(63, 76)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(63, 77)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(63, 78)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(63, 79)",
           "CKA: 0.193<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(63, 80)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(63, 81)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(63, 82)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(63, 83)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(63, 84)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(63, 85)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(63, 86)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(63, 87)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(63, 88)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(63, 89)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(63, 90)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(63, 91)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(63, 92)",
           "CKA: 0.14<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(63, 93)",
           "CKA: 0.14<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(63, 94)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(63, 95)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(63, 96)",
           "CKA: 0.184<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(63, 97)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(63, 98)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(63, 99)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(63, 100)",
           "CKA: 0.126<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(63, 101)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(63, 102)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(63, 103)",
           "CKA: 0.11<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: classifier.dropout<br>(63, 104)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: classifier.dense<br>(63, 105)",
           "CKA: 0.0841<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: classifier.out_proj<br>(63, 106)",
           "CKA: 0.0841<br>Base: roberta.encoder.layer.4.attention.self.key<br>Pretrained: classifier<br>(63, 107)"
          ],
          [
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(64, 0)",
           "CKA: -1.44e-11<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(64, 1)",
           "CKA: 0.00471<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(64, 2)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(64, 3)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(64, 4)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.embeddings<br>(64, 5)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(64, 6)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(64, 7)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(64, 8)",
           "CKA: 0.188<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(64, 9)",
           "CKA: 0.188<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(64, 10)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(64, 11)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(64, 12)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(64, 13)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(64, 14)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(64, 15)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(64, 16)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(64, 17)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(64, 18)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(64, 19)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(64, 20)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(64, 21)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(64, 22)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(64, 23)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(64, 24)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(64, 25)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(64, 26)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(64, 27)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(64, 28)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(64, 29)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(64, 30)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(64, 31)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(64, 32)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(64, 33)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(64, 34)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(64, 35)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(64, 36)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(64, 37)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(64, 38)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(64, 39)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(64, 40)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(64, 41)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(64, 42)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(64, 43)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(64, 44)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(64, 45)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(64, 46)",
           "CKA: 0.523<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(64, 47)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(64, 48)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(64, 49)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(64, 50)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(64, 51)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(64, 52)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(64, 53)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(64, 54)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(64, 55)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(64, 56)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(64, 57)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(64, 58)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(64, 59)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(64, 60)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(64, 61)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(64, 62)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(64, 63)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(64, 64)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(64, 65)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(64, 66)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(64, 67)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(64, 68)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(64, 69)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(64, 70)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(64, 71)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(64, 72)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(64, 73)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(64, 74)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(64, 75)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(64, 76)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(64, 77)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(64, 78)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(64, 79)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(64, 80)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(64, 81)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(64, 82)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(64, 83)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(64, 84)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(64, 85)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(64, 86)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(64, 87)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(64, 88)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(64, 89)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(64, 90)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(64, 91)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(64, 92)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(64, 93)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(64, 94)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(64, 95)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(64, 96)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(64, 97)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(64, 98)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(64, 99)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(64, 100)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(64, 101)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(64, 102)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(64, 103)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: classifier.dropout<br>(64, 104)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: classifier.dense<br>(64, 105)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: classifier.out_proj<br>(64, 106)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.4.attention.self.value<br>Pretrained: classifier<br>(64, 107)"
          ],
          [
           "CKA: 0.165<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(65, 0)",
           "CKA: -6.47e-13<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(65, 1)",
           "CKA: 0.0038<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(65, 2)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(65, 3)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(65, 4)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.embeddings<br>(65, 5)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(65, 6)",
           "CKA: 0.135<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(65, 7)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(65, 8)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(65, 9)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(65, 10)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(65, 11)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(65, 12)",
           "CKA: 0.165<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(65, 13)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(65, 14)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(65, 15)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(65, 16)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(65, 17)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(65, 18)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(65, 19)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(65, 20)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(65, 21)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(65, 22)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(65, 23)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(65, 24)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(65, 25)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(65, 26)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(65, 27)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(65, 28)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(65, 29)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(65, 30)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(65, 31)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(65, 32)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(65, 33)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(65, 34)",
           "CKA: 0.192<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(65, 35)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(65, 36)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(65, 37)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(65, 38)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(65, 39)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(65, 40)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(65, 41)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(65, 42)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(65, 43)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(65, 44)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(65, 45)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(65, 46)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(65, 47)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(65, 48)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(65, 49)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(65, 50)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(65, 51)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(65, 52)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(65, 53)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(65, 54)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(65, 55)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(65, 56)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(65, 57)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(65, 58)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(65, 59)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(65, 60)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(65, 61)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(65, 62)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(65, 63)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(65, 64)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(65, 65)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(65, 66)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(65, 67)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(65, 68)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(65, 69)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(65, 70)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(65, 71)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(65, 72)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(65, 73)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(65, 74)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(65, 75)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(65, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(65, 77)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(65, 78)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(65, 79)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(65, 80)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(65, 81)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(65, 82)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(65, 83)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(65, 84)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(65, 85)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(65, 86)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(65, 87)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(65, 88)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(65, 89)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(65, 90)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(65, 91)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(65, 92)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(65, 93)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(65, 94)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(65, 95)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(65, 96)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(65, 97)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(65, 98)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(65, 99)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(65, 100)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(65, 101)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(65, 102)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(65, 103)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: classifier.dropout<br>(65, 104)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: classifier.dense<br>(65, 105)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: classifier.out_proj<br>(65, 106)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.attention.output.dense<br>Pretrained: classifier<br>(65, 107)"
          ],
          [
           "CKA: 0.165<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(66, 0)",
           "CKA: -6.47e-13<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(66, 1)",
           "CKA: 0.0038<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(66, 2)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(66, 3)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(66, 4)",
           "CKA: 0.172<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(66, 5)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(66, 6)",
           "CKA: 0.135<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(66, 7)",
           "CKA: 0.164<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(66, 8)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(66, 9)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(66, 10)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(66, 11)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(66, 12)",
           "CKA: 0.165<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(66, 13)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(66, 14)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(66, 15)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(66, 16)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(66, 17)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(66, 18)",
           "CKA: 0.177<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(66, 19)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(66, 20)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(66, 21)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(66, 22)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(66, 23)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(66, 24)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(66, 25)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(66, 26)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(66, 27)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(66, 28)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(66, 29)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(66, 30)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(66, 31)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(66, 32)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(66, 33)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(66, 34)",
           "CKA: 0.192<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(66, 35)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(66, 36)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(66, 37)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(66, 38)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(66, 39)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(66, 40)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(66, 41)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(66, 42)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(66, 43)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(66, 44)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(66, 45)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(66, 46)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(66, 47)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(66, 48)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(66, 49)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(66, 50)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(66, 51)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(66, 52)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(66, 53)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(66, 54)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(66, 55)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(66, 56)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(66, 57)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(66, 58)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(66, 59)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(66, 60)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(66, 61)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(66, 62)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(66, 63)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(66, 64)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(66, 65)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(66, 66)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(66, 67)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(66, 68)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(66, 69)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(66, 70)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(66, 71)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(66, 72)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(66, 73)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(66, 74)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(66, 75)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(66, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(66, 77)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(66, 78)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(66, 79)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(66, 80)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(66, 81)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(66, 82)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(66, 83)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(66, 84)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(66, 85)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(66, 86)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(66, 87)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(66, 88)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(66, 89)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(66, 90)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(66, 91)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(66, 92)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(66, 93)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(66, 94)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(66, 95)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(66, 96)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(66, 97)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(66, 98)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(66, 99)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(66, 100)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(66, 101)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(66, 102)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(66, 103)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: classifier.dropout<br>(66, 104)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: classifier.dense<br>(66, 105)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(66, 106)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.attention.output.dropout<br>Pretrained: classifier<br>(66, 107)"
          ],
          [
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(67, 0)",
           "CKA: -4.27e-11<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(67, 1)",
           "CKA: 0.0105<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(67, 2)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(67, 3)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(67, 4)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(67, 5)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(67, 6)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(67, 7)",
           "CKA: 0.66<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(67, 8)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(67, 9)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(67, 10)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(67, 11)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(67, 12)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(67, 13)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(67, 14)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(67, 15)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(67, 16)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(67, 17)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(67, 18)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(67, 19)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(67, 20)",
           "CKA: 0.591<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(67, 21)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(67, 22)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(67, 23)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(67, 24)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(67, 25)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(67, 26)",
           "CKA: 0.749<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(67, 27)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(67, 28)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(67, 29)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(67, 30)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(67, 31)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(67, 32)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(67, 33)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(67, 34)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(67, 35)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(67, 36)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(67, 37)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(67, 38)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(67, 39)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(67, 40)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(67, 41)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(67, 42)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(67, 43)",
           "CKA: 0.717<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(67, 44)",
           "CKA: 0.717<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(67, 45)",
           "CKA: 0.76<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(67, 46)",
           "CKA: 0.76<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(67, 47)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(67, 48)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(67, 49)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(67, 50)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(67, 51)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(67, 52)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(67, 53)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(67, 54)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(67, 55)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(67, 56)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(67, 57)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(67, 58)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(67, 59)",
           "CKA: 0.724<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(67, 60)",
           "CKA: 0.724<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(67, 61)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(67, 62)",
           "CKA: 0.594<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(67, 63)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(67, 64)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(67, 65)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(67, 66)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(67, 67)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(67, 68)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(67, 69)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(67, 70)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(67, 71)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(67, 72)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(67, 73)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(67, 74)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(67, 75)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(67, 76)",
           "CKA: 0.565<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(67, 77)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(67, 78)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(67, 79)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(67, 80)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(67, 81)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(67, 82)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(67, 83)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(67, 84)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(67, 85)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(67, 86)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(67, 87)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(67, 88)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(67, 89)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(67, 90)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(67, 91)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(67, 92)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(67, 93)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(67, 94)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(67, 95)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(67, 96)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(67, 97)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(67, 98)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(67, 99)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(67, 100)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(67, 101)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(67, 102)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(67, 103)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(67, 104)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(67, 105)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(67, 106)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.4.attention.output.LayerNorm<br>Pretrained: classifier<br>(67, 107)"
          ],
          [
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(68, 0)",
           "CKA: -4.27e-11<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(68, 1)",
           "CKA: 0.0105<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(68, 2)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(68, 3)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(68, 4)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.embeddings<br>(68, 5)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(68, 6)",
           "CKA: 0.574<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(68, 7)",
           "CKA: 0.66<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(68, 8)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(68, 9)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(68, 10)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(68, 11)",
           "CKA: 0.72<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(68, 12)",
           "CKA: 0.7<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(68, 13)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(68, 14)",
           "CKA: 0.708<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(68, 15)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(68, 16)",
           "CKA: 0.658<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(68, 17)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(68, 18)",
           "CKA: 0.731<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(68, 19)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(68, 20)",
           "CKA: 0.591<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(68, 21)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(68, 22)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(68, 23)",
           "CKA: 0.495<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(68, 24)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(68, 25)",
           "CKA: 0.756<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(68, 26)",
           "CKA: 0.749<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(68, 27)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(68, 28)",
           "CKA: 0.75<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(68, 29)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(68, 30)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(68, 31)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(68, 32)",
           "CKA: 0.754<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(68, 33)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(68, 34)",
           "CKA: 0.68<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(68, 35)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(68, 36)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(68, 37)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(68, 38)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(68, 39)",
           "CKA: 0.761<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(68, 40)",
           "CKA: 0.759<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(68, 41)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(68, 42)",
           "CKA: 0.762<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(68, 43)",
           "CKA: 0.717<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(68, 44)",
           "CKA: 0.717<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(68, 45)",
           "CKA: 0.76<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(68, 46)",
           "CKA: 0.76<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(68, 47)",
           "CKA: 0.638<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(68, 48)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(68, 49)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(68, 50)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(68, 51)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(68, 52)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(68, 53)",
           "CKA: 0.732<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(68, 54)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(68, 55)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(68, 56)",
           "CKA: 0.699<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(68, 57)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(68, 58)",
           "CKA: 0.654<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(68, 59)",
           "CKA: 0.724<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(68, 60)",
           "CKA: 0.724<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(68, 61)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(68, 62)",
           "CKA: 0.594<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(68, 63)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(68, 64)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(68, 65)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(68, 66)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(68, 67)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(68, 68)",
           "CKA: 0.63<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(68, 69)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(68, 70)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(68, 71)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(68, 72)",
           "CKA: 0.629<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(68, 73)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(68, 74)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(68, 75)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(68, 76)",
           "CKA: 0.565<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(68, 77)",
           "CKA: 0.617<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(68, 78)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(68, 79)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(68, 80)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(68, 81)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(68, 82)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(68, 83)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(68, 84)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(68, 85)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(68, 86)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(68, 87)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(68, 88)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(68, 89)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(68, 90)",
           "CKA: 0.543<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(68, 91)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(68, 92)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(68, 93)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(68, 94)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(68, 95)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(68, 96)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(68, 97)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(68, 98)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(68, 99)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(68, 100)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(68, 101)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(68, 102)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(68, 103)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: classifier.dropout<br>(68, 104)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: classifier.dense<br>(68, 105)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: classifier.out_proj<br>(68, 106)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.4.attention.output<br>Pretrained: classifier<br>(68, 107)"
          ],
          [
           "CKA: 0.631<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(69, 0)",
           "CKA: -3.47e-11<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(69, 1)",
           "CKA: 0.00815<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(69, 2)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(69, 3)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(69, 4)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.embeddings<br>(69, 5)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(69, 6)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(69, 7)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(69, 8)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(69, 9)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(69, 10)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(69, 11)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(69, 12)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(69, 13)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(69, 14)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(69, 15)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(69, 16)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(69, 17)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(69, 18)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(69, 19)",
           "CKA: 0.599<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(69, 20)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(69, 21)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(69, 22)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(69, 23)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(69, 24)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(69, 25)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(69, 26)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(69, 27)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(69, 28)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(69, 29)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(69, 30)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(69, 31)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(69, 32)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(69, 33)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(69, 34)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(69, 35)",
           "CKA: 0.648<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(69, 36)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(69, 37)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(69, 38)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(69, 39)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(69, 40)",
           "CKA: 0.705<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(69, 41)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(69, 42)",
           "CKA: 0.709<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(69, 43)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(69, 44)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(69, 45)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(69, 46)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(69, 47)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(69, 48)",
           "CKA: 0.622<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(69, 49)",
           "CKA: 0.659<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(69, 50)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(69, 51)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(69, 52)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(69, 53)",
           "CKA: 0.681<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(69, 54)",
           "CKA: 0.645<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(69, 55)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(69, 56)",
           "CKA: 0.655<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(69, 57)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(69, 58)",
           "CKA: 0.612<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(69, 59)",
           "CKA: 0.675<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(69, 60)",
           "CKA: 0.675<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(69, 61)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(69, 62)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(69, 63)",
           "CKA: 0.666<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(69, 64)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(69, 65)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(69, 66)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(69, 67)",
           "CKA: 0.657<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(69, 68)",
           "CKA: 0.61<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(69, 69)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(69, 70)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(69, 71)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(69, 72)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(69, 73)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(69, 74)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(69, 75)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(69, 76)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(69, 77)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(69, 78)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(69, 79)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(69, 80)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(69, 81)",
           "CKA: 0.596<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(69, 82)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(69, 83)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(69, 84)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(69, 85)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(69, 86)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(69, 87)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(69, 88)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(69, 89)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(69, 90)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(69, 91)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(69, 92)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(69, 93)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(69, 94)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(69, 95)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(69, 96)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(69, 97)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(69, 98)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(69, 99)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(69, 100)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(69, 101)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(69, 102)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(69, 103)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: classifier.dropout<br>(69, 104)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: classifier.dense<br>(69, 105)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: classifier.out_proj<br>(69, 106)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.4.intermediate.dense<br>Pretrained: classifier<br>(69, 107)"
          ],
          [
           "CKA: 0.625<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(70, 0)",
           "CKA: -1.03e-11<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(70, 1)",
           "CKA: 0.00732<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(70, 2)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(70, 3)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(70, 4)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(70, 5)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(70, 6)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(70, 7)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(70, 8)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(70, 9)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(70, 10)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(70, 11)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(70, 12)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(70, 13)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(70, 14)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(70, 15)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(70, 16)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(70, 17)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(70, 18)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(70, 19)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(70, 20)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(70, 21)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(70, 22)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(70, 23)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(70, 24)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(70, 25)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(70, 26)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(70, 27)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(70, 28)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(70, 29)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(70, 30)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(70, 31)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(70, 32)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(70, 33)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(70, 34)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(70, 35)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(70, 36)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(70, 37)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(70, 38)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(70, 39)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(70, 40)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(70, 41)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(70, 42)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(70, 43)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(70, 44)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(70, 45)",
           "CKA: 0.695<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(70, 46)",
           "CKA: 0.695<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(70, 47)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(70, 48)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(70, 49)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(70, 50)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(70, 51)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(70, 52)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(70, 53)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(70, 54)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(70, 55)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(70, 56)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(70, 57)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(70, 58)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(70, 59)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(70, 60)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(70, 61)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(70, 62)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(70, 63)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(70, 64)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(70, 65)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(70, 66)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(70, 67)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(70, 68)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(70, 69)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(70, 70)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(70, 71)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(70, 72)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(70, 73)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(70, 74)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(70, 75)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(70, 76)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(70, 77)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(70, 78)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(70, 79)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(70, 80)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(70, 81)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(70, 82)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(70, 83)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(70, 84)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(70, 85)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(70, 86)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(70, 87)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(70, 88)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(70, 89)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(70, 90)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(70, 91)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(70, 92)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(70, 93)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(70, 94)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(70, 95)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(70, 96)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(70, 97)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(70, 98)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(70, 99)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(70, 100)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(70, 101)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(70, 102)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(70, 103)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(70, 104)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(70, 105)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(70, 106)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(70, 107)"
          ],
          [
           "CKA: 0.625<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(71, 0)",
           "CKA: -1.03e-11<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(71, 1)",
           "CKA: 0.00732<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(71, 2)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(71, 3)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(71, 4)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.embeddings<br>(71, 5)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(71, 6)",
           "CKA: 0.517<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(71, 7)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(71, 8)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(71, 9)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(71, 10)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(71, 11)",
           "CKA: 0.65<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(71, 12)",
           "CKA: 0.631<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(71, 13)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(71, 14)",
           "CKA: 0.644<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(71, 15)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(71, 16)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(71, 17)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(71, 18)",
           "CKA: 0.663<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(71, 19)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(71, 20)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(71, 21)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(71, 22)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(71, 23)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(71, 24)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(71, 25)",
           "CKA: 0.687<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(71, 26)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(71, 27)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(71, 28)",
           "CKA: 0.689<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(71, 29)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(71, 30)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(71, 31)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(71, 32)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(71, 33)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(71, 34)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(71, 35)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(71, 36)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(71, 37)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(71, 38)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(71, 39)",
           "CKA: 0.694<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(71, 40)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(71, 41)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(71, 42)",
           "CKA: 0.703<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(71, 43)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(71, 44)",
           "CKA: 0.665<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(71, 45)",
           "CKA: 0.695<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(71, 46)",
           "CKA: 0.695<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(71, 47)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(71, 48)",
           "CKA: 0.613<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(71, 49)",
           "CKA: 0.656<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(71, 50)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(71, 51)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(71, 52)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(71, 53)",
           "CKA: 0.674<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(71, 54)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(71, 55)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(71, 56)",
           "CKA: 0.646<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(71, 57)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(71, 58)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(71, 59)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(71, 60)",
           "CKA: 0.668<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(71, 61)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(71, 62)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(71, 63)",
           "CKA: 0.667<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(71, 64)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(71, 65)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(71, 66)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(71, 67)",
           "CKA: 0.651<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(71, 68)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(71, 69)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(71, 70)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(71, 71)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(71, 72)",
           "CKA: 0.615<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(71, 73)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(71, 74)",
           "CKA: 0.64<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(71, 75)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(71, 76)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(71, 77)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(71, 78)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(71, 79)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(71, 80)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(71, 81)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(71, 82)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(71, 83)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(71, 84)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(71, 85)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(71, 86)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(71, 87)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(71, 88)",
           "CKA: 0.578<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(71, 89)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(71, 90)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(71, 91)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(71, 92)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(71, 93)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(71, 94)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(71, 95)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(71, 96)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(71, 97)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(71, 98)",
           "CKA: 0.454<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(71, 99)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(71, 100)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(71, 101)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(71, 102)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(71, 103)",
           "CKA: 0.341<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: classifier.dropout<br>(71, 104)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: classifier.dense<br>(71, 105)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: classifier.out_proj<br>(71, 106)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.4.intermediate<br>Pretrained: classifier<br>(71, 107)"
          ],
          [
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(72, 0)",
           "CKA: -6.41e-12<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(72, 1)",
           "CKA: 0.00551<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(72, 2)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(72, 3)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(72, 4)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.embeddings<br>(72, 5)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(72, 6)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(72, 7)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(72, 8)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(72, 9)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(72, 10)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(72, 11)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(72, 12)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(72, 13)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(72, 14)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(72, 15)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(72, 16)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(72, 17)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(72, 18)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(72, 19)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(72, 20)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(72, 21)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(72, 22)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(72, 23)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(72, 24)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(72, 25)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(72, 26)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(72, 27)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(72, 28)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(72, 29)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(72, 30)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(72, 31)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(72, 32)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(72, 33)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(72, 34)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(72, 35)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(72, 36)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(72, 37)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(72, 38)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(72, 39)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(72, 40)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(72, 41)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(72, 42)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(72, 43)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(72, 44)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(72, 45)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(72, 46)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(72, 47)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(72, 48)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(72, 49)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(72, 50)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(72, 51)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(72, 52)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(72, 53)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(72, 54)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(72, 55)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(72, 56)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(72, 57)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(72, 58)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(72, 59)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(72, 60)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(72, 61)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(72, 62)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(72, 63)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(72, 64)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(72, 65)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(72, 66)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(72, 67)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(72, 68)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(72, 69)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(72, 70)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(72, 71)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(72, 72)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(72, 73)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(72, 74)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(72, 75)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(72, 76)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(72, 77)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(72, 78)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(72, 79)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(72, 80)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(72, 81)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(72, 82)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(72, 83)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(72, 84)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(72, 85)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(72, 86)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(72, 87)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(72, 88)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(72, 89)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(72, 90)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(72, 91)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(72, 92)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(72, 93)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(72, 94)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(72, 95)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(72, 96)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(72, 97)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(72, 98)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(72, 99)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(72, 100)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(72, 101)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(72, 102)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(72, 103)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: classifier.dropout<br>(72, 104)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: classifier.dense<br>(72, 105)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: classifier.out_proj<br>(72, 106)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.output.dense<br>Pretrained: classifier<br>(72, 107)"
          ],
          [
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(73, 0)",
           "CKA: -6.41e-12<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(73, 1)",
           "CKA: 0.00551<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(73, 2)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(73, 3)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(73, 4)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.embeddings<br>(73, 5)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(73, 6)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(73, 7)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(73, 8)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(73, 9)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(73, 10)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(73, 11)",
           "CKA: 0.581<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(73, 12)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(73, 13)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(73, 14)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(73, 15)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(73, 16)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(73, 17)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(73, 18)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(73, 19)",
           "CKA: 0.559<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(73, 20)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(73, 21)",
           "CKA: 0.544<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(73, 22)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(73, 23)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(73, 24)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(73, 25)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(73, 26)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(73, 27)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(73, 28)",
           "CKA: 0.626<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(73, 29)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(73, 30)",
           "CKA: 0.592<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(73, 31)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(73, 32)",
           "CKA: 0.609<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(73, 33)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(73, 34)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(73, 35)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(73, 36)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(73, 37)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(73, 38)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(73, 39)",
           "CKA: 0.604<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(73, 40)",
           "CKA: 0.616<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(73, 41)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(73, 42)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(73, 43)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(73, 44)",
           "CKA: 0.611<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(73, 45)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(73, 46)",
           "CKA: 0.608<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(73, 47)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(73, 48)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(73, 49)",
           "CKA: 0.606<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(73, 50)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(73, 51)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(73, 52)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(73, 53)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(73, 54)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(73, 55)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(73, 56)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(73, 57)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(73, 58)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(73, 59)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(73, 60)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(73, 61)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(73, 62)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(73, 63)",
           "CKA: 0.602<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(73, 64)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(73, 65)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(73, 66)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(73, 67)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(73, 68)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(73, 69)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(73, 70)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(73, 71)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(73, 72)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(73, 73)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(73, 74)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(73, 75)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(73, 76)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(73, 77)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(73, 78)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(73, 79)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(73, 80)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(73, 81)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(73, 82)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(73, 83)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(73, 84)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(73, 85)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(73, 86)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(73, 87)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(73, 88)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(73, 89)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(73, 90)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(73, 91)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(73, 92)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(73, 93)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(73, 94)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(73, 95)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(73, 96)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(73, 97)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(73, 98)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(73, 99)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(73, 100)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(73, 101)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(73, 102)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(73, 103)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: classifier.dropout<br>(73, 104)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: classifier.dense<br>(73, 105)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: classifier.out_proj<br>(73, 106)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.4.output.dropout<br>Pretrained: classifier<br>(73, 107)"
          ],
          [
           "CKA: 0.669<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(74, 0)",
           "CKA: -4.35e-11<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(74, 1)",
           "CKA: 0.00932<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(74, 2)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(74, 3)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(74, 4)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(74, 5)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(74, 6)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(74, 7)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(74, 8)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(74, 9)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(74, 10)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(74, 11)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(74, 12)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(74, 13)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(74, 14)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(74, 15)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(74, 16)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(74, 17)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(74, 18)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(74, 19)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(74, 20)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(74, 21)",
           "CKA: 0.645<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(74, 22)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(74, 23)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(74, 24)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(74, 25)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(74, 26)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(74, 27)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(74, 28)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(74, 29)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(74, 30)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(74, 31)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(74, 32)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(74, 33)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(74, 34)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(74, 35)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(74, 36)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(74, 37)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(74, 38)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(74, 39)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(74, 40)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(74, 41)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(74, 42)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(74, 43)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(74, 44)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(74, 45)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(74, 46)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(74, 47)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(74, 48)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(74, 49)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(74, 50)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(74, 51)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(74, 52)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(74, 53)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(74, 54)",
           "CKA: 0.652<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(74, 55)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(74, 56)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(74, 57)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(74, 58)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(74, 59)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(74, 60)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(74, 61)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(74, 62)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(74, 63)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(74, 64)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(74, 65)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(74, 66)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(74, 67)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(74, 68)",
           "CKA: 0.599<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(74, 69)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(74, 70)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(74, 71)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(74, 72)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(74, 73)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(74, 74)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(74, 75)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(74, 76)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(74, 77)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(74, 78)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(74, 79)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(74, 80)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(74, 81)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(74, 82)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(74, 83)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(74, 84)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(74, 85)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(74, 86)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(74, 87)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(74, 88)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(74, 89)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(74, 90)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(74, 91)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(74, 92)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(74, 93)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(74, 94)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(74, 95)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(74, 96)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(74, 97)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(74, 98)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(74, 99)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(74, 100)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(74, 101)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(74, 102)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(74, 103)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: classifier.dropout<br>(74, 104)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: classifier.dense<br>(74, 105)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(74, 106)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.output.LayerNorm<br>Pretrained: classifier<br>(74, 107)"
          ],
          [
           "CKA: 0.669<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(75, 0)",
           "CKA: -4.35e-11<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(75, 1)",
           "CKA: 0.00932<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(75, 2)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(75, 3)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings.dropout<br>(75, 4)",
           "CKA: 0.684<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.embeddings<br>(75, 5)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(75, 6)",
           "CKA: 0.557<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(75, 7)",
           "CKA: 0.637<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(75, 8)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(75, 9)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(75, 10)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(75, 11)",
           "CKA: 0.693<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(75, 12)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(75, 13)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(75, 14)",
           "CKA: 0.683<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(75, 15)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(75, 16)",
           "CKA: 0.639<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(75, 17)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(75, 18)",
           "CKA: 0.706<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.0.output<br>(75, 19)",
           "CKA: 0.634<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(75, 20)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(75, 21)",
           "CKA: 0.645<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(75, 22)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(75, 23)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(75, 24)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(75, 25)",
           "CKA: 0.726<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(75, 26)",
           "CKA: 0.721<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(75, 27)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(75, 28)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(75, 29)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(75, 30)",
           "CKA: 0.676<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(75, 31)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(75, 32)",
           "CKA: 0.723<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.1.output<br>(75, 33)",
           "CKA: 0.635<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(75, 34)",
           "CKA: 0.649<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(75, 35)",
           "CKA: 0.673<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(75, 36)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(75, 37)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(75, 38)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(75, 39)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(75, 40)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(75, 41)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(75, 42)",
           "CKA: 0.735<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(75, 43)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(75, 44)",
           "CKA: 0.692<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(75, 45)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(75, 46)",
           "CKA: 0.728<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.2.output<br>(75, 47)",
           "CKA: 0.607<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(75, 48)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(75, 49)",
           "CKA: 0.686<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(75, 50)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(75, 51)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(75, 52)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(75, 53)",
           "CKA: 0.697<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(75, 54)",
           "CKA: 0.652<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(75, 55)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(75, 56)",
           "CKA: 0.664<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(75, 57)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(75, 58)",
           "CKA: 0.623<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(75, 59)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(75, 60)",
           "CKA: 0.691<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.3.output<br>(75, 61)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(75, 62)",
           "CKA: 0.576<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(75, 63)",
           "CKA: 0.678<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(75, 64)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(75, 65)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(75, 66)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(75, 67)",
           "CKA: 0.661<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(75, 68)",
           "CKA: 0.599<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(75, 69)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(75, 70)",
           "CKA: 0.621<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(75, 71)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(75, 72)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(75, 73)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(75, 74)",
           "CKA: 0.643<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.4.output<br>(75, 75)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(75, 76)",
           "CKA: 0.541<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(75, 77)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(75, 78)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(75, 79)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(75, 80)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(75, 81)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(75, 82)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(75, 83)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(75, 84)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(75, 85)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(75, 86)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(75, 87)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(75, 88)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.5.output<br>(75, 89)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(75, 90)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(75, 91)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(75, 92)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(75, 93)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(75, 94)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(75, 95)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(75, 96)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(75, 97)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(75, 98)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(75, 99)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(75, 100)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(75, 101)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(75, 102)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.4.output<br>Pretrained: roberta.encoder.layer.6.output<br>(75, 103)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.4.output<br>Pretrained: classifier.dropout<br>(75, 104)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.4.output<br>Pretrained: classifier.dense<br>(75, 105)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.output<br>Pretrained: classifier.out_proj<br>(75, 106)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.4.output<br>Pretrained: classifier<br>(75, 107)"
          ],
          [
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(76, 0)",
           "CKA: -5.55e-12<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(76, 1)",
           "CKA: 0.00301<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(76, 2)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(76, 3)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(76, 4)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.embeddings<br>(76, 5)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(76, 6)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(76, 7)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(76, 8)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(76, 9)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(76, 10)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(76, 11)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(76, 12)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(76, 13)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(76, 14)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(76, 15)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(76, 16)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(76, 17)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(76, 18)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(76, 19)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(76, 20)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(76, 21)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(76, 22)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(76, 23)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(76, 24)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(76, 25)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(76, 26)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(76, 27)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(76, 28)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(76, 29)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(76, 30)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(76, 31)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(76, 32)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(76, 33)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(76, 34)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(76, 35)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(76, 36)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(76, 37)",
           "CKA: 0.35<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(76, 38)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(76, 39)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(76, 40)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(76, 41)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(76, 42)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(76, 43)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(76, 44)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(76, 45)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(76, 46)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(76, 47)",
           "CKA: 0.361<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(76, 48)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(76, 49)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(76, 50)",
           "CKA: 0.261<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(76, 51)",
           "CKA: 0.261<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(76, 52)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(76, 53)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(76, 54)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(76, 55)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(76, 56)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(76, 57)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(76, 58)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(76, 59)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(76, 60)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(76, 61)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(76, 62)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(76, 63)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(76, 64)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(76, 65)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(76, 66)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(76, 67)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(76, 68)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(76, 69)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(76, 70)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(76, 71)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(76, 72)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(76, 73)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(76, 74)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(76, 75)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(76, 76)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(76, 77)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(76, 78)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(76, 79)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(76, 80)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(76, 81)",
           "CKA: 0.519<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(76, 82)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(76, 83)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(76, 84)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(76, 85)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(76, 86)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(76, 87)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(76, 88)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(76, 89)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(76, 90)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(76, 91)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(76, 92)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(76, 93)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(76, 94)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(76, 95)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(76, 96)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(76, 97)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(76, 98)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(76, 99)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(76, 100)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(76, 101)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(76, 102)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(76, 103)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: classifier.dropout<br>(76, 104)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: classifier.dense<br>(76, 105)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: classifier.out_proj<br>(76, 106)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.5.attention.self.query<br>Pretrained: classifier<br>(76, 107)"
          ],
          [
           "CKA: 0.543<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(77, 0)",
           "CKA: -6.09e-12<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(77, 1)",
           "CKA: 0.00638<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(77, 2)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(77, 3)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(77, 4)",
           "CKA: 0.555<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.embeddings<br>(77, 5)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(77, 6)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(77, 7)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(77, 8)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(77, 9)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(77, 10)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(77, 11)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(77, 12)",
           "CKA: 0.552<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(77, 13)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(77, 14)",
           "CKA: 0.562<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(77, 15)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(77, 16)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(77, 17)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(77, 18)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(77, 19)",
           "CKA: 0.536<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(77, 20)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(77, 21)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(77, 22)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(77, 23)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(77, 24)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(77, 25)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(77, 26)",
           "CKA: 0.586<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(77, 27)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(77, 28)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(77, 29)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(77, 30)",
           "CKA: 0.561<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(77, 31)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(77, 32)",
           "CKA: 0.582<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(77, 33)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(77, 34)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(77, 35)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(77, 36)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(77, 37)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(77, 38)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(77, 39)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(77, 40)",
           "CKA: 0.571<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(77, 41)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(77, 42)",
           "CKA: 0.587<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(77, 43)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(77, 44)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(77, 45)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(77, 46)",
           "CKA: 0.572<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(77, 47)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(77, 48)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(77, 49)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(77, 50)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(77, 51)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(77, 52)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(77, 53)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(77, 54)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(77, 55)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(77, 56)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(77, 57)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(77, 58)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(77, 59)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(77, 60)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(77, 61)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(77, 62)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(77, 63)",
           "CKA: 0.535<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(77, 64)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(77, 65)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(77, 66)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(77, 67)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(77, 68)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(77, 69)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(77, 70)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(77, 71)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(77, 72)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(77, 73)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(77, 74)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(77, 75)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(77, 76)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(77, 77)",
           "CKA: 0.446<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(77, 78)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(77, 79)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(77, 80)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(77, 81)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(77, 82)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(77, 83)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(77, 84)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(77, 85)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(77, 86)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(77, 87)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(77, 88)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(77, 89)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(77, 90)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(77, 91)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(77, 92)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(77, 93)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(77, 94)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(77, 95)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(77, 96)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(77, 97)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(77, 98)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(77, 99)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(77, 100)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(77, 101)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(77, 102)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(77, 103)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: classifier.dropout<br>(77, 104)",
           "CKA: 0.227<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: classifier.dense<br>(77, 105)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: classifier.out_proj<br>(77, 106)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.attention.self.key<br>Pretrained: classifier<br>(77, 107)"
          ],
          [
           "CKA: 0.465<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(78, 0)",
           "CKA: -1e-11<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(78, 1)",
           "CKA: 0.0068<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(78, 2)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(78, 3)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(78, 4)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.embeddings<br>(78, 5)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(78, 6)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(78, 7)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(78, 8)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(78, 9)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(78, 10)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(78, 11)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(78, 12)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(78, 13)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(78, 14)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(78, 15)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(78, 16)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(78, 17)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(78, 18)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(78, 19)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(78, 20)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(78, 21)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(78, 22)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(78, 23)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(78, 24)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(78, 25)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(78, 26)",
           "CKA: 0.52<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(78, 27)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(78, 28)",
           "CKA: 0.525<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(78, 29)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(78, 30)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(78, 31)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(78, 32)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(78, 33)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(78, 34)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(78, 35)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(78, 36)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(78, 37)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(78, 38)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(78, 39)",
           "CKA: 0.532<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(78, 40)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(78, 41)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(78, 42)",
           "CKA: 0.548<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(78, 43)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(78, 44)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(78, 45)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(78, 46)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(78, 47)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(78, 48)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(78, 49)",
           "CKA: 0.538<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(78, 50)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(78, 51)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(78, 52)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(78, 53)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(78, 54)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(78, 55)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(78, 56)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(78, 57)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(78, 58)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(78, 59)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(78, 60)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(78, 61)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(78, 62)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(78, 63)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(78, 64)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(78, 65)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(78, 66)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(78, 67)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(78, 68)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(78, 69)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(78, 70)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(78, 71)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(78, 72)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(78, 73)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(78, 74)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(78, 75)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(78, 76)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(78, 77)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(78, 78)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(78, 79)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(78, 80)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(78, 81)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(78, 82)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(78, 83)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(78, 84)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(78, 85)",
           "CKA: 0.446<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(78, 86)",
           "CKA: 0.446<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(78, 87)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(78, 88)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(78, 89)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(78, 90)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(78, 91)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(78, 92)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(78, 93)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(78, 94)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(78, 95)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(78, 96)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(78, 97)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(78, 98)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(78, 99)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(78, 100)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(78, 101)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(78, 102)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(78, 103)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: classifier.dropout<br>(78, 104)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: classifier.dense<br>(78, 105)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: classifier.out_proj<br>(78, 106)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.5.attention.self.value<br>Pretrained: classifier<br>(78, 107)"
          ],
          [
           "CKA: 0.172<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(79, 0)",
           "CKA: 4.24e-12<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(79, 1)",
           "CKA: 0.00205<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(79, 2)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(79, 3)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(79, 4)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.embeddings<br>(79, 5)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(79, 6)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(79, 7)",
           "CKA: 0.173<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(79, 8)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(79, 9)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(79, 10)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(79, 11)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(79, 12)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(79, 13)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(79, 14)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(79, 15)",
           "CKA: 0.159<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(79, 16)",
           "CKA: 0.159<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(79, 17)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(79, 18)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(79, 19)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(79, 20)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(79, 21)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(79, 22)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(79, 23)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(79, 24)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(79, 25)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(79, 26)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(79, 27)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(79, 28)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(79, 29)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(79, 30)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(79, 31)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(79, 32)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(79, 33)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(79, 34)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(79, 35)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(79, 36)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(79, 37)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(79, 38)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(79, 39)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(79, 40)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(79, 41)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(79, 42)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(79, 43)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(79, 44)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(79, 45)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(79, 46)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(79, 47)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(79, 48)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(79, 49)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(79, 50)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(79, 51)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(79, 52)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(79, 53)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(79, 54)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(79, 55)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(79, 56)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(79, 57)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(79, 58)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(79, 59)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(79, 60)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(79, 61)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(79, 62)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(79, 63)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(79, 64)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(79, 65)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(79, 66)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(79, 67)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(79, 68)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(79, 69)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(79, 70)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(79, 71)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(79, 72)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(79, 73)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(79, 74)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(79, 75)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(79, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(79, 77)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(79, 78)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(79, 79)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(79, 80)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(79, 81)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(79, 82)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(79, 83)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(79, 84)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(79, 85)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(79, 86)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(79, 87)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(79, 88)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(79, 89)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(79, 90)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(79, 91)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(79, 92)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(79, 93)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(79, 94)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(79, 95)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(79, 96)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(79, 97)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(79, 98)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(79, 99)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(79, 100)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(79, 101)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(79, 102)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(79, 103)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: classifier.dropout<br>(79, 104)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: classifier.dense<br>(79, 105)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: classifier.out_proj<br>(79, 106)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dense<br>Pretrained: classifier<br>(79, 107)"
          ],
          [
           "CKA: 0.172<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(80, 0)",
           "CKA: 4.24e-12<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(80, 1)",
           "CKA: 0.00205<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(80, 2)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(80, 3)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(80, 4)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(80, 5)",
           "CKA: 0.163<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(80, 6)",
           "CKA: 0.167<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(80, 7)",
           "CKA: 0.173<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(80, 8)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(80, 9)",
           "CKA: 0.128<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(80, 10)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(80, 11)",
           "CKA: 0.175<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(80, 12)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(80, 13)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(80, 14)",
           "CKA: 0.17<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(80, 15)",
           "CKA: 0.159<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(80, 16)",
           "CKA: 0.159<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(80, 17)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(80, 18)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(80, 19)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(80, 20)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(80, 21)",
           "CKA: 0.206<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(80, 22)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(80, 23)",
           "CKA: 0.291<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(80, 24)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(80, 25)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(80, 26)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(80, 27)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(80, 28)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(80, 29)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(80, 30)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(80, 31)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(80, 32)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(80, 33)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(80, 34)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(80, 35)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(80, 36)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(80, 37)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(80, 38)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(80, 39)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(80, 40)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(80, 41)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(80, 42)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(80, 43)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(80, 44)",
           "CKA: 0.179<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(80, 45)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(80, 46)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(80, 47)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(80, 48)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(80, 49)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(80, 50)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(80, 51)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(80, 52)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(80, 53)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(80, 54)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(80, 55)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(80, 56)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(80, 57)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(80, 58)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(80, 59)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(80, 60)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(80, 61)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(80, 62)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(80, 63)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(80, 64)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(80, 65)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(80, 66)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(80, 67)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(80, 68)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(80, 69)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(80, 70)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(80, 71)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(80, 72)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(80, 73)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(80, 74)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(80, 75)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(80, 76)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(80, 77)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(80, 78)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(80, 79)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(80, 80)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(80, 81)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(80, 82)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(80, 83)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(80, 84)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(80, 85)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(80, 86)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(80, 87)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(80, 88)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(80, 89)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(80, 90)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(80, 91)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(80, 92)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(80, 93)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(80, 94)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(80, 95)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(80, 96)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(80, 97)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(80, 98)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(80, 99)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(80, 100)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(80, 101)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(80, 102)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(80, 103)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: classifier.dropout<br>(80, 104)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: classifier.dense<br>(80, 105)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(80, 106)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.attention.output.dropout<br>Pretrained: classifier<br>(80, 107)"
          ],
          [
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(81, 0)",
           "CKA: -2.22e-11<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(81, 1)",
           "CKA: 0.0064<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(81, 2)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(81, 3)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(81, 4)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(81, 5)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(81, 6)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(81, 7)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(81, 8)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(81, 9)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(81, 10)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(81, 11)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(81, 12)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(81, 13)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(81, 14)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(81, 15)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(81, 16)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(81, 17)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(81, 18)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(81, 19)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(81, 20)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(81, 21)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(81, 22)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(81, 23)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(81, 24)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(81, 25)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(81, 26)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(81, 27)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(81, 28)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(81, 29)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(81, 30)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(81, 31)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(81, 32)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(81, 33)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(81, 34)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(81, 35)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(81, 36)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(81, 37)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(81, 38)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(81, 39)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(81, 40)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(81, 41)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(81, 42)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(81, 43)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(81, 44)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(81, 45)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(81, 46)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(81, 47)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(81, 48)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(81, 49)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(81, 50)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(81, 51)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(81, 52)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(81, 53)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(81, 54)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(81, 55)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(81, 56)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(81, 57)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(81, 58)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(81, 59)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(81, 60)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(81, 61)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(81, 62)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(81, 63)",
           "CKA: 0.567<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(81, 64)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(81, 65)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(81, 66)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(81, 67)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(81, 68)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(81, 69)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(81, 70)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(81, 71)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(81, 72)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(81, 73)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(81, 74)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(81, 75)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(81, 76)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(81, 77)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(81, 78)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(81, 79)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(81, 80)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(81, 81)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(81, 82)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(81, 83)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(81, 84)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(81, 85)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(81, 86)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(81, 87)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(81, 88)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(81, 89)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(81, 90)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(81, 91)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(81, 92)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(81, 93)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(81, 94)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(81, 95)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(81, 96)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(81, 97)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(81, 98)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(81, 99)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(81, 100)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(81, 101)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(81, 102)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(81, 103)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(81, 104)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(81, 105)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(81, 106)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.attention.output.LayerNorm<br>Pretrained: classifier<br>(81, 107)"
          ],
          [
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(82, 0)",
           "CKA: -2.22e-11<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(82, 1)",
           "CKA: 0.0064<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(82, 2)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(82, 3)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(82, 4)",
           "CKA: 0.549<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.embeddings<br>(82, 5)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(82, 6)",
           "CKA: 0.441<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(82, 7)",
           "CKA: 0.51<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(82, 8)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(82, 9)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(82, 10)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(82, 11)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(82, 12)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(82, 13)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(82, 14)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(82, 15)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(82, 16)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(82, 17)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(82, 18)",
           "CKA: 0.563<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(82, 19)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(82, 20)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(82, 21)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(82, 22)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(82, 23)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(82, 24)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(82, 25)",
           "CKA: 0.588<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(82, 26)",
           "CKA: 0.577<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(82, 27)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(82, 28)",
           "CKA: 0.573<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(82, 29)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(82, 30)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(82, 31)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(82, 32)",
           "CKA: 0.58<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(82, 33)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(82, 34)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(82, 35)",
           "CKA: 0.546<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(82, 36)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(82, 37)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(82, 38)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(82, 39)",
           "CKA: 0.6<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(82, 40)",
           "CKA: 0.595<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(82, 41)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(82, 42)",
           "CKA: 0.59<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(82, 43)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(82, 44)",
           "CKA: 0.537<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(82, 45)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(82, 46)",
           "CKA: 0.593<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(82, 47)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(82, 48)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(82, 49)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(82, 50)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(82, 51)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(82, 52)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(82, 53)",
           "CKA: 0.589<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(82, 54)",
           "CKA: 0.564<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(82, 55)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(82, 56)",
           "CKA: 0.57<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(82, 57)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(82, 58)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(82, 59)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(82, 60)",
           "CKA: 0.584<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(82, 61)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(82, 62)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(82, 63)",
           "CKA: 0.567<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(82, 64)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(82, 65)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(82, 66)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(82, 67)",
           "CKA: 0.585<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(82, 68)",
           "CKA: 0.556<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(82, 69)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(82, 70)",
           "CKA: 0.569<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(82, 71)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(82, 72)",
           "CKA: 0.545<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(82, 73)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(82, 74)",
           "CKA: 0.579<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(82, 75)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(82, 76)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(82, 77)",
           "CKA: 0.539<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(82, 78)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(82, 79)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(82, 80)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(82, 81)",
           "CKA: 0.55<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(82, 82)",
           "CKA: 0.531<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(82, 83)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(82, 84)",
           "CKA: 0.527<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(82, 85)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(82, 86)",
           "CKA: 0.496<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(82, 87)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(82, 88)",
           "CKA: 0.551<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(82, 89)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(82, 90)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(82, 91)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(82, 92)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(82, 93)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(82, 94)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(82, 95)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(82, 96)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(82, 97)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(82, 98)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(82, 99)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(82, 100)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(82, 101)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(82, 102)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(82, 103)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: classifier.dropout<br>(82, 104)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: classifier.dense<br>(82, 105)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: classifier.out_proj<br>(82, 106)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.attention.output<br>Pretrained: classifier<br>(82, 107)"
          ],
          [
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(83, 0)",
           "CKA: -7.14e-12<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(83, 1)",
           "CKA: 0.00554<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(83, 2)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(83, 3)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(83, 4)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.embeddings<br>(83, 5)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(83, 6)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(83, 7)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(83, 8)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(83, 9)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(83, 10)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(83, 11)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(83, 12)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(83, 13)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(83, 14)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(83, 15)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(83, 16)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(83, 17)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(83, 18)",
           "CKA: 0.45<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(83, 19)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(83, 20)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(83, 21)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(83, 22)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(83, 23)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(83, 24)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(83, 25)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(83, 26)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(83, 27)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(83, 28)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(83, 29)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(83, 30)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(83, 31)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(83, 32)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(83, 33)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(83, 34)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(83, 35)",
           "CKA: 0.433<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(83, 36)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(83, 37)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(83, 38)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(83, 39)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(83, 40)",
           "CKA: 0.482<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(83, 41)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(83, 42)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(83, 43)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(83, 44)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(83, 45)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(83, 46)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(83, 47)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(83, 48)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(83, 49)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(83, 50)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(83, 51)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(83, 52)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(83, 53)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(83, 54)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(83, 55)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(83, 56)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(83, 57)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(83, 58)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(83, 59)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(83, 60)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(83, 61)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(83, 62)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(83, 63)",
           "CKA: 0.492<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(83, 64)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(83, 65)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(83, 66)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(83, 67)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(83, 68)",
           "CKA: 0.514<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(83, 69)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(83, 70)",
           "CKA: 0.512<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(83, 71)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(83, 72)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(83, 73)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(83, 74)",
           "CKA: 0.497<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(83, 75)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(83, 76)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(83, 77)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(83, 78)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(83, 79)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(83, 80)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(83, 81)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(83, 82)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(83, 83)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(83, 84)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(83, 85)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(83, 86)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(83, 87)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(83, 88)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(83, 89)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(83, 90)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(83, 91)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(83, 92)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(83, 93)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(83, 94)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(83, 95)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(83, 96)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(83, 97)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(83, 98)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(83, 99)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(83, 100)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(83, 101)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(83, 102)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(83, 103)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: classifier.dropout<br>(83, 104)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: classifier.dense<br>(83, 105)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: classifier.out_proj<br>(83, 106)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.5.intermediate.dense<br>Pretrained: classifier<br>(83, 107)"
          ],
          [
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(84, 0)",
           "CKA: -3.44e-12<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(84, 1)",
           "CKA: 0.00498<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(84, 2)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(84, 3)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(84, 4)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(84, 5)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(84, 6)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(84, 7)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(84, 8)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(84, 9)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(84, 10)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(84, 11)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(84, 12)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(84, 13)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(84, 14)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(84, 15)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(84, 16)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(84, 17)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(84, 18)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(84, 19)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(84, 20)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(84, 21)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(84, 22)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(84, 23)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(84, 24)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(84, 25)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(84, 26)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(84, 27)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(84, 28)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(84, 29)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(84, 30)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(84, 31)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(84, 32)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(84, 33)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(84, 34)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(84, 35)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(84, 36)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(84, 37)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(84, 38)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(84, 39)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(84, 40)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(84, 41)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(84, 42)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(84, 43)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(84, 44)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(84, 45)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(84, 46)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(84, 47)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(84, 48)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(84, 49)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(84, 50)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(84, 51)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(84, 52)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(84, 53)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(84, 54)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(84, 55)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(84, 56)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(84, 57)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(84, 58)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(84, 59)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(84, 60)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(84, 61)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(84, 62)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(84, 63)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(84, 64)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(84, 65)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(84, 66)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(84, 67)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(84, 68)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(84, 69)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(84, 70)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(84, 71)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(84, 72)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(84, 73)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(84, 74)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(84, 75)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(84, 76)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(84, 77)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(84, 78)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(84, 79)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(84, 80)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(84, 81)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(84, 82)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(84, 83)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(84, 84)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(84, 85)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(84, 86)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(84, 87)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(84, 88)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(84, 89)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(84, 90)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(84, 91)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(84, 92)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(84, 93)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(84, 94)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(84, 95)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(84, 96)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(84, 97)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(84, 98)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(84, 99)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(84, 100)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(84, 101)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(84, 102)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(84, 103)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(84, 104)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(84, 105)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(84, 106)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(84, 107)"
          ],
          [
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(85, 0)",
           "CKA: -3.44e-12<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(85, 1)",
           "CKA: 0.00498<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(85, 2)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(85, 3)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(85, 4)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.embeddings<br>(85, 5)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(85, 6)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(85, 7)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(85, 8)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(85, 9)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(85, 10)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(85, 11)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(85, 12)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(85, 13)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(85, 14)",
           "CKA: 0.427<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(85, 15)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(85, 16)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(85, 17)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(85, 18)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(85, 19)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(85, 20)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(85, 21)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(85, 22)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(85, 23)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(85, 24)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(85, 25)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(85, 26)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(85, 27)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(85, 28)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(85, 29)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(85, 30)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(85, 31)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(85, 32)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(85, 33)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(85, 34)",
           "CKA: 0.406<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(85, 35)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(85, 36)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(85, 37)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(85, 38)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(85, 39)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(85, 40)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(85, 41)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(85, 42)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(85, 43)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(85, 44)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(85, 45)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(85, 46)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(85, 47)",
           "CKA: 0.407<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(85, 48)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(85, 49)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(85, 50)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(85, 51)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(85, 52)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(85, 53)",
           "CKA: 0.471<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(85, 54)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(85, 55)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(85, 56)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(85, 57)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(85, 58)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(85, 59)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(85, 60)",
           "CKA: 0.473<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(85, 61)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(85, 62)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(85, 63)",
           "CKA: 0.481<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(85, 64)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(85, 65)",
           "CKA: 0.358<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(85, 66)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(85, 67)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(85, 68)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(85, 69)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(85, 70)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(85, 71)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(85, 72)",
           "CKA: 0.476<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(85, 73)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(85, 74)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(85, 75)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(85, 76)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(85, 77)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(85, 78)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(85, 79)",
           "CKA: 0.307<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(85, 80)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(85, 81)",
           "CKA: 0.455<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(85, 82)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(85, 83)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(85, 84)",
           "CKA: 0.437<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(85, 85)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(85, 86)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(85, 87)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(85, 88)",
           "CKA: 0.462<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(85, 89)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(85, 90)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(85, 91)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(85, 92)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(85, 93)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(85, 94)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(85, 95)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(85, 96)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(85, 97)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(85, 98)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(85, 99)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(85, 100)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(85, 101)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(85, 102)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(85, 103)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: classifier.dropout<br>(85, 104)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: classifier.dense<br>(85, 105)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: classifier.out_proj<br>(85, 106)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.intermediate<br>Pretrained: classifier<br>(85, 107)"
          ],
          [
           "CKA: 0.289<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(86, 0)",
           "CKA: 1.45e-12<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(86, 1)",
           "CKA: 0.00364<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(86, 2)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(86, 3)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(86, 4)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.embeddings<br>(86, 5)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(86, 6)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(86, 7)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(86, 8)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(86, 9)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(86, 10)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(86, 11)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(86, 12)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(86, 13)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(86, 14)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(86, 15)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(86, 16)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(86, 17)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(86, 18)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(86, 19)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(86, 20)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(86, 21)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(86, 22)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(86, 23)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(86, 24)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(86, 25)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(86, 26)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(86, 27)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(86, 28)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(86, 29)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(86, 30)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(86, 31)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(86, 32)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(86, 33)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(86, 34)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(86, 35)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(86, 36)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(86, 37)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(86, 38)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(86, 39)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(86, 40)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(86, 41)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(86, 42)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(86, 43)",
           "CKA: 0.306<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(86, 44)",
           "CKA: 0.306<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(86, 45)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(86, 46)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(86, 47)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(86, 48)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(86, 49)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(86, 50)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(86, 51)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(86, 52)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(86, 53)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(86, 54)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(86, 55)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(86, 56)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(86, 57)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(86, 58)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(86, 59)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(86, 60)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(86, 61)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(86, 62)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(86, 63)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(86, 64)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(86, 65)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(86, 66)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(86, 67)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(86, 68)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(86, 69)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(86, 70)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(86, 71)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(86, 72)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(86, 73)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(86, 74)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(86, 75)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(86, 76)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(86, 77)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(86, 78)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(86, 79)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(86, 80)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(86, 81)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(86, 82)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(86, 83)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(86, 84)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(86, 85)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(86, 86)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(86, 87)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(86, 88)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(86, 89)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(86, 90)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(86, 91)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(86, 92)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(86, 93)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(86, 94)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(86, 95)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(86, 96)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(86, 97)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(86, 98)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(86, 99)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(86, 100)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(86, 101)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(86, 102)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(86, 103)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: classifier.dropout<br>(86, 104)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: classifier.dense<br>(86, 105)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: classifier.out_proj<br>(86, 106)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.5.output.dense<br>Pretrained: classifier<br>(86, 107)"
          ],
          [
           "CKA: 0.289<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(87, 0)",
           "CKA: 1.45e-12<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(87, 1)",
           "CKA: 0.00364<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(87, 2)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(87, 3)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(87, 4)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.embeddings<br>(87, 5)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(87, 6)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(87, 7)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(87, 8)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(87, 9)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(87, 10)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(87, 11)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(87, 12)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(87, 13)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(87, 14)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(87, 15)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(87, 16)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(87, 17)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(87, 18)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(87, 19)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(87, 20)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(87, 21)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(87, 22)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(87, 23)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(87, 24)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(87, 25)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(87, 26)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(87, 27)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(87, 28)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(87, 29)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(87, 30)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(87, 31)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(87, 32)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(87, 33)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(87, 34)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(87, 35)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(87, 36)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(87, 37)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(87, 38)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(87, 39)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(87, 40)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(87, 41)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(87, 42)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(87, 43)",
           "CKA: 0.306<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(87, 44)",
           "CKA: 0.306<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(87, 45)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(87, 46)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(87, 47)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(87, 48)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(87, 49)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(87, 50)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(87, 51)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(87, 52)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(87, 53)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(87, 54)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(87, 55)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(87, 56)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(87, 57)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(87, 58)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(87, 59)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(87, 60)",
           "CKA: 0.348<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(87, 61)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(87, 62)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(87, 63)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(87, 64)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(87, 65)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(87, 66)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(87, 67)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(87, 68)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(87, 69)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(87, 70)",
           "CKA: 0.381<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(87, 71)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(87, 72)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(87, 73)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(87, 74)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(87, 75)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(87, 76)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(87, 77)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(87, 78)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(87, 79)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(87, 80)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(87, 81)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(87, 82)",
           "CKA: 0.312<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(87, 83)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(87, 84)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(87, 85)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(87, 86)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(87, 87)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(87, 88)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(87, 89)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(87, 90)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(87, 91)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(87, 92)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(87, 93)",
           "CKA: 0.191<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(87, 94)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(87, 95)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(87, 96)",
           "CKA: 0.224<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(87, 97)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(87, 98)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(87, 99)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(87, 100)",
           "CKA: 0.213<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(87, 101)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(87, 102)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(87, 103)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: classifier.dropout<br>(87, 104)",
           "CKA: 0.182<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: classifier.dense<br>(87, 105)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: classifier.out_proj<br>(87, 106)",
           "CKA: 0.136<br>Base: roberta.encoder.layer.5.output.dropout<br>Pretrained: classifier<br>(87, 107)"
          ],
          [
           "CKA: 0.434<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(88, 0)",
           "CKA: -1.5e-11<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(88, 1)",
           "CKA: 0.00347<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(88, 2)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(88, 3)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(88, 4)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(88, 5)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(88, 6)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(88, 7)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(88, 8)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(88, 9)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(88, 10)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(88, 11)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(88, 12)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(88, 13)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(88, 14)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(88, 15)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(88, 16)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(88, 17)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(88, 18)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(88, 19)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(88, 20)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(88, 21)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(88, 22)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(88, 23)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(88, 24)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(88, 25)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(88, 26)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(88, 27)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(88, 28)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(88, 29)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(88, 30)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(88, 31)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(88, 32)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(88, 33)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(88, 34)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(88, 35)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(88, 36)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(88, 37)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(88, 38)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(88, 39)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(88, 40)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(88, 41)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(88, 42)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(88, 43)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(88, 44)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(88, 45)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(88, 46)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(88, 47)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(88, 48)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(88, 49)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(88, 50)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(88, 51)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(88, 52)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(88, 53)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(88, 54)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(88, 55)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(88, 56)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(88, 57)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(88, 58)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(88, 59)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(88, 60)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(88, 61)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(88, 62)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(88, 63)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(88, 64)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(88, 65)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(88, 66)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(88, 67)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(88, 68)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(88, 69)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(88, 70)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(88, 71)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(88, 72)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(88, 73)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(88, 74)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(88, 75)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(88, 76)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(88, 77)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(88, 78)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(88, 79)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(88, 80)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(88, 81)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(88, 82)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(88, 83)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(88, 84)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(88, 85)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(88, 86)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(88, 87)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(88, 88)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(88, 89)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(88, 90)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(88, 91)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(88, 92)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(88, 93)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(88, 94)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(88, 95)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(88, 96)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(88, 97)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(88, 98)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(88, 99)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(88, 100)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(88, 101)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(88, 102)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(88, 103)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: classifier.dropout<br>(88, 104)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: classifier.dense<br>(88, 105)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(88, 106)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.output.LayerNorm<br>Pretrained: classifier<br>(88, 107)"
          ],
          [
           "CKA: 0.434<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(89, 0)",
           "CKA: -1.5e-11<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(89, 1)",
           "CKA: 0.00347<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(89, 2)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(89, 3)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings.dropout<br>(89, 4)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.embeddings<br>(89, 5)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(89, 6)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(89, 7)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(89, 8)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(89, 9)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(89, 10)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(89, 11)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(89, 12)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(89, 13)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(89, 14)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(89, 15)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(89, 16)",
           "CKA: 0.412<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(89, 17)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(89, 18)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.0.output<br>(89, 19)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(89, 20)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(89, 21)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(89, 22)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(89, 23)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(89, 24)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(89, 25)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(89, 26)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(89, 27)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(89, 28)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(89, 29)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(89, 30)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(89, 31)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(89, 32)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.1.output<br>(89, 33)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(89, 34)",
           "CKA: 0.41<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(89, 35)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(89, 36)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(89, 37)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(89, 38)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(89, 39)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(89, 40)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(89, 41)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(89, 42)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(89, 43)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(89, 44)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(89, 45)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(89, 46)",
           "CKA: 0.479<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.2.output<br>(89, 47)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(89, 48)",
           "CKA: 0.404<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(89, 49)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(89, 50)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(89, 51)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(89, 52)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(89, 53)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(89, 54)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(89, 55)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(89, 56)",
           "CKA: 0.477<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(89, 57)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(89, 58)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(89, 59)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(89, 60)",
           "CKA: 0.484<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.3.output<br>(89, 61)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(89, 62)",
           "CKA: 0.43<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(89, 63)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(89, 64)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(89, 65)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(89, 66)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(89, 67)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(89, 68)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(89, 69)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(89, 70)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(89, 71)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(89, 72)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(89, 73)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(89, 74)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.4.output<br>(89, 75)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(89, 76)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(89, 77)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(89, 78)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(89, 79)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(89, 80)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(89, 81)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(89, 82)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(89, 83)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(89, 84)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(89, 85)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(89, 86)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(89, 87)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(89, 88)",
           "CKA: 0.469<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.5.output<br>(89, 89)",
           "CKA: 0.432<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(89, 90)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(89, 91)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(89, 92)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(89, 93)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(89, 94)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(89, 95)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(89, 96)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(89, 97)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(89, 98)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(89, 99)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(89, 100)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(89, 101)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(89, 102)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.5.output<br>Pretrained: roberta.encoder.layer.6.output<br>(89, 103)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.5.output<br>Pretrained: classifier.dropout<br>(89, 104)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.5.output<br>Pretrained: classifier.dense<br>(89, 105)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.output<br>Pretrained: classifier.out_proj<br>(89, 106)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.5.output<br>Pretrained: classifier<br>(89, 107)"
          ],
          [
           "CKA: 0.24<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings.word_embeddings<br>(90, 0)",
           "CKA: -7.89e-13<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(90, 1)",
           "CKA: 0.00231<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings.position_embeddings<br>(90, 2)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings.LayerNorm<br>(90, 3)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings.dropout<br>(90, 4)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.embeddings<br>(90, 5)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(90, 6)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(90, 7)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(90, 8)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(90, 9)",
           "CKA: 0.171<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(90, 10)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(90, 11)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(90, 12)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(90, 13)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(90, 14)",
           "CKA: 0.246<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(90, 15)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(90, 16)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(90, 17)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(90, 18)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.0.output<br>(90, 19)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(90, 20)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(90, 21)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(90, 22)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(90, 23)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(90, 24)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(90, 25)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(90, 26)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(90, 27)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(90, 28)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(90, 29)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(90, 30)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(90, 31)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(90, 32)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.1.output<br>(90, 33)",
           "CKA: 0.262<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(90, 34)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(90, 35)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(90, 36)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(90, 37)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(90, 38)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(90, 39)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(90, 40)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(90, 41)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(90, 42)",
           "CKA: 0.295<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(90, 43)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(90, 44)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(90, 45)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(90, 46)",
           "CKA: 0.298<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.2.output<br>(90, 47)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(90, 48)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(90, 49)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(90, 50)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(90, 51)",
           "CKA: 0.216<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(90, 52)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(90, 53)",
           "CKA: 0.322<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(90, 54)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(90, 55)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(90, 56)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(90, 57)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(90, 58)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(90, 59)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(90, 60)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.3.output<br>(90, 61)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(90, 62)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(90, 63)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(90, 64)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(90, 65)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(90, 66)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(90, 67)",
           "CKA: 0.377<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(90, 68)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(90, 69)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(90, 70)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(90, 71)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(90, 72)",
           "CKA: 0.419<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(90, 73)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(90, 74)",
           "CKA: 0.399<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.4.output<br>(90, 75)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(90, 76)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(90, 77)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(90, 78)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(90, 79)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(90, 80)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(90, 81)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(90, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(90, 83)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(90, 84)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(90, 85)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(90, 86)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(90, 87)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(90, 88)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.5.output<br>(90, 89)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(90, 90)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(90, 91)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(90, 92)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(90, 93)",
           "CKA: 0.49<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(90, 94)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(90, 95)",
           "CKA: 0.515<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(90, 96)",
           "CKA: 0.504<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(90, 97)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(90, 98)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(90, 99)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(90, 100)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(90, 101)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(90, 102)",
           "CKA: 0.553<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: roberta.encoder.layer.6.output<br>(90, 103)",
           "CKA: 0.513<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: classifier.dropout<br>(90, 104)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: classifier.dense<br>(90, 105)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: classifier.out_proj<br>(90, 106)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.6.attention.self.query<br>Pretrained: classifier<br>(90, 107)"
          ],
          [
           "CKA: 0.472<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings.word_embeddings<br>(91, 0)",
           "CKA: -2.74e-12<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(91, 1)",
           "CKA: 0.00174<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings.position_embeddings<br>(91, 2)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings.LayerNorm<br>(91, 3)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings.dropout<br>(91, 4)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.embeddings<br>(91, 5)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(91, 6)",
           "CKA: 0.37<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(91, 7)",
           "CKA: 0.457<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(91, 8)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(91, 9)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(91, 10)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(91, 11)",
           "CKA: 0.491<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(91, 12)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(91, 13)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(91, 14)",
           "CKA: 0.498<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(91, 15)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(91, 16)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(91, 17)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(91, 18)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.0.output<br>(91, 19)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(91, 20)",
           "CKA: 0.409<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(91, 21)",
           "CKA: 0.478<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(91, 22)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(91, 23)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(91, 24)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(91, 25)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(91, 26)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(91, 27)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(91, 28)",
           "CKA: 0.524<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(91, 29)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(91, 30)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(91, 31)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(91, 32)",
           "CKA: 0.518<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.1.output<br>(91, 33)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(91, 34)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(91, 35)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(91, 36)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(91, 37)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(91, 38)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(91, 39)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(91, 40)",
           "CKA: 0.526<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(91, 41)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(91, 42)",
           "CKA: 0.529<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(91, 43)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(91, 44)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(91, 45)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(91, 46)",
           "CKA: 0.521<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.2.output<br>(91, 47)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(91, 48)",
           "CKA: 0.417<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(91, 49)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(91, 50)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(91, 51)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(91, 52)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(91, 53)",
           "CKA: 0.507<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(91, 54)",
           "CKA: 0.485<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(91, 55)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(91, 56)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(91, 57)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(91, 58)",
           "CKA: 0.466<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(91, 59)",
           "CKA: 0.509<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(91, 60)",
           "CKA: 0.509<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.3.output<br>(91, 61)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(91, 62)",
           "CKA: 0.447<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(91, 63)",
           "CKA: 0.534<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(91, 64)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(91, 65)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(91, 66)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(91, 67)",
           "CKA: 0.494<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(91, 68)",
           "CKA: 0.463<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(91, 69)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(91, 70)",
           "CKA: 0.467<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(91, 71)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(91, 72)",
           "CKA: 0.456<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(91, 73)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(91, 74)",
           "CKA: 0.483<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.4.output<br>(91, 75)",
           "CKA: 0.369<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(91, 76)",
           "CKA: 0.398<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(91, 77)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(91, 78)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(91, 79)",
           "CKA: 0.287<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(91, 80)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(91, 81)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(91, 82)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(91, 83)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(91, 84)",
           "CKA: 0.4<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(91, 85)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(91, 86)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(91, 87)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(91, 88)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.5.output<br>(91, 89)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(91, 90)",
           "CKA: 0.434<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(91, 91)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(91, 92)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(91, 93)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(91, 94)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(91, 95)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(91, 96)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(91, 97)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(91, 98)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(91, 99)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(91, 100)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(91, 101)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(91, 102)",
           "CKA: 0.394<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: roberta.encoder.layer.6.output<br>(91, 103)",
           "CKA: 0.331<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: classifier.dropout<br>(91, 104)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: classifier.dense<br>(91, 105)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: classifier.out_proj<br>(91, 106)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.attention.self.key<br>Pretrained: classifier<br>(91, 107)"
          ],
          [
           "CKA: 0.276<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings.word_embeddings<br>(92, 0)",
           "CKA: -9.08e-12<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(92, 1)",
           "CKA: 0.00192<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings.position_embeddings<br>(92, 2)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings.LayerNorm<br>(92, 3)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings.dropout<br>(92, 4)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.embeddings<br>(92, 5)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(92, 6)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(92, 7)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(92, 8)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(92, 9)",
           "CKA: 0.155<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(92, 10)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(92, 11)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(92, 12)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(92, 13)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(92, 14)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(92, 15)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(92, 16)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(92, 17)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(92, 18)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.0.output<br>(92, 19)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(92, 20)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(92, 21)",
           "CKA: 0.276<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(92, 22)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(92, 23)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(92, 24)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(92, 25)",
           "CKA: 0.326<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(92, 26)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(92, 27)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(92, 28)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(92, 29)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(92, 30)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(92, 31)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(92, 32)",
           "CKA: 0.315<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.1.output<br>(92, 33)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(92, 34)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(92, 35)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(92, 36)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(92, 37)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(92, 38)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(92, 39)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(92, 40)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(92, 41)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(92, 42)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(92, 43)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(92, 44)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(92, 45)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(92, 46)",
           "CKA: 0.345<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.2.output<br>(92, 47)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(92, 48)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(92, 49)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(92, 50)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(92, 51)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(92, 52)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(92, 53)",
           "CKA: 0.366<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(92, 54)",
           "CKA: 0.362<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(92, 55)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(92, 56)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(92, 57)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(92, 58)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(92, 59)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(92, 60)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.3.output<br>(92, 61)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(92, 62)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(92, 63)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(92, 64)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(92, 65)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(92, 66)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(92, 67)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(92, 68)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(92, 69)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(92, 70)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(92, 71)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(92, 72)",
           "CKA: 0.435<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(92, 73)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(92, 74)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.4.output<br>(92, 75)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(92, 76)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(92, 77)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(92, 78)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(92, 79)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(92, 80)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(92, 81)",
           "CKA: 0.44<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(92, 82)",
           "CKA: 0.465<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(92, 83)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(92, 84)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(92, 85)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(92, 86)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(92, 87)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(92, 88)",
           "CKA: 0.472<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.5.output<br>(92, 89)",
           "CKA: 0.506<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(92, 90)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(92, 91)",
           "CKA: 0.502<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(92, 92)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(92, 93)",
           "CKA: 0.468<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(92, 94)",
           "CKA: 0.493<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(92, 95)",
           "CKA: 0.493<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(92, 96)",
           "CKA: 0.459<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(92, 97)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(92, 98)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(92, 99)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(92, 100)",
           "CKA: 0.423<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(92, 101)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(92, 102)",
           "CKA: 0.503<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: roberta.encoder.layer.6.output<br>(92, 103)",
           "CKA: 0.489<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: classifier.dropout<br>(92, 104)",
           "CKA: 0.5<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: classifier.dense<br>(92, 105)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: classifier.out_proj<br>(92, 106)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.6.attention.self.value<br>Pretrained: classifier<br>(92, 107)"
          ],
          [
           "CKA: 0.151<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(93, 0)",
           "CKA: -8.43e-12<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(93, 1)",
           "CKA: 0.00301<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(93, 2)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(93, 3)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(93, 4)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.embeddings<br>(93, 5)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(93, 6)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(93, 7)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(93, 8)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(93, 9)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(93, 10)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(93, 11)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(93, 12)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(93, 13)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(93, 14)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(93, 15)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(93, 16)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(93, 17)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(93, 18)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(93, 19)",
           "CKA: 0.141<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(93, 20)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(93, 21)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(93, 22)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(93, 23)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(93, 24)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(93, 25)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(93, 26)",
           "CKA: 0.192<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(93, 27)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(93, 28)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(93, 29)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(93, 30)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(93, 31)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(93, 32)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(93, 33)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(93, 34)",
           "CKA: 0.169<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(93, 35)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(93, 36)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(93, 37)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(93, 38)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(93, 39)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(93, 40)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(93, 41)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(93, 42)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(93, 43)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(93, 44)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(93, 45)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(93, 46)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(93, 47)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(93, 48)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(93, 49)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(93, 50)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(93, 51)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(93, 52)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(93, 53)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(93, 54)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(93, 55)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(93, 56)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(93, 57)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(93, 58)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(93, 59)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(93, 60)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(93, 61)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(93, 62)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(93, 63)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(93, 64)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(93, 65)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(93, 66)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(93, 67)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(93, 68)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(93, 69)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(93, 70)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(93, 71)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(93, 72)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(93, 73)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(93, 74)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(93, 75)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(93, 76)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(93, 77)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(93, 78)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(93, 79)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(93, 80)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(93, 81)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(93, 82)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(93, 83)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(93, 84)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(93, 85)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(93, 86)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(93, 87)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(93, 88)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(93, 89)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(93, 90)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(93, 91)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(93, 92)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(93, 93)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(93, 94)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(93, 95)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(93, 96)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(93, 97)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(93, 98)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(93, 99)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(93, 100)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(93, 101)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(93, 102)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(93, 103)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: classifier.dropout<br>(93, 104)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: classifier.dense<br>(93, 105)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: classifier.out_proj<br>(93, 106)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dense<br>Pretrained: classifier<br>(93, 107)"
          ],
          [
           "CKA: 0.151<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(94, 0)",
           "CKA: -8.43e-12<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(94, 1)",
           "CKA: 0.00301<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(94, 2)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(94, 3)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(94, 4)",
           "CKA: 0.157<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.embeddings<br>(94, 5)",
           "CKA: 0.119<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(94, 6)",
           "CKA: 0.113<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(94, 7)",
           "CKA: 0.143<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(94, 8)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(94, 9)",
           "CKA: 0.145<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(94, 10)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(94, 11)",
           "CKA: 0.161<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(94, 12)",
           "CKA: 0.149<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(94, 13)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(94, 14)",
           "CKA: 0.147<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(94, 15)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(94, 16)",
           "CKA: 0.134<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(94, 17)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(94, 18)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(94, 19)",
           "CKA: 0.141<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(94, 20)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(94, 21)",
           "CKA: 0.162<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(94, 22)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(94, 23)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(94, 24)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(94, 25)",
           "CKA: 0.209<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(94, 26)",
           "CKA: 0.192<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(94, 27)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(94, 28)",
           "CKA: 0.178<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(94, 29)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(94, 30)",
           "CKA: 0.153<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(94, 31)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(94, 32)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(94, 33)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(94, 34)",
           "CKA: 0.169<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(94, 35)",
           "CKA: 0.21<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(94, 36)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(94, 37)",
           "CKA: 0.313<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(94, 38)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(94, 39)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(94, 40)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(94, 41)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(94, 42)",
           "CKA: 0.217<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(94, 43)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(94, 44)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(94, 45)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(94, 46)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(94, 47)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(94, 48)",
           "CKA: 0.196<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(94, 49)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(94, 50)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(94, 51)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(94, 52)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(94, 53)",
           "CKA: 0.28<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(94, 54)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(94, 55)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(94, 56)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(94, 57)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(94, 58)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(94, 59)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(94, 60)",
           "CKA: 0.278<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(94, 61)",
           "CKA: 0.282<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(94, 62)",
           "CKA: 0.254<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(94, 63)",
           "CKA: 0.241<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(94, 64)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(94, 65)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(94, 66)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(94, 67)",
           "CKA: 0.329<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(94, 68)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(94, 69)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(94, 70)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(94, 71)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(94, 72)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(94, 73)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(94, 74)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(94, 75)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(94, 76)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(94, 77)",
           "CKA: 0.461<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(94, 78)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(94, 79)",
           "CKA: 0.46<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(94, 80)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(94, 81)",
           "CKA: 0.421<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(94, 82)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(94, 83)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(94, 84)",
           "CKA: 0.511<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(94, 85)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(94, 86)",
           "CKA: 0.554<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(94, 87)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(94, 88)",
           "CKA: 0.464<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(94, 89)",
           "CKA: 0.522<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(94, 90)",
           "CKA: 0.451<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(94, 91)",
           "CKA: 0.533<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(94, 92)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(94, 93)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(94, 94)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(94, 95)",
           "CKA: 0.501<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(94, 96)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(94, 97)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(94, 98)",
           "CKA: 0.499<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(94, 99)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(94, 100)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(94, 101)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(94, 102)",
           "CKA: 0.516<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(94, 103)",
           "CKA: 0.547<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: classifier.dropout<br>(94, 104)",
           "CKA: 0.568<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: classifier.dense<br>(94, 105)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: classifier.out_proj<br>(94, 106)",
           "CKA: 0.528<br>Base: roberta.encoder.layer.6.attention.output.dropout<br>Pretrained: classifier<br>(94, 107)"
          ],
          [
           "CKA: 0.312<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(95, 0)",
           "CKA: -2.16e-11<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(95, 1)",
           "CKA: 0.00294<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(95, 2)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(95, 3)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(95, 4)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(95, 5)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(95, 6)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(95, 7)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(95, 8)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(95, 9)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(95, 10)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(95, 11)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(95, 12)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(95, 13)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(95, 14)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(95, 15)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(95, 16)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(95, 17)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(95, 18)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(95, 19)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(95, 20)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(95, 21)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(95, 22)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(95, 23)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(95, 24)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(95, 25)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(95, 26)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(95, 27)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(95, 28)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(95, 29)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(95, 30)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(95, 31)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(95, 32)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(95, 33)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(95, 34)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(95, 35)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(95, 36)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(95, 37)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(95, 38)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(95, 39)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(95, 40)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(95, 41)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(95, 42)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(95, 43)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(95, 44)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(95, 45)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(95, 46)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(95, 47)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(95, 48)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(95, 49)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(95, 50)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(95, 51)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(95, 52)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(95, 53)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(95, 54)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(95, 55)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(95, 56)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(95, 57)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(95, 58)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(95, 59)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(95, 60)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(95, 61)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(95, 62)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(95, 63)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(95, 64)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(95, 65)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(95, 66)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(95, 67)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(95, 68)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(95, 69)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(95, 70)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(95, 71)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(95, 72)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(95, 73)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(95, 74)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(95, 75)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(95, 76)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(95, 77)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(95, 78)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(95, 79)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(95, 80)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(95, 81)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(95, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(95, 83)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(95, 84)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(95, 85)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(95, 86)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(95, 87)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(95, 88)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(95, 89)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(95, 90)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(95, 91)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(95, 92)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(95, 93)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(95, 94)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(95, 95)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(95, 96)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(95, 97)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(95, 98)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(95, 99)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(95, 100)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(95, 101)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(95, 102)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(95, 103)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: classifier.dropout<br>(95, 104)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: classifier.dense<br>(95, 105)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(95, 106)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.6.attention.output.LayerNorm<br>Pretrained: classifier<br>(95, 107)"
          ],
          [
           "CKA: 0.312<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(96, 0)",
           "CKA: -2.16e-11<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(96, 1)",
           "CKA: 0.00294<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(96, 2)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(96, 3)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings.dropout<br>(96, 4)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.embeddings<br>(96, 5)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(96, 6)",
           "CKA: 0.247<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(96, 7)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(96, 8)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(96, 9)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(96, 10)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(96, 11)",
           "CKA: 0.325<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(96, 12)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(96, 13)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(96, 14)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(96, 15)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(96, 16)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(96, 17)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(96, 18)",
           "CKA: 0.332<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.0.output<br>(96, 19)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(96, 20)",
           "CKA: 0.283<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(96, 21)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(96, 22)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(96, 23)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(96, 24)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(96, 25)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(96, 26)",
           "CKA: 0.344<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(96, 27)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(96, 28)",
           "CKA: 0.337<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(96, 29)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(96, 30)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(96, 31)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(96, 32)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.1.output<br>(96, 33)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(96, 34)",
           "CKA: 0.305<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(96, 35)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(96, 36)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(96, 37)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(96, 38)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(96, 39)",
           "CKA: 0.374<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(96, 40)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(96, 41)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(96, 42)",
           "CKA: 0.356<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(96, 43)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(96, 44)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(96, 45)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(96, 46)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.2.output<br>(96, 47)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(96, 48)",
           "CKA: 0.302<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(96, 49)",
           "CKA: 0.33<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(96, 50)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(96, 51)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(96, 52)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(96, 53)",
           "CKA: 0.384<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(96, 54)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(96, 55)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(96, 56)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(96, 57)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(96, 58)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(96, 59)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(96, 60)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.3.output<br>(96, 61)",
           "CKA: 0.308<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(96, 62)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(96, 63)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(96, 64)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(96, 65)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(96, 66)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(96, 67)",
           "CKA: 0.42<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(96, 68)",
           "CKA: 0.426<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(96, 69)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(96, 70)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(96, 71)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(96, 72)",
           "CKA: 0.439<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(96, 73)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(96, 74)",
           "CKA: 0.438<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.4.output<br>(96, 75)",
           "CKA: 0.408<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(96, 76)",
           "CKA: 0.388<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(96, 77)",
           "CKA: 0.436<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(96, 78)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(96, 79)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(96, 80)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(96, 81)",
           "CKA: 0.443<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(96, 82)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(96, 83)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(96, 84)",
           "CKA: 0.442<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(96, 85)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(96, 86)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(96, 87)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(96, 88)",
           "CKA: 0.474<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.5.output<br>(96, 89)",
           "CKA: 0.486<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(96, 90)",
           "CKA: 0.475<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(96, 91)",
           "CKA: 0.48<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(96, 92)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(96, 93)",
           "CKA: 0.428<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(96, 94)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(96, 95)",
           "CKA: 0.488<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(96, 96)",
           "CKA: 0.448<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(96, 97)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(96, 98)",
           "CKA: 0.452<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(96, 99)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(96, 100)",
           "CKA: 0.415<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(96, 101)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(96, 102)",
           "CKA: 0.487<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: roberta.encoder.layer.6.output<br>(96, 103)",
           "CKA: 0.453<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: classifier.dropout<br>(96, 104)",
           "CKA: 0.458<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: classifier.dense<br>(96, 105)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: classifier.out_proj<br>(96, 106)",
           "CKA: 0.422<br>Base: roberta.encoder.layer.6.attention.output<br>Pretrained: classifier<br>(96, 107)"
          ],
          [
           "CKA: 0.229<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(97, 0)",
           "CKA: -2.08e-12<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(97, 1)",
           "CKA: 0.00313<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(97, 2)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(97, 3)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings.dropout<br>(97, 4)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.embeddings<br>(97, 5)",
           "CKA: 0.174<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(97, 6)",
           "CKA: 0.192<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(97, 7)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(97, 8)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(97, 9)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(97, 10)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(97, 11)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(97, 12)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(97, 13)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(97, 14)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(97, 15)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(97, 16)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(97, 17)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(97, 18)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(97, 19)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(97, 20)",
           "CKA: 0.229<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(97, 21)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(97, 22)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(97, 23)",
           "CKA: 0.166<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(97, 24)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(97, 25)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(97, 26)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(97, 27)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(97, 28)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(97, 29)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(97, 30)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(97, 31)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(97, 32)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(97, 33)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(97, 34)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(97, 35)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(97, 36)",
           "CKA: 0.189<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(97, 37)",
           "CKA: 0.189<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(97, 38)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(97, 39)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(97, 40)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(97, 41)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(97, 42)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(97, 43)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(97, 44)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(97, 45)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(97, 46)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(97, 47)",
           "CKA: 0.239<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(97, 48)",
           "CKA: 0.218<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(97, 49)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(97, 50)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(97, 51)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(97, 52)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(97, 53)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(97, 54)",
           "CKA: 0.297<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(97, 55)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(97, 56)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(97, 57)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(97, 58)",
           "CKA: 0.268<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(97, 59)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(97, 60)",
           "CKA: 0.293<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(97, 61)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(97, 62)",
           "CKA: 0.245<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(97, 63)",
           "CKA: 0.3<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(97, 64)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(97, 65)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(97, 66)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(97, 67)",
           "CKA: 0.339<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(97, 68)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(97, 69)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(97, 70)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(97, 71)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(97, 72)",
           "CKA: 0.367<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(97, 73)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(97, 74)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(97, 75)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(97, 76)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(97, 77)",
           "CKA: 0.351<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(97, 78)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(97, 79)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(97, 80)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(97, 81)",
           "CKA: 0.36<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(97, 82)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(97, 83)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(97, 84)",
           "CKA: 0.357<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(97, 85)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(97, 86)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(97, 87)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(97, 88)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(97, 89)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(97, 90)",
           "CKA: 0.413<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(97, 91)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(97, 92)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(97, 93)",
           "CKA: 0.375<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(97, 94)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(97, 95)",
           "CKA: 0.416<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(97, 96)",
           "CKA: 0.391<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(97, 97)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(97, 98)",
           "CKA: 0.396<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(97, 99)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(97, 100)",
           "CKA: 0.39<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(97, 101)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(97, 102)",
           "CKA: 0.444<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(97, 103)",
           "CKA: 0.389<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: classifier.dropout<br>(97, 104)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: classifier.dense<br>(97, 105)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: classifier.out_proj<br>(97, 106)",
           "CKA: 0.354<br>Base: roberta.encoder.layer.6.intermediate.dense<br>Pretrained: classifier<br>(97, 107)"
          ],
          [
           "CKA: 0.227<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.word_embeddings<br>(98, 0)",
           "CKA: -5.33e-12<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(98, 1)",
           "CKA: 0.00305<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.position_embeddings<br>(98, 2)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.LayerNorm<br>(98, 3)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings.dropout<br>(98, 4)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.embeddings<br>(98, 5)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(98, 6)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(98, 7)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(98, 8)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(98, 9)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(98, 10)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(98, 11)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(98, 12)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(98, 13)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(98, 14)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(98, 15)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(98, 16)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(98, 17)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(98, 18)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.0.output<br>(98, 19)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(98, 20)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(98, 21)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(98, 22)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(98, 23)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(98, 24)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(98, 25)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(98, 26)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(98, 27)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(98, 28)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(98, 29)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(98, 30)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(98, 31)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(98, 32)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.1.output<br>(98, 33)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(98, 34)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(98, 35)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(98, 36)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(98, 37)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(98, 38)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(98, 39)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(98, 40)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(98, 41)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(98, 42)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(98, 43)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(98, 44)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(98, 45)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(98, 46)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.2.output<br>(98, 47)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(98, 48)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(98, 49)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(98, 50)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(98, 51)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(98, 52)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(98, 53)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(98, 54)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(98, 55)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(98, 56)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(98, 57)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(98, 58)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(98, 59)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(98, 60)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.3.output<br>(98, 61)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(98, 62)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(98, 63)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(98, 64)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(98, 65)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(98, 66)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(98, 67)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(98, 68)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(98, 69)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(98, 70)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(98, 71)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(98, 72)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(98, 73)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(98, 74)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.4.output<br>(98, 75)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(98, 76)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(98, 77)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(98, 78)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(98, 79)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(98, 80)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(98, 81)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(98, 82)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(98, 83)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(98, 84)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(98, 85)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(98, 86)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(98, 87)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(98, 88)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.5.output<br>(98, 89)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(98, 90)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(98, 91)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(98, 92)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(98, 93)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(98, 94)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(98, 95)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(98, 96)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(98, 97)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(98, 98)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(98, 99)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(98, 100)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(98, 101)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(98, 102)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: roberta.encoder.layer.6.output<br>(98, 103)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: classifier.dropout<br>(98, 104)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: classifier.dense<br>(98, 105)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: classifier.out_proj<br>(98, 106)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>Pretrained: classifier<br>(98, 107)"
          ],
          [
           "CKA: 0.227<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings.word_embeddings<br>(99, 0)",
           "CKA: -5.33e-12<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(99, 1)",
           "CKA: 0.00305<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings.position_embeddings<br>(99, 2)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings.LayerNorm<br>(99, 3)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings.dropout<br>(99, 4)",
           "CKA: 0.236<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.embeddings<br>(99, 5)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(99, 6)",
           "CKA: 0.176<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(99, 7)",
           "CKA: 0.225<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(99, 8)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(99, 9)",
           "CKA: 0.183<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(99, 10)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(99, 11)",
           "CKA: 0.242<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(99, 12)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(99, 13)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(99, 14)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(99, 15)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(99, 16)",
           "CKA: 0.219<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(99, 17)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(99, 18)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.0.output<br>(99, 19)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(99, 20)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(99, 21)",
           "CKA: 0.235<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(99, 22)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(99, 23)",
           "CKA: 0.18<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(99, 24)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(99, 25)",
           "CKA: 0.265<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(99, 26)",
           "CKA: 0.257<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(99, 27)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(99, 28)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(99, 29)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(99, 30)",
           "CKA: 0.23<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(99, 31)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(99, 32)",
           "CKA: 0.259<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.1.output<br>(99, 33)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(99, 34)",
           "CKA: 0.231<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(99, 35)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(99, 36)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(99, 37)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(99, 38)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(99, 39)",
           "CKA: 0.277<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(99, 40)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(99, 41)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(99, 42)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(99, 43)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(99, 44)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(99, 45)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(99, 46)",
           "CKA: 0.269<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.2.output<br>(99, 47)",
           "CKA: 0.238<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(99, 48)",
           "CKA: 0.215<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(99, 49)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(99, 50)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(99, 51)",
           "CKA: 0.197<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(99, 52)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(99, 53)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(99, 54)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(99, 55)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(99, 56)",
           "CKA: 0.285<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(99, 57)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(99, 58)",
           "CKA: 0.26<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(99, 59)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(99, 60)",
           "CKA: 0.289<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.3.output<br>(99, 61)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(99, 62)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(99, 63)",
           "CKA: 0.292<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(99, 64)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(99, 65)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(99, 66)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(99, 67)",
           "CKA: 0.328<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(99, 68)",
           "CKA: 0.346<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(99, 69)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(99, 70)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(99, 71)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(99, 72)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(99, 73)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(99, 74)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.4.output<br>(99, 75)",
           "CKA: 0.333<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(99, 76)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(99, 77)",
           "CKA: 0.338<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(99, 78)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(99, 79)",
           "CKA: 0.272<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(99, 80)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(99, 81)",
           "CKA: 0.347<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(99, 82)",
           "CKA: 0.343<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(99, 83)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(99, 84)",
           "CKA: 0.336<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(99, 85)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(99, 86)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(99, 87)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(99, 88)",
           "CKA: 0.376<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.5.output<br>(99, 89)",
           "CKA: 0.403<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(99, 90)",
           "CKA: 0.387<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(99, 91)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(99, 92)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(99, 93)",
           "CKA: 0.364<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(99, 94)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(99, 95)",
           "CKA: 0.402<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(99, 96)",
           "CKA: 0.38<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(99, 97)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(99, 98)",
           "CKA: 0.378<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(99, 99)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(99, 100)",
           "CKA: 0.372<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(99, 101)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(99, 102)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: roberta.encoder.layer.6.output<br>(99, 103)",
           "CKA: 0.373<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: classifier.dropout<br>(99, 104)",
           "CKA: 0.383<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: classifier.dense<br>(99, 105)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: classifier.out_proj<br>(99, 106)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.intermediate<br>Pretrained: classifier<br>(99, 107)"
          ],
          [
           "CKA: 0.189<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(100, 0)",
           "CKA: 1.16e-11<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(100, 1)",
           "CKA: 0.00406<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(100, 2)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(100, 3)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings.dropout<br>(100, 4)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.embeddings<br>(100, 5)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(100, 6)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(100, 7)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(100, 8)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(100, 9)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(100, 10)",
           "CKA: 0.205<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(100, 11)",
           "CKA: 0.205<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(100, 12)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(100, 13)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(100, 14)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(100, 15)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(100, 16)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(100, 17)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(100, 18)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(100, 19)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(100, 20)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(100, 21)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(100, 22)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(100, 23)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(100, 24)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(100, 25)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(100, 26)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(100, 27)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(100, 28)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(100, 29)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(100, 30)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(100, 31)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(100, 32)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(100, 33)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(100, 34)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(100, 35)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(100, 36)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(100, 37)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(100, 38)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(100, 39)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(100, 40)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(100, 41)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(100, 42)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(100, 43)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(100, 44)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(100, 45)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(100, 46)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(100, 47)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(100, 48)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(100, 49)",
           "CKA: 0.218<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(100, 50)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(100, 51)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(100, 52)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(100, 53)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(100, 54)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(100, 55)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(100, 56)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(100, 57)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(100, 58)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(100, 59)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(100, 60)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(100, 61)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(100, 62)",
           "CKA: 0.188<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(100, 63)",
           "CKA: 0.262<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(100, 64)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(100, 65)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(100, 66)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(100, 67)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(100, 68)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(100, 69)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(100, 70)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(100, 71)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(100, 72)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(100, 73)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(100, 74)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(100, 75)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(100, 76)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(100, 77)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(100, 78)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(100, 79)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(100, 80)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(100, 81)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(100, 82)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(100, 83)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(100, 84)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(100, 85)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(100, 86)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(100, 87)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(100, 88)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(100, 89)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(100, 90)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(100, 91)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(100, 92)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(100, 93)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(100, 94)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(100, 95)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(100, 96)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(100, 97)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(100, 98)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(100, 99)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(100, 100)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(100, 101)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(100, 102)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(100, 103)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: classifier.dropout<br>(100, 104)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: classifier.dense<br>(100, 105)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: classifier.out_proj<br>(100, 106)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.6.output.dense<br>Pretrained: classifier<br>(100, 107)"
          ],
          [
           "CKA: 0.189<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(101, 0)",
           "CKA: 1.16e-11<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(101, 1)",
           "CKA: 0.00406<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(101, 2)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(101, 3)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings.dropout<br>(101, 4)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.embeddings<br>(101, 5)",
           "CKA: 0.139<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(101, 6)",
           "CKA: 0.16<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(101, 7)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(101, 8)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(101, 9)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(101, 10)",
           "CKA: 0.205<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(101, 11)",
           "CKA: 0.205<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(101, 12)",
           "CKA: 0.194<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(101, 13)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(101, 14)",
           "CKA: 0.201<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(101, 15)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(101, 16)",
           "CKA: 0.19<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(101, 17)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(101, 18)",
           "CKA: 0.212<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(101, 19)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(101, 20)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(101, 21)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(101, 22)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(101, 23)",
           "CKA: 0.129<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(101, 24)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(101, 25)",
           "CKA: 0.223<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(101, 26)",
           "CKA: 0.221<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(101, 27)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(101, 28)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(101, 29)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(101, 30)",
           "CKA: 0.207<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(101, 31)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(101, 32)",
           "CKA: 0.222<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(101, 33)",
           "CKA: 0.203<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(101, 34)",
           "CKA: 0.204<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(101, 35)",
           "CKA: 0.211<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(101, 36)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(101, 37)",
           "CKA: 0.144<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(101, 38)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(101, 39)",
           "CKA: 0.232<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(101, 40)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(101, 41)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(101, 42)",
           "CKA: 0.233<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(101, 43)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(101, 44)",
           "CKA: 0.208<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(101, 45)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(101, 46)",
           "CKA: 0.228<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(101, 47)",
           "CKA: 0.198<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(101, 48)",
           "CKA: 0.186<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(101, 49)",
           "CKA: 0.218<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(101, 50)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(101, 51)",
           "CKA: 0.2<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(101, 52)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(101, 53)",
           "CKA: 0.25<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(101, 54)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(101, 55)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(101, 56)",
           "CKA: 0.248<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(101, 57)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(101, 58)",
           "CKA: 0.226<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(101, 59)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(101, 60)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(101, 61)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(101, 62)",
           "CKA: 0.188<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(101, 63)",
           "CKA: 0.262<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(101, 64)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(101, 65)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(101, 66)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(101, 67)",
           "CKA: 0.299<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(101, 68)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(101, 69)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(101, 70)",
           "CKA: 0.334<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(101, 71)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(101, 72)",
           "CKA: 0.335<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(101, 73)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(101, 74)",
           "CKA: 0.314<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(101, 75)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(101, 76)",
           "CKA: 0.304<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(101, 77)",
           "CKA: 0.318<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(101, 78)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(101, 79)",
           "CKA: 0.27<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(101, 80)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(101, 81)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(101, 82)",
           "CKA: 0.316<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(101, 83)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(101, 84)",
           "CKA: 0.31<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(101, 85)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(101, 86)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(101, 87)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(101, 88)",
           "CKA: 0.342<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(101, 89)",
           "CKA: 0.371<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(101, 90)",
           "CKA: 0.363<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(101, 91)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(101, 92)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(101, 93)",
           "CKA: 0.324<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(101, 94)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(101, 95)",
           "CKA: 0.359<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(101, 96)",
           "CKA: 0.34<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(101, 97)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(101, 98)",
           "CKA: 0.353<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(101, 99)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(101, 100)",
           "CKA: 0.355<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(101, 101)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(101, 102)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(101, 103)",
           "CKA: 0.309<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: classifier.dropout<br>(101, 104)",
           "CKA: 0.32<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: classifier.dense<br>(101, 105)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: classifier.out_proj<br>(101, 106)",
           "CKA: 0.274<br>Base: roberta.encoder.layer.6.output.dropout<br>Pretrained: classifier<br>(101, 107)"
          ],
          [
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings.word_embeddings<br>(102, 0)",
           "CKA: -1.87e-11<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(102, 1)",
           "CKA: 0.00192<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings.position_embeddings<br>(102, 2)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings.LayerNorm<br>(102, 3)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings.dropout<br>(102, 4)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.embeddings<br>(102, 5)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(102, 6)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(102, 7)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(102, 8)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(102, 9)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(102, 10)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(102, 11)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(102, 12)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(102, 13)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(102, 14)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(102, 15)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(102, 16)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(102, 17)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(102, 18)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.0.output<br>(102, 19)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(102, 20)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(102, 21)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(102, 22)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(102, 23)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(102, 24)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(102, 25)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(102, 26)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(102, 27)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(102, 28)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(102, 29)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(102, 30)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(102, 31)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(102, 32)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.1.output<br>(102, 33)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(102, 34)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(102, 35)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(102, 36)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(102, 37)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(102, 38)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(102, 39)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(102, 40)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(102, 41)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(102, 42)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(102, 43)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(102, 44)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(102, 45)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(102, 46)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.2.output<br>(102, 47)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(102, 48)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(102, 49)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(102, 50)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(102, 51)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(102, 52)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(102, 53)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(102, 54)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(102, 55)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(102, 56)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(102, 57)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(102, 58)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(102, 59)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(102, 60)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.3.output<br>(102, 61)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(102, 62)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(102, 63)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(102, 64)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(102, 65)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(102, 66)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(102, 67)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(102, 68)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(102, 69)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(102, 70)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(102, 71)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(102, 72)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(102, 73)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(102, 74)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.4.output<br>(102, 75)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(102, 76)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(102, 77)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(102, 78)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(102, 79)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(102, 80)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(102, 81)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(102, 82)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(102, 83)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(102, 84)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(102, 85)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(102, 86)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(102, 87)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(102, 88)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.5.output<br>(102, 89)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(102, 90)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(102, 91)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(102, 92)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(102, 93)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(102, 94)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(102, 95)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(102, 96)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(102, 97)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(102, 98)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(102, 99)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(102, 100)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(102, 101)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(102, 102)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: roberta.encoder.layer.6.output<br>(102, 103)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: classifier.dropout<br>(102, 104)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: classifier.dense<br>(102, 105)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: classifier.out_proj<br>(102, 106)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.6.output.LayerNorm<br>Pretrained: classifier<br>(102, 107)"
          ],
          [
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings.word_embeddings<br>(103, 0)",
           "CKA: -1.87e-11<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(103, 1)",
           "CKA: 0.00192<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings.position_embeddings<br>(103, 2)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings.LayerNorm<br>(103, 3)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings.dropout<br>(103, 4)",
           "CKA: 0.263<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.embeddings<br>(103, 5)",
           "CKA: 0.185<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(103, 6)",
           "CKA: 0.199<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(103, 7)",
           "CKA: 0.249<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(103, 8)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(103, 9)",
           "CKA: 0.187<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(103, 10)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(103, 11)",
           "CKA: 0.267<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(103, 12)",
           "CKA: 0.252<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(103, 13)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(103, 14)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(103, 15)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(103, 16)",
           "CKA: 0.237<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(103, 17)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(103, 18)",
           "CKA: 0.273<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.0.output<br>(103, 19)",
           "CKA: 0.251<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(103, 20)",
           "CKA: 0.244<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(103, 21)",
           "CKA: 0.258<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(103, 22)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(103, 23)",
           "CKA: 0.214<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(103, 24)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(103, 25)",
           "CKA: 0.294<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(103, 26)",
           "CKA: 0.286<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(103, 27)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(103, 28)",
           "CKA: 0.279<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(103, 29)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(103, 30)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(103, 31)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(103, 32)",
           "CKA: 0.288<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.1.output<br>(103, 33)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(103, 34)",
           "CKA: 0.253<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(103, 35)",
           "CKA: 0.281<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(103, 36)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(103, 37)",
           "CKA: 0.234<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(103, 38)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(103, 39)",
           "CKA: 0.311<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(103, 40)",
           "CKA: 0.303<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(103, 41)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(103, 42)",
           "CKA: 0.296<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(103, 43)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(103, 44)",
           "CKA: 0.256<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(103, 45)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(103, 46)",
           "CKA: 0.301<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.2.output<br>(103, 47)",
           "CKA: 0.266<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(103, 48)",
           "CKA: 0.243<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(103, 49)",
           "CKA: 0.271<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(103, 50)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(103, 51)",
           "CKA: 0.22<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(103, 52)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(103, 53)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(103, 54)",
           "CKA: 0.319<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(103, 55)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(103, 56)",
           "CKA: 0.317<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(103, 57)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(103, 58)",
           "CKA: 0.29<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(103, 59)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(103, 60)",
           "CKA: 0.323<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.3.output<br>(103, 61)",
           "CKA: 0.255<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(103, 62)",
           "CKA: 0.275<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(103, 63)",
           "CKA: 0.321<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(103, 64)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(103, 65)",
           "CKA: 0.349<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(103, 66)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(103, 67)",
           "CKA: 0.365<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(103, 68)",
           "CKA: 0.379<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(103, 69)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(103, 70)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(103, 71)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(103, 72)",
           "CKA: 0.397<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(103, 73)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(103, 74)",
           "CKA: 0.386<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.4.output<br>(103, 75)",
           "CKA: 0.368<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(103, 76)",
           "CKA: 0.352<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(103, 77)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(103, 78)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(103, 79)",
           "CKA: 0.327<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(103, 80)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(103, 81)",
           "CKA: 0.393<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(103, 82)",
           "CKA: 0.392<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(103, 83)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(103, 84)",
           "CKA: 0.395<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(103, 85)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(103, 86)",
           "CKA: 0.401<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(103, 87)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(103, 88)",
           "CKA: 0.424<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.5.output<br>(103, 89)",
           "CKA: 0.449<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(103, 90)",
           "CKA: 0.431<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(103, 91)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(103, 92)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(103, 93)",
           "CKA: 0.411<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(103, 94)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(103, 95)",
           "CKA: 0.445<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(103, 96)",
           "CKA: 0.418<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(103, 97)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(103, 98)",
           "CKA: 0.425<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(103, 99)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(103, 100)",
           "CKA: 0.405<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(103, 101)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(103, 102)",
           "CKA: 0.47<br>Base: roberta.encoder.layer.6.output<br>Pretrained: roberta.encoder.layer.6.output<br>(103, 103)",
           "CKA: 0.414<br>Base: roberta.encoder.layer.6.output<br>Pretrained: classifier.dropout<br>(103, 104)",
           "CKA: 0.429<br>Base: roberta.encoder.layer.6.output<br>Pretrained: classifier.dense<br>(103, 105)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.6.output<br>Pretrained: classifier.out_proj<br>(103, 106)",
           "CKA: 0.382<br>Base: roberta.encoder.layer.6.output<br>Pretrained: classifier<br>(103, 107)"
          ],
          [
           "CKA: 0.107<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings.word_embeddings<br>(104, 0)",
           "CKA: -2.31e-13<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(104, 1)",
           "CKA: 0.000903<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings.position_embeddings<br>(104, 2)",
           "CKA: 0.11<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings.LayerNorm<br>(104, 3)",
           "CKA: 0.11<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings.dropout<br>(104, 4)",
           "CKA: 0.11<br>Base: classifier.dropout<br>Pretrained: roberta.embeddings<br>(104, 5)",
           "CKA: 0.0847<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(104, 6)",
           "CKA: 0.0786<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(104, 7)",
           "CKA: 0.102<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(104, 8)",
           "CKA: 0.0932<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(104, 9)",
           "CKA: 0.0932<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(104, 10)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(104, 11)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(104, 12)",
           "CKA: 0.104<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(104, 13)",
           "CKA: 0.1<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(104, 14)",
           "CKA: 0.1<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(104, 15)",
           "CKA: 0.0898<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(104, 16)",
           "CKA: 0.0898<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(104, 17)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(104, 18)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.0.output<br>(104, 19)",
           "CKA: 0.0976<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(104, 20)",
           "CKA: 0.102<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(104, 21)",
           "CKA: 0.114<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(104, 22)",
           "CKA: 0.183<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(104, 23)",
           "CKA: 0.183<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(104, 24)",
           "CKA: 0.146<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(104, 25)",
           "CKA: 0.146<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(104, 26)",
           "CKA: 0.134<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(104, 27)",
           "CKA: 0.124<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(104, 28)",
           "CKA: 0.124<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(104, 29)",
           "CKA: 0.107<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(104, 30)",
           "CKA: 0.107<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(104, 31)",
           "CKA: 0.139<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(104, 32)",
           "CKA: 0.139<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.1.output<br>(104, 33)",
           "CKA: 0.135<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(104, 34)",
           "CKA: 0.121<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(104, 35)",
           "CKA: 0.156<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(104, 36)",
           "CKA: 0.207<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(104, 37)",
           "CKA: 0.207<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(104, 38)",
           "CKA: 0.176<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(104, 39)",
           "CKA: 0.176<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(104, 40)",
           "CKA: 0.162<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(104, 41)",
           "CKA: 0.148<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(104, 42)",
           "CKA: 0.148<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(104, 43)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(104, 44)",
           "CKA: 0.111<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(104, 45)",
           "CKA: 0.165<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(104, 46)",
           "CKA: 0.165<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.2.output<br>(104, 47)",
           "CKA: 0.142<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(104, 48)",
           "CKA: 0.136<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(104, 49)",
           "CKA: 0.129<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(104, 50)",
           "CKA: 0.133<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(104, 51)",
           "CKA: 0.133<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(104, 52)",
           "CKA: 0.193<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(104, 53)",
           "CKA: 0.193<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(104, 54)",
           "CKA: 0.19<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(104, 55)",
           "CKA: 0.185<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(104, 56)",
           "CKA: 0.185<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(104, 57)",
           "CKA: 0.153<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(104, 58)",
           "CKA: 0.153<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(104, 59)",
           "CKA: 0.188<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(104, 60)",
           "CKA: 0.188<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.3.output<br>(104, 61)",
           "CKA: 0.186<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(104, 62)",
           "CKA: 0.16<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(104, 63)",
           "CKA: 0.167<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(104, 64)",
           "CKA: 0.31<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(104, 65)",
           "CKA: 0.31<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(104, 66)",
           "CKA: 0.25<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(104, 67)",
           "CKA: 0.25<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(104, 68)",
           "CKA: 0.264<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(104, 69)",
           "CKA: 0.314<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(104, 70)",
           "CKA: 0.314<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(104, 71)",
           "CKA: 0.343<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(104, 72)",
           "CKA: 0.343<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(104, 73)",
           "CKA: 0.296<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(104, 74)",
           "CKA: 0.296<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.4.output<br>(104, 75)",
           "CKA: 0.313<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(104, 76)",
           "CKA: 0.269<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(104, 77)",
           "CKA: 0.38<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(104, 78)",
           "CKA: 0.387<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(104, 79)",
           "CKA: 0.387<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(104, 80)",
           "CKA: 0.356<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(104, 81)",
           "CKA: 0.356<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(104, 82)",
           "CKA: 0.397<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(104, 83)",
           "CKA: 0.435<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(104, 84)",
           "CKA: 0.435<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(104, 85)",
           "CKA: 0.504<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(104, 86)",
           "CKA: 0.504<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(104, 87)",
           "CKA: 0.422<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(104, 88)",
           "CKA: 0.422<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.5.output<br>(104, 89)",
           "CKA: 0.532<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(104, 90)",
           "CKA: 0.43<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(104, 91)",
           "CKA: 0.545<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(104, 92)",
           "CKA: 0.544<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(104, 93)",
           "CKA: 0.544<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(104, 94)",
           "CKA: 0.511<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(104, 95)",
           "CKA: 0.511<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(104, 96)",
           "CKA: 0.517<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(104, 97)",
           "CKA: 0.532<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(104, 98)",
           "CKA: 0.532<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(104, 99)",
           "CKA: 0.493<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(104, 100)",
           "CKA: 0.493<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(104, 101)",
           "CKA: 0.552<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(104, 102)",
           "CKA: 0.552<br>Base: classifier.dropout<br>Pretrained: roberta.encoder.layer.6.output<br>(104, 103)",
           "CKA: 0.648<br>Base: classifier.dropout<br>Pretrained: classifier.dropout<br>(104, 104)",
           "CKA: 0.65<br>Base: classifier.dropout<br>Pretrained: classifier.dense<br>(104, 105)",
           "CKA: 0.636<br>Base: classifier.dropout<br>Pretrained: classifier.out_proj<br>(104, 106)",
           "CKA: 0.636<br>Base: classifier.dropout<br>Pretrained: classifier<br>(104, 107)"
          ],
          [
           "CKA: 0.107<br>Base: classifier.dense<br>Pretrained: roberta.embeddings.word_embeddings<br>(105, 0)",
           "CKA: -3.08e-13<br>Base: classifier.dense<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(105, 1)",
           "CKA: 0.00074<br>Base: classifier.dense<br>Pretrained: roberta.embeddings.position_embeddings<br>(105, 2)",
           "CKA: 0.11<br>Base: classifier.dense<br>Pretrained: roberta.embeddings.LayerNorm<br>(105, 3)",
           "CKA: 0.11<br>Base: classifier.dense<br>Pretrained: roberta.embeddings.dropout<br>(105, 4)",
           "CKA: 0.11<br>Base: classifier.dense<br>Pretrained: roberta.embeddings<br>(105, 5)",
           "CKA: 0.0826<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(105, 6)",
           "CKA: 0.0772<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(105, 7)",
           "CKA: 0.103<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(105, 8)",
           "CKA: 0.0983<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(105, 9)",
           "CKA: 0.0983<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(105, 10)",
           "CKA: 0.111<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(105, 11)",
           "CKA: 0.111<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(105, 12)",
           "CKA: 0.104<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(105, 13)",
           "CKA: 0.101<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(105, 14)",
           "CKA: 0.101<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(105, 15)",
           "CKA: 0.0899<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(105, 16)",
           "CKA: 0.0899<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(105, 17)",
           "CKA: 0.112<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(105, 18)",
           "CKA: 0.112<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.0.output<br>(105, 19)",
           "CKA: 0.1<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(105, 20)",
           "CKA: 0.106<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(105, 21)",
           "CKA: 0.115<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(105, 22)",
           "CKA: 0.209<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(105, 23)",
           "CKA: 0.209<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(105, 24)",
           "CKA: 0.152<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(105, 25)",
           "CKA: 0.152<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(105, 26)",
           "CKA: 0.14<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(105, 27)",
           "CKA: 0.129<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(105, 28)",
           "CKA: 0.129<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(105, 29)",
           "CKA: 0.113<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(105, 30)",
           "CKA: 0.113<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(105, 31)",
           "CKA: 0.145<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(105, 32)",
           "CKA: 0.145<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.1.output<br>(105, 33)",
           "CKA: 0.143<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(105, 34)",
           "CKA: 0.125<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(105, 35)",
           "CKA: 0.165<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(105, 36)",
           "CKA: 0.231<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(105, 37)",
           "CKA: 0.231<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(105, 38)",
           "CKA: 0.187<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(105, 39)",
           "CKA: 0.187<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(105, 40)",
           "CKA: 0.171<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(105, 41)",
           "CKA: 0.156<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(105, 42)",
           "CKA: 0.156<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(105, 43)",
           "CKA: 0.117<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(105, 44)",
           "CKA: 0.117<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(105, 45)",
           "CKA: 0.174<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(105, 46)",
           "CKA: 0.174<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.2.output<br>(105, 47)",
           "CKA: 0.148<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(105, 48)",
           "CKA: 0.143<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(105, 49)",
           "CKA: 0.133<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(105, 50)",
           "CKA: 0.146<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(105, 51)",
           "CKA: 0.146<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(105, 52)",
           "CKA: 0.205<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(105, 53)",
           "CKA: 0.205<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(105, 54)",
           "CKA: 0.202<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(105, 55)",
           "CKA: 0.195<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(105, 56)",
           "CKA: 0.195<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(105, 57)",
           "CKA: 0.16<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(105, 58)",
           "CKA: 0.16<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(105, 59)",
           "CKA: 0.2<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(105, 60)",
           "CKA: 0.2<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.3.output<br>(105, 61)",
           "CKA: 0.2<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(105, 62)",
           "CKA: 0.167<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(105, 63)",
           "CKA: 0.18<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(105, 64)",
           "CKA: 0.333<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(105, 65)",
           "CKA: 0.333<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(105, 66)",
           "CKA: 0.263<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(105, 67)",
           "CKA: 0.263<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(105, 68)",
           "CKA: 0.28<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(105, 69)",
           "CKA: 0.338<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(105, 70)",
           "CKA: 0.338<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(105, 71)",
           "CKA: 0.368<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(105, 72)",
           "CKA: 0.368<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(105, 73)",
           "CKA: 0.313<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(105, 74)",
           "CKA: 0.313<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.4.output<br>(105, 75)",
           "CKA: 0.331<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(105, 76)",
           "CKA: 0.291<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(105, 77)",
           "CKA: 0.408<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(105, 78)",
           "CKA: 0.427<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(105, 79)",
           "CKA: 0.427<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(105, 80)",
           "CKA: 0.371<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(105, 81)",
           "CKA: 0.371<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(105, 82)",
           "CKA: 0.414<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(105, 83)",
           "CKA: 0.46<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(105, 84)",
           "CKA: 0.46<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(105, 85)",
           "CKA: 0.531<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(105, 86)",
           "CKA: 0.531<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(105, 87)",
           "CKA: 0.434<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(105, 88)",
           "CKA: 0.434<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.5.output<br>(105, 89)",
           "CKA: 0.547<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(105, 90)",
           "CKA: 0.444<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(105, 91)",
           "CKA: 0.561<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(105, 92)",
           "CKA: 0.577<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(105, 93)",
           "CKA: 0.577<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(105, 94)",
           "CKA: 0.52<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(105, 95)",
           "CKA: 0.52<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(105, 96)",
           "CKA: 0.529<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(105, 97)",
           "CKA: 0.548<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(105, 98)",
           "CKA: 0.548<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(105, 99)",
           "CKA: 0.509<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(105, 100)",
           "CKA: 0.509<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(105, 101)",
           "CKA: 0.578<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(105, 102)",
           "CKA: 0.578<br>Base: classifier.dense<br>Pretrained: roberta.encoder.layer.6.output<br>(105, 103)",
           "CKA: 0.639<br>Base: classifier.dense<br>Pretrained: classifier.dropout<br>(105, 104)",
           "CKA: 0.659<br>Base: classifier.dense<br>Pretrained: classifier.dense<br>(105, 105)",
           "CKA: 0.622<br>Base: classifier.dense<br>Pretrained: classifier.out_proj<br>(105, 106)",
           "CKA: 0.622<br>Base: classifier.dense<br>Pretrained: classifier<br>(105, 107)"
          ],
          [
           "CKA: 0.102<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings.word_embeddings<br>(106, 0)",
           "CKA: -4.04e-14<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(106, 1)",
           "CKA: 0.000863<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings.position_embeddings<br>(106, 2)",
           "CKA: 0.105<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings.LayerNorm<br>(106, 3)",
           "CKA: 0.105<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings.dropout<br>(106, 4)",
           "CKA: 0.105<br>Base: classifier.out_proj<br>Pretrained: roberta.embeddings<br>(106, 5)",
           "CKA: 0.0814<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(106, 6)",
           "CKA: 0.0764<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(106, 7)",
           "CKA: 0.0971<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(106, 8)",
           "CKA: 0.0847<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(106, 9)",
           "CKA: 0.0847<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(106, 10)",
           "CKA: 0.106<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(106, 11)",
           "CKA: 0.106<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(106, 12)",
           "CKA: 0.0995<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(106, 13)",
           "CKA: 0.0961<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(106, 14)",
           "CKA: 0.0961<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(106, 15)",
           "CKA: 0.086<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(106, 16)",
           "CKA: 0.086<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(106, 17)",
           "CKA: 0.106<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(106, 18)",
           "CKA: 0.106<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.0.output<br>(106, 19)",
           "CKA: 0.0919<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(106, 20)",
           "CKA: 0.0956<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(106, 21)",
           "CKA: 0.109<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(106, 22)",
           "CKA: 0.178<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(106, 23)",
           "CKA: 0.178<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(106, 24)",
           "CKA: 0.14<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(106, 25)",
           "CKA: 0.14<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(106, 26)",
           "CKA: 0.128<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(106, 27)",
           "CKA: 0.119<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(106, 28)",
           "CKA: 0.119<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(106, 29)",
           "CKA: 0.103<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(106, 30)",
           "CKA: 0.103<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(106, 31)",
           "CKA: 0.134<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(106, 32)",
           "CKA: 0.134<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.1.output<br>(106, 33)",
           "CKA: 0.129<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(106, 34)",
           "CKA: 0.116<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(106, 35)",
           "CKA: 0.15<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(106, 36)",
           "CKA: 0.202<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(106, 37)",
           "CKA: 0.202<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(106, 38)",
           "CKA: 0.17<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(106, 39)",
           "CKA: 0.17<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(106, 40)",
           "CKA: 0.156<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(106, 41)",
           "CKA: 0.143<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(106, 42)",
           "CKA: 0.143<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(106, 43)",
           "CKA: 0.107<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(106, 44)",
           "CKA: 0.107<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(106, 45)",
           "CKA: 0.159<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(106, 46)",
           "CKA: 0.159<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.2.output<br>(106, 47)",
           "CKA: 0.138<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(106, 48)",
           "CKA: 0.132<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(106, 49)",
           "CKA: 0.125<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(106, 50)",
           "CKA: 0.127<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(106, 51)",
           "CKA: 0.127<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(106, 52)",
           "CKA: 0.186<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(106, 53)",
           "CKA: 0.186<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(106, 54)",
           "CKA: 0.184<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(106, 55)",
           "CKA: 0.179<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(106, 56)",
           "CKA: 0.179<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(106, 57)",
           "CKA: 0.148<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(106, 58)",
           "CKA: 0.148<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(106, 59)",
           "CKA: 0.181<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(106, 60)",
           "CKA: 0.181<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.3.output<br>(106, 61)",
           "CKA: 0.18<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(106, 62)",
           "CKA: 0.157<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(106, 63)",
           "CKA: 0.158<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(106, 64)",
           "CKA: 0.296<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(106, 65)",
           "CKA: 0.296<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(106, 66)",
           "CKA: 0.241<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(106, 67)",
           "CKA: 0.241<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(106, 68)",
           "CKA: 0.254<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(106, 69)",
           "CKA: 0.303<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(106, 70)",
           "CKA: 0.303<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(106, 71)",
           "CKA: 0.332<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(106, 72)",
           "CKA: 0.332<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(106, 73)",
           "CKA: 0.286<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(106, 74)",
           "CKA: 0.286<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.4.output<br>(106, 75)",
           "CKA: 0.301<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(106, 76)",
           "CKA: 0.257<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(106, 77)",
           "CKA: 0.368<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(106, 78)",
           "CKA: 0.377<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(106, 79)",
           "CKA: 0.377<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(106, 80)",
           "CKA: 0.347<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(106, 81)",
           "CKA: 0.347<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(106, 82)",
           "CKA: 0.388<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(106, 83)",
           "CKA: 0.425<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(106, 84)",
           "CKA: 0.425<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(106, 85)",
           "CKA: 0.495<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(106, 86)",
           "CKA: 0.495<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(106, 87)",
           "CKA: 0.411<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(106, 88)",
           "CKA: 0.411<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.5.output<br>(106, 89)",
           "CKA: 0.518<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(106, 90)",
           "CKA: 0.419<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(106, 91)",
           "CKA: 0.532<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(106, 92)",
           "CKA: 0.528<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(106, 93)",
           "CKA: 0.528<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(106, 94)",
           "CKA: 0.499<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(106, 95)",
           "CKA: 0.499<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(106, 96)",
           "CKA: 0.503<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(106, 97)",
           "CKA: 0.517<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(106, 98)",
           "CKA: 0.517<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(106, 99)",
           "CKA: 0.476<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(106, 100)",
           "CKA: 0.476<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(106, 101)",
           "CKA: 0.533<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(106, 102)",
           "CKA: 0.533<br>Base: classifier.out_proj<br>Pretrained: roberta.encoder.layer.6.output<br>(106, 103)",
           "CKA: 0.643<br>Base: classifier.out_proj<br>Pretrained: classifier.dropout<br>(106, 104)",
           "CKA: 0.641<br>Base: classifier.out_proj<br>Pretrained: classifier.dense<br>(106, 105)",
           "CKA: 0.634<br>Base: classifier.out_proj<br>Pretrained: classifier.out_proj<br>(106, 106)",
           "CKA: 0.634<br>Base: classifier.out_proj<br>Pretrained: classifier<br>(106, 107)"
          ],
          [
           "CKA: 0.102<br>Base: classifier<br>Pretrained: roberta.embeddings.word_embeddings<br>(107, 0)",
           "CKA: -4.04e-14<br>Base: classifier<br>Pretrained: roberta.embeddings.token_type_embeddings<br>(107, 1)",
           "CKA: 0.000863<br>Base: classifier<br>Pretrained: roberta.embeddings.position_embeddings<br>(107, 2)",
           "CKA: 0.105<br>Base: classifier<br>Pretrained: roberta.embeddings.LayerNorm<br>(107, 3)",
           "CKA: 0.105<br>Base: classifier<br>Pretrained: roberta.embeddings.dropout<br>(107, 4)",
           "CKA: 0.105<br>Base: classifier<br>Pretrained: roberta.embeddings<br>(107, 5)",
           "CKA: 0.0814<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.self.query<br>(107, 6)",
           "CKA: 0.0764<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.self.key<br>(107, 7)",
           "CKA: 0.0971<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.self.value<br>(107, 8)",
           "CKA: 0.0847<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.output.dense<br>(107, 9)",
           "CKA: 0.0847<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.output.dropout<br>(107, 10)",
           "CKA: 0.106<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.output.LayerNorm<br>(107, 11)",
           "CKA: 0.106<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.attention.output<br>(107, 12)",
           "CKA: 0.0995<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.intermediate.dense<br>(107, 13)",
           "CKA: 0.0961<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.intermediate.intermediate_act_fn<br>(107, 14)",
           "CKA: 0.0961<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.intermediate<br>(107, 15)",
           "CKA: 0.086<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.output.dense<br>(107, 16)",
           "CKA: 0.086<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.output.dropout<br>(107, 17)",
           "CKA: 0.106<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.output.LayerNorm<br>(107, 18)",
           "CKA: 0.106<br>Base: classifier<br>Pretrained: roberta.encoder.layer.0.output<br>(107, 19)",
           "CKA: 0.0919<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.self.query<br>(107, 20)",
           "CKA: 0.0956<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.self.key<br>(107, 21)",
           "CKA: 0.109<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.self.value<br>(107, 22)",
           "CKA: 0.178<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.output.dense<br>(107, 23)",
           "CKA: 0.178<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.output.dropout<br>(107, 24)",
           "CKA: 0.14<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.output.LayerNorm<br>(107, 25)",
           "CKA: 0.14<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.attention.output<br>(107, 26)",
           "CKA: 0.128<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.intermediate.dense<br>(107, 27)",
           "CKA: 0.119<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.intermediate.intermediate_act_fn<br>(107, 28)",
           "CKA: 0.119<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.intermediate<br>(107, 29)",
           "CKA: 0.103<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.output.dense<br>(107, 30)",
           "CKA: 0.103<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.output.dropout<br>(107, 31)",
           "CKA: 0.134<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.output.LayerNorm<br>(107, 32)",
           "CKA: 0.134<br>Base: classifier<br>Pretrained: roberta.encoder.layer.1.output<br>(107, 33)",
           "CKA: 0.129<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.self.query<br>(107, 34)",
           "CKA: 0.116<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.self.key<br>(107, 35)",
           "CKA: 0.15<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.self.value<br>(107, 36)",
           "CKA: 0.202<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.output.dense<br>(107, 37)",
           "CKA: 0.202<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.output.dropout<br>(107, 38)",
           "CKA: 0.17<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.output.LayerNorm<br>(107, 39)",
           "CKA: 0.17<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.attention.output<br>(107, 40)",
           "CKA: 0.156<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.intermediate.dense<br>(107, 41)",
           "CKA: 0.143<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.intermediate.intermediate_act_fn<br>(107, 42)",
           "CKA: 0.143<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.intermediate<br>(107, 43)",
           "CKA: 0.107<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.output.dense<br>(107, 44)",
           "CKA: 0.107<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.output.dropout<br>(107, 45)",
           "CKA: 0.159<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.output.LayerNorm<br>(107, 46)",
           "CKA: 0.159<br>Base: classifier<br>Pretrained: roberta.encoder.layer.2.output<br>(107, 47)",
           "CKA: 0.138<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.self.query<br>(107, 48)",
           "CKA: 0.132<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.self.key<br>(107, 49)",
           "CKA: 0.125<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.self.value<br>(107, 50)",
           "CKA: 0.127<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.output.dense<br>(107, 51)",
           "CKA: 0.127<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.output.dropout<br>(107, 52)",
           "CKA: 0.186<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.output.LayerNorm<br>(107, 53)",
           "CKA: 0.186<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.attention.output<br>(107, 54)",
           "CKA: 0.184<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.intermediate.dense<br>(107, 55)",
           "CKA: 0.179<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.intermediate.intermediate_act_fn<br>(107, 56)",
           "CKA: 0.179<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.intermediate<br>(107, 57)",
           "CKA: 0.148<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.output.dense<br>(107, 58)",
           "CKA: 0.148<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.output.dropout<br>(107, 59)",
           "CKA: 0.181<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.output.LayerNorm<br>(107, 60)",
           "CKA: 0.181<br>Base: classifier<br>Pretrained: roberta.encoder.layer.3.output<br>(107, 61)",
           "CKA: 0.18<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.self.query<br>(107, 62)",
           "CKA: 0.157<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.self.key<br>(107, 63)",
           "CKA: 0.158<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.self.value<br>(107, 64)",
           "CKA: 0.296<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.output.dense<br>(107, 65)",
           "CKA: 0.296<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.output.dropout<br>(107, 66)",
           "CKA: 0.241<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.output.LayerNorm<br>(107, 67)",
           "CKA: 0.241<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.attention.output<br>(107, 68)",
           "CKA: 0.254<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.intermediate.dense<br>(107, 69)",
           "CKA: 0.303<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.intermediate.intermediate_act_fn<br>(107, 70)",
           "CKA: 0.303<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.intermediate<br>(107, 71)",
           "CKA: 0.332<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.output.dense<br>(107, 72)",
           "CKA: 0.332<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.output.dropout<br>(107, 73)",
           "CKA: 0.286<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.output.LayerNorm<br>(107, 74)",
           "CKA: 0.286<br>Base: classifier<br>Pretrained: roberta.encoder.layer.4.output<br>(107, 75)",
           "CKA: 0.301<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.self.query<br>(107, 76)",
           "CKA: 0.257<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.self.key<br>(107, 77)",
           "CKA: 0.368<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.self.value<br>(107, 78)",
           "CKA: 0.377<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.output.dense<br>(107, 79)",
           "CKA: 0.377<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.output.dropout<br>(107, 80)",
           "CKA: 0.347<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.output.LayerNorm<br>(107, 81)",
           "CKA: 0.347<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.attention.output<br>(107, 82)",
           "CKA: 0.388<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.intermediate.dense<br>(107, 83)",
           "CKA: 0.425<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.intermediate.intermediate_act_fn<br>(107, 84)",
           "CKA: 0.425<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.intermediate<br>(107, 85)",
           "CKA: 0.495<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.output.dense<br>(107, 86)",
           "CKA: 0.495<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.output.dropout<br>(107, 87)",
           "CKA: 0.411<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.output.LayerNorm<br>(107, 88)",
           "CKA: 0.411<br>Base: classifier<br>Pretrained: roberta.encoder.layer.5.output<br>(107, 89)",
           "CKA: 0.518<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.self.query<br>(107, 90)",
           "CKA: 0.419<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.self.key<br>(107, 91)",
           "CKA: 0.532<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.self.value<br>(107, 92)",
           "CKA: 0.528<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.output.dense<br>(107, 93)",
           "CKA: 0.528<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.output.dropout<br>(107, 94)",
           "CKA: 0.499<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.output.LayerNorm<br>(107, 95)",
           "CKA: 0.499<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.attention.output<br>(107, 96)",
           "CKA: 0.503<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.intermediate.dense<br>(107, 97)",
           "CKA: 0.517<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.intermediate.intermediate_act_fn<br>(107, 98)",
           "CKA: 0.517<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.intermediate<br>(107, 99)",
           "CKA: 0.476<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.output.dense<br>(107, 100)",
           "CKA: 0.476<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.output.dropout<br>(107, 101)",
           "CKA: 0.533<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.output.LayerNorm<br>(107, 102)",
           "CKA: 0.533<br>Base: classifier<br>Pretrained: roberta.encoder.layer.6.output<br>(107, 103)",
           "CKA: 0.643<br>Base: classifier<br>Pretrained: classifier.dropout<br>(107, 104)",
           "CKA: 0.641<br>Base: classifier<br>Pretrained: classifier.dense<br>(107, 105)",
           "CKA: 0.634<br>Base: classifier<br>Pretrained: classifier.out_proj<br>(107, 106)",
           "CKA: 0.634<br>Base: classifier<br>Pretrained: classifier<br>(107, 107)"
          ]
         ],
         "hovertemplate": "%{customdata}<extra></extra>",
         "showscale": true,
         "type": "heatmap",
         "z": {
          "bdata": "aF9sP66LU6ibwn47halwP4WpcD+FqXA/FvgdP8AuPj91Il4/lNqePpTanj56g3I/eoNyP4sWaz+d9W4/nfVuP52uWz+drls/4CB1P+AgdT9zllU/tipCPx0qWj8O1MQ+DtTEPrCKbj+wim4/JgltP4JVcT+CVXE/TWdhP01nYT/gRW4/4EVuP8PvRT8m+Fg/bktbPxLwwj4S8MI+vv1gP779YD/4ZWE/81xoP/NcaD8Lv14/C79eP27hYj9u4WI/Mn03P0pJRj8b11Y/jrDKPo6wyj4i5E0/IuRNPx+WOz8kRkM/JEZDP1x2Pz9cdj8/GLJMPxiyTD8iggY/ktwkP9qIRD8z9JA+M/SQPjIPMT8yDzE///MRP0H8ET9B/BE/LFcMPyxXDD8rWiQ/K1okP3tR2j6lPwU/TvIBP8D0fj7A9H4+228CP9tvAj+Whcg+ysnAPsrJwD7nLZs+5y2bPrPa6j6z2uo+o/OHPsj/zD6+s5M+WuAaPlrgGj5616M+etejPqhJWj5dgmk+XYJpPuLyBj7i8gY+KolbPiqJWz72+Nw9MHDiPdbEnT3WxJ09IdILqgAAAAAM4cajM6MrrTOjK60zoyutNpaRq1Y6Yq1BLz+sO2HZqzth2aubyUGum8lBrgsbJK7/BUet/wVHrdQBJa3UASWt60BMrutATK6zk42tzmyxrNhBdq0GjlmtBo5ZrcDFeq7AxXquV/gxrtLJXa3SyV2tqo4nraqOJ609y5KuPcuSrjrm9K3qCxOuogrOrfX58az1+fGsi16jroteo64LEoCu4dqfreHan631Fy2t9RctrY2kra6NpK2ufA8brlXPc61zbOWtFFyBrRRcga2LXL2ui1y9rr5vk65eeLutXni7rf2PLq39jy6tCsHNrgrBza6ObVCubgoZrpP28q2rXQKtq10CrXMI2K5zCNiusOLWrhpDDa4aQw2upSNgraUjYK0CvsyuAr7Mrgprvq1878atcwXtrWcTja1nE42ttlvtrrZb7a7bE/KuUQI1rlECNa61NJCttTSQrf7Hya7+x8musBvQrcpNsa3VLRqudZzirXWc4q2YRJ+umESfrtkbIK8+f0CuPn9ArrslIq67JSKuNggErjYIBK5tfQ+qEA4qqnzerah83q2obuGzPQYmMybNzEw+3UJbPd1CWz3dQls923LzPe6HQz2R0TE9rsm6PK7JujzD7d08w+3dPLCegz0OnLw9Dpy8Pco6Dj7KOg4+G++jPBvvozwQAB09DkXDPLt+Mjy40yg8uNMoPCZAkjwmQJI8KzbOPCoolTwqKJU8qmIFPapiBT1Lmmo8S5pqPN3ItzxKf4k8cW8MPDXQeTw10Hk8ZtJePGbSXjwuXT08VXXsO1V17Dt6Cdw7egncO1Y4OzxWODs8KllmO2btgzxaA4M7BBUGPAQVBjyiqko8oqpKPDLXHzzjoNs746DbO+Tutzvk7rc7an04PGp9ODzAct87FVP/O1ZYWDyQkZM8kJGTPJpGQDyaRkA8vKM6PEX3BjxF9wY8ZqPqO2aj6jv4Uzs8+FM7PIl+hTydH+87TVE3PLABRDywAUQ8aS5KPGkuSjxTAZM8/vl8PP75fDy555A8ueeQPJvTgTyb04E8MLZXPOJFazxD5B482mqtO9pqrTtF5Gs8ReRrPMCzYDx620M8ettDPJK9sjuSvbI7zwwDPM8MAzzx3hc7EpcLO76/NDq+vzQ68+JsP1mkjquXQyk8tehwP7XocD+16HA/Q6keP4mPPj9lVV4/W/+ePlv/nj6Kl3I/ipdyPyp3az8Yq28/GKtvP5jlXD+Y5Vw/kkJ1P5JCdT+A71U/omlCP7JiWj+Jy8Q+icvEPg2ibj8Nom4/UDZtP9h/cT/Yf3E/e7lhP3u5YT8aXW4/Gl1uP3YRRj8DJVk/olVbP0L3wj5C98I+CgxhPwoMYT+le2E/5nloP+Z5aD/G3l4/xt5eP+vsYj/r7GI/Dnc3P9xoRj8aAFc/zKzKPsysyj4G8U0/BvFNPxygOz+HR0M/h0dDP2FkPz9hZD8/YrdMP2K3TD+XYQY/Y+UkP5OURD/eFZE+3hWRPmYYMT9mGDE/xQcSP+QDEj/kAxI/zlwMP85cDD8OYiQ/DmIkPyaU2j6PQgU/q/IBP8L3fj7C934+VHkCP1R5Aj/PuMg+JvPAPibzwD4Gdps+BnabPsMD6z7DA+s+tyKIPm0gzT7PzpM+XQIbPl0CGz5cAKQ+XACkPnukWj4yuGk+MrhpPhgKBz4YCgc+da9bPnWvWz76Et09Kn3iPWbYnT1m2J098+JsP1mkjquXQyk8tehwP7XocD+16HA/Q6keP4mPPj9lVV4/W/+ePlv/nj6Kl3I/ipdyPyp3az8Yq28/GKtvP5jlXD+Y5Vw/kkJ1P5JCdT+A71U/omlCP7JiWj+Jy8Q+icvEPg2ibj8Nom4/UDZtP9h/cT/Yf3E/e7lhP3u5YT8aXW4/Gl1uP3YRRj8DJVk/olVbP0L3wj5C98I+CgxhPwoMYT+le2E/5nloP+Z5aD/G3l4/xt5eP+vsYj/r7GI/Dnc3P9xoRj8aAFc/zKzKPsysyj4G8U0/BvFNPxygOz+HR0M/h0dDP2FkPz9hZD8/YrdMP2K3TD+XYQY/Y+UkP5OURD/eFZE+3hWRPmYYMT9mGDE/xQcSP+QDEj/kAxI/zlwMP85cDD8OYiQ/DmIkPyaU2j6PQgU/q/IBP8L3fj7C934+VHkCP1R5Aj/PuMg+JvPAPibzwD4Gdps+BnabPsMD6z7DA+s+tyKIPm0gzT7PzpM+XQIbPl0CGz5cAKQ+XACkPnukWj4yuGk+MrhpPhgKBz4YCgc+da9bPnWvWz76Et09Kn3iPWbYnT1m2J098+JsP1mkjquXQyk8tehwP7XocD+16HA/Q6keP4mPPj9lVV4/W/+ePlv/nj6Kl3I/ipdyPyp3az8Yq28/GKtvP5jlXD+Y5Vw/kkJ1P5JCdT+A71U/omlCP7JiWj+Jy8Q+icvEPg2ibj8Nom4/UDZtP9h/cT/Yf3E/e7lhP3u5YT8aXW4/Gl1uP3YRRj8DJVk/olVbP0L3wj5C98I+CgxhPwoMYT+le2E/5nloP+Z5aD/G3l4/xt5eP+vsYj/r7GI/Dnc3P9xoRj8aAFc/zKzKPsysyj4G8U0/BvFNPxygOz+HR0M/h0dDP2FkPz9hZD8/YrdMP2K3TD+XYQY/Y+UkP5OURD/eFZE+3hWRPmYYMT9mGDE/xQcSP+QDEj/kAxI/zlwMP85cDD8OYiQ/DmIkPyaU2j6PQgU/q/IBP8L3fj7C934+VHkCP1R5Aj/PuMg+JvPAPibzwD4Gdps+BnabPsMD6z7DA+s+tyKIPm0gzT7PzpM+XQIbPl0CGz5cAKQ+XACkPnukWj4yuGk+MrhpPhgKBz4YCgc+da9bPnWvWz76Et09Kn3iPWbYnT1m2J09SZ0iP1c9tao6kkE8WpclP1qXJT9alyU/7a3gProqDT97oBo/zhFOPs4RTj5FTSc/RU0nP/USJj8dqyk/HaspP+71Hj/u9R4/JOkrPyTpKz/1ySc/QyYGPyhPGj9Tr44+U6+OPte3KD/Xtyg//Q8nP54PKz+eDys/Kz8gPys/ID8h5yo/IecqP2P/CD9IWSA/WrgfP/vHkj77x5I+KmsiPyprIj+VISQ/rugqP67oKj+inyU/op8lP8pLJT/KSyU/dl8LP/CJJj/u0yI/O/GwPjvxsD6aaRg/mmkYP8MUDT+dFRI/nRUSPwUTBj8FEwY/ehEVP3oRFT+rjsU+/dfmPrA5Cz8tcUs+LXFLPnNmAD9zZgA/o6HbPjVW3D41Vtw+DkvSPg5L0j79neo+/Z3qPgXwqj5iB8k+f425PjdGNj43RjY+IQC5PiEAuT5CK5Q+qjGRPqoxkT4F2mE+BdphPhEKoj4RCqI+onU9Pvwtij53UkY+C2TSPQtk0j3FUlw+xVJcPoXFDT4avR0+Gr0dPoP5qT2D+ak95EoQPuRKED4IXpY91SqcPQaIVT0GiFU9ovoyP1JkTqpKLrA7yXo0P8l6ND/JejQ/zuoVPxbSBT82YCk/ENWoPhDVqD7UtzA/1LcwPxpuMD8+tyU/PrclPwZ2Cz8Gdgs/NRsmPzUbJj/xqw0/ncD1Ps53ET+2FJc+thSXPk1qJD9NaiQ/IucgPxy1Ij8ctSI/1l0SP9ZdEj9+mh8/fpofP/7RBj/OMg4/is0XP34PnT5+D50+nEgaP5xIGj+KNBc/AKsWPwCrFj8k3wo/JN8KPxbEFz8WxBc/igf6PsK8Cz+i5f8+lPaTPpT2kz4KYAo/CmAKP1jq+D7b5P4+2+T+PhYB9T4WAfU+IKgHPyCoBz/YINY+HW3UPkIbAD+KyHs+ish7Pr3z7z698+8+7J3BPg/BwT4PwcE+X3O/Pl9zvz6oPeE+qD3hPtralT7G3bc+1AGyPubLPj7myz4+1fG1PtXxtT5KsIw+khKGPpIShj5Fk0g+RZNIPq58oj6ufKI+yQU7Ptp7lz5BgkY+Bn/QPQZ/0D2SqWE+kqlhPiL3FD4GmCM+BpgjPgg2rD0INqw9k5gOPpOYDj6Wloo9d+uPPX2+Qz19vkM9fiBcPyrqCap7yek7VRdgP1UXYD9VF2A/Ai0RPwsvLj/zm08/Ph2XPj4dlz7dzmE/3c5hP8PcXT8KB2I/CgdiP9MAVD/TAFQ/ml1mP5pdZj9a1E0/a4Q0P7gTTz+CQb8+gkG/PmtkYD9rZGA/+7JgP92wYz/dsGM/GHFZPxhxWT8D52E/A+dhP1oSQT+KQE0/jZdVP63bwD6t28A+EMNVPxDDVT/DN1Y/eENbP3hDWz/961M//etTP56fWD+en1g/Mj8wP5oWQj8WYkw/6pjBPuqYwT6ZlkQ/mZZEP/Q5ND8a0Do/GtA6P1K9OT9SvTk/f4FEP3+BRD/OGAU/G/AhPwQoQz+aK4w+miuMPkCkKT9ApCk/TrMMP6p0DD+qdAw/tq4GP7auBj/KvRw/yr0cP8Kq0j5gg/8+Zo73Pk6odz5OqHc+e874PnvO+D61xsE+QlC7PkJQuz7UCpU+1AqVPt2H3j7dh94+tUmBPmozwD4ea4s+boMTPm6DEz63jJo+t4yaPqKmTj5WZl4+VmZePrBf/D2wX/w9jS5MPo0uTD4+dtI9fTfZPYoqmD2KKpg9oqHKPiB03Kgg3RA8rifOPq4nzj6uJ84+KkyuPv3+sT5yCsE+QzHgPkMx4D7Nw80+zcPNPt7gvj40Q7Q+NEO0Psb2mj7G9po+06zFPtOsxT5o27I+gm12PnzauT7GQrg+xkK4Ptac3D7WnNw+PvLNPhCUxj4QlMY+5aC1PuWgtT5d/tY+Xf7WPrgrzz7CjcM+ImXJPp0oBz+dKAc/2un9Ptrp/T7+7vE+AP3ePgD93j4DpLs+A6S7PjsU9D47FPQ+hbHnPmCC/z4ph8s+hQD3PoUA9z5GnAw/RpwMPwMrDj8DUgw/A1IMP80N9j7NDfY+cOMKP3DjCj+2exM/w8EGP+UU8j5YjbU+WI21PjepDT83qQ0/9JQFPw3EAj8NxAI/A2rqPgNq6j5c8gc/XPIHP8AO5j4Nces+MIzlPn81oD5/NaA+usf8PrrH/D7ob+I+tXzcPrV83D7yTKI+8kyiPgaG4D4GhuA+DcKePoj6xj7dko0+aBM5PmgTOT6Nzas+jc2rPopCgD7mhoY+5oaGPqJAEj6iQBI+EE5cPhBOXD5tMOc9jtXzPVpSnD1aUpw9oqHKPiB03Kgg3RA8rifOPq4nzj6uJ84+KkyuPv3+sT5yCsE+QzHgPkMx4D7Nw80+zcPNPt7gvj40Q7Q+NEO0Psb2mj7G9po+06zFPtOsxT5o27I+gm12PnzauT7GQrg+xkK4Ptac3D7WnNw+PvLNPhCUxj4QlMY+5aC1PuWgtT5d/tY+Xf7WPrgrzz7CjcM+ImXJPp0oBz+dKAc/2un9Ptrp/T7+7vE+AP3ePgD93j4DpLs+A6S7PjsU9D47FPQ+hbHnPmCC/z4ph8s+hQD3PoUA9z5GnAw/RpwMPwMrDj8DUgw/A1IMP80N9j7NDfY+cOMKP3DjCj+2exM/w8EGP+UU8j5YjbU+WI21PjepDT83qQ0/9JQFPw3EAj8NxAI/A2rqPgNq6j5c8gc/XPIHP8AO5j4Nces+MIzlPn81oD5/NaA+usf8PrrH/D7ob+I+tXzcPrV83D7yTKI+8kyiPgaG4D4GhuA+DcKePoj6xj7dko0+aBM5PmgTOT6Nzas+jc2rPopCgD7mhoY+5oaGPqJAEj6iQBI+EE5cPhBOXD5tMOc9jtXzPVpSnD1aUpw9s59sP4baTa0Kr3U8U5NwP1OTcD9Tk3A/DukeP3EyQD9q0F4/YlWfPmJVnz6Oe3I/jntyPzCVaz8iKnA/IipwPybqXT8m6l0/7lt1P+5bdT+aLlY/pwBDP/oeWz+yvsQ+sr7EPkC5bj9AuW4/trBtP14icj9eInI/g7diP4O3Yj9NnG4/TZxuP9IrRj/emFk/jRpbP3XGwD51xsA+2+BgP9vgYD+z1GE/2DdpP9g3aT/gJWA/4CVgPzDPYj8wz2I/Lrk3PyjgRD9YaVg/ChfMPgoXzD69J04/vSdOPylhPD8yC0Q/MgtEP6sCQD+rAkA/bfFMP23xTD9nPAU/AGolP4IiRT9Cr5I+Qq+SPqevMT+nrzE/fJ4TP1dCEz9XQhM/rmQNP65kDT/94CQ//eAkP4Z72z7KGQY/qO4BP3uafj57mn4+2vMCP9rzAj9bpMk+14XBPteFwT5GmZs+RpmbPgYy7D4GMuw+C+uIPmVbzj52hZU+gBwePoAcHj4DlKU+A5SlPtB8XT69cWw+vXFsPresCT63rAk+hl5ePoZeXj5oTeE9RmHmPZsYoT2bGKE9s59sP4baTa0Kr3U8U5NwP1OTcD9Tk3A/DukeP3EyQD9q0F4/YlWfPmJVnz6Oe3I/jntyPzCVaz8iKnA/IipwPybqXT8m6l0/7lt1P+5bdT+aLlY/pwBDP/oeWz+yvsQ+sr7EPkC5bj9AuW4/trBtP14icj9eInI/g7diP4O3Yj9NnG4/TZxuP9IrRj/emFk/jRpbP3XGwD51xsA+2+BgP9vgYD+z1GE/2DdpP9g3aT/gJWA/4CVgPzDPYj8wz2I/Lrk3PyjgRD9YaVg/ChfMPgoXzD69J04/vSdOPylhPD8yC0Q/MgtEP6sCQD+rAkA/bfFMP23xTD9nPAU/AGolP4IiRT9Cr5I+Qq+SPqevMT+nrzE/fJ4TP1dCEz9XQhM/rmQNP65kDT/94CQ//eAkP4Z72z7KGQY/qO4BP3uafj57mn4+2vMCP9rzAj9bpMk+14XBPteFwT5GmZs+RpmbPgYy7D4GMuw+C+uIPmVbzj52hZU+gBwePoAcHj4DlKU+A5SlPtB8XT69cWw+vXFsPresCT63rAk+hl5ePoZeXj5oTeE9RmHmPZsYoT2bGKE9duRjP1+xBa0KxS08FTJoPxUyaD8VMmg/fnEUP6d2PT/Wdlg/DkOUPg5DlD7+zGo//sxqP9LqZD9iGms/YhprP6MtWz+jLVs/tY9vP7WPbz+YQFE/SDhFP3YJVD8m8bw+JvG8PsNdaD/DXWg/it5pPxjwbj8Y8G4/845jP/OOYz/+NGo//jRqPz5SQj8iYFc/KA1VPwJltT4CZbU+4l5bP+JeWz/9zF4/7UZnP+1GZz+TwWI/k8FiPwooXz8KKF8/BRc1P8iZQT/wu1g/aAvKPmgLyj5De0o/Q3tKP0RrOj8qQEI/KkBCP1CkQD9QpEA/JoBKPyaASj/WBwE/CAolP1t3Rj+P2Y4+j9mOPrP4Lj+z+C4/2rQSP5xHEj+cRxI/AY8MPwGPDD+a9SE/mvUhP9P41D6lfQQ/Dh7+PmLLdT5iy3U+2rT/Ptq0/z56vcQ+3WO9Pt1jvT6oW5k+qFuZPoOG5j6DhuY+l/uDPrBJzD6ZL5I+PbQZPj20GT4imKE+IpihPqqKVT4Vv2U+Fb9lPo17Bj6NewY+wDxZPsA8WT4g8N09oMbhPUb3nT1G950941hkPyVtFKw/vCI8UqFoP1KhaD9SoWg/EmMWP9WeQD8dM1o/ESSVPhEklT7VMWs/1TFrPyXAZT9992s/ffdrPzXqWz816ls/xQNwP8UDcD9NE1I/SplGPzKXVT9K370+St+9PrviaD+74mg/BZpqP9rzbz/a828/FjpkPxY6ZD/jrmo/465qP/AuQj8+Ilg/3m1VP1s6tT5bOrU+VsVbP1bFWz+bo18/ro5oP66OaD8AO2Q/ADtkP2aRXz9mkV8/ozI2P1ZqQj+6PVo/TOfLPkznyz6J80o/ifNKPzEnOz/nE0M/5xNDP91CQT/dQkE/vuxKP77sSj8hZgE/PsElP9ZzRj/XOI8+1ziPPk1YLz9NWC8/u10TP+X0Ej/l9BI/tEYNP7RGDT/UVyI/1FciP41x1T5bKgU/6Gr+PuDUdj7g1HY+8ykAP/MpAD/Pp8U+ol6+PqJevj7q3Zk+6t2ZPub/5j7m/+Y+IEeEPgvazD7IO5I+cIsZPnCLGT7Gu6E+xruhPnpoVT46EGY+OhBmPnr8BT56/AU+G9FYPhvRWD4Qst09unzhPS6anT0ump0941hkPyVtFKw/vCI8UqFoP1KhaD9SoWg/EmMWP9WeQD8dM1o/ESSVPhEklT7VMWs/1TFrPyXAZT9992s/ffdrPzXqWz816ls/xQNwP8UDcD9NE1I/SplGPzKXVT9K370+St+9PrviaD+74mg/BZpqP9rzbz/a828/FjpkPxY6ZD/jrmo/465qP/AuQj8+Ilg/3m1VP1s6tT5bOrU+VsVbP1bFWz+bo18/ro5oP66OaD8AO2Q/ADtkP2aRXz9mkV8/ozI2P1ZqQj+6PVo/TOfLPkznyz6J80o/ifNKPzEnOz/nE0M/5xNDP91CQT/dQkE/vuxKP77sSj8hZgE/PsElP9ZzRj/XOI8+1ziPPk1YLz9NWC8/u10TP+X0Ej/l9BI/tEYNP7RGDT/UVyI/1FciP41x1T5bKgU/6Gr+PuDUdj7g1HY+8ykAP/MpAD/Pp8U+ol6+PqJevj7q3Zk+6t2ZPub/5j7m/+Y+IEeEPgvazD7IO5I+cIsZPnCLGT7Gu6E+xruhPnpoVT46EGY+OhBmPnr8BT56/AU+G9FYPhvRWD4Qst09unzhPS6anT0ump09diBMP1QnhquE16Y7mgxQP5oMUD+aDFA/ZmMIP8Z2PT9BQUo/B4WAPgeFgD4odlI/KHZSP7WJUT+y+lo/svpaPwLsUD8C7FA/jvJaP47yWj+9zUM/bgM/P65ISj9A0ag+QNGoPtpWUz/aVlM/vkxZP6A3YT+gN2E/q6VZP6ulWT+qe1c/qntXPz56Mj9YMUo/cztCP9Jenj7SXp4+k/1HP5P9Rz+6NlE/GjRdPxo0XT8+QV8/PkFfP4P/TT+D/00//UcrPzI2OD+bD1k/L3jKPi94yj4yeTs/Mnk7P2slMT8OUTg/DlE4P45GNj+ORjY/qhU8P6oVPD9WLek+gpIdP5kfPT82OoM+NjqDPhslIj8bJSI/DrINPxPIDD8TyAw/HoQFPx6EBT9kyRQ/ZMkUP1J/wz7Az/Y+qhHjPmCFWD5ghVg+YKDoPmCg6D7XpbY+DbuwPg27sD7gyo4+4MqOPhLU0D4S1NA+FgxuPkvywD6KE4M+qXoHPql6Bz7HW5E+x1uRPkX+OT4FIk0+BSJNPiO87T0jvO091jNCPtYzQj7giMc9kzjJPSJliT0iZYk9diBMP1QnhquE16Y7mgxQP5oMUD+aDFA/ZmMIP8Z2PT9BQUo/B4WAPgeFgD4odlI/KHZSP7WJUT+y+lo/svpaPwLsUD8C7FA/jvJaP47yWj+9zUM/bgM/P65ISj9A0ag+QNGoPtpWUz/aVlM/vkxZP6A3YT+gN2E/q6VZP6ulWT+qe1c/qntXPz56Mj9YMUo/cztCP9Jenj7SXp4+k/1HP5P9Rz+6NlE/GjRdPxo0XT8+QV8/PkFfP4P/TT+D/00//UcrPzI2OD+bD1k/L3jKPi94yj4yeTs/Mnk7P2slMT8OUTg/DlE4P45GNj+ORjY/qhU8P6oVPD9WLek+gpIdP5kfPT82OoM+NjqDPhslIj8bJSI/DrINPxPIDD8TyAw/HoQFPx6EBT9kyRQ/ZMkUP1J/wz7Az/Y+qhHjPmCFWD5ghVg+YKDoPmCg6D7XpbY+DbuwPg27sD7gyo4+4MqOPhLU0D4S1NA+FgxuPkvywD6KE4M+qXoHPql6Bz7HW5E+x1uRPkX+OT4FIk0+BSJNPiO87T0jvO091jNCPtYzQj7giMc9kzjJPSJliT0iZYk9m75oP8hHjK2hW4c8mKtsP5irbD+Yq2w/rpsbP6PJQD/Y51w//82bPv/Nmz6IvG4/iLxuP+o1aT8qEW8/KhFvP/MmXz/zJl8/sDRzP7A0cz89rFU/NktFP1JgWj/1ZsE+9WbBPrstbD+7LWw/uqpsP8WZcT/FmXE/xQNkP8UDZD+GG20/hhttP20NRj9CGVk/M49ZP9LCvD7Swrw+oM9eP6DPXj96aGE/iINpP4iDaT/qhWI/6oViPxqsYT8arGE/v7Q3P3bmRD/+/lo/FafMPhWnzD4SCU0/EglNP7x7PD8CA0Q/AgNEP6GRQD+hkUA/eFZMP3hWTD+eygM/kiYmP7IiRz9qtpA+araQPl7lMD9e5TA/D04UP2yqEz9sqhM/GJINPxiSDT8OtCM/DrQjP9s32j5EtQU/sTAAP/NYeD7zWHg+sZEBP7GRAT9S78c+chXAPnIVwD4eFJs+HhSbPgtm6T4LZuk+xdCGPnbszT637ZM+2gscPtoLHD40qKM+NKijPpO4WT5KFGk+ShRpPvx+CD78fgg+s6FbPrOhWz5yM989m4LjPUrWnj1K1p49m75oP8hHjK2hW4c8mKtsP5irbD+Yq2w/rpsbP6PJQD/Y51w//82bPv/Nmz6IvG4/iLxuP+o1aT8qEW8/KhFvP/MmXz/zJl8/sDRzP7A0cz89rFU/NktFP1JgWj/1ZsE+9WbBPrstbD+7LWw/uqpsP8WZcT/FmXE/xQNkP8UDZD+GG20/hhttP20NRj9CGVk/M49ZP9LCvD7Swrw+oM9eP6DPXj96aGE/iINpP4iDaT/qhWI/6oViPxqsYT8arGE/v7Q3P3bmRD/+/lo/FafMPhWnzD4SCU0/EglNP7x7PD8CA0Q/AgNEP6GRQD+hkUA/eFZMP3hWTD+eygM/kiYmP7IiRz9qtpA+araQPl7lMD9e5TA/D04UP2yqEz9sqhM/GJINPxiSDT8OtCM/DrQjP9s32j5EtQU/sTAAP/NYeD7zWHg+sZEBP7GRAT9S78c+chXAPnIVwD4eFJs+HhSbPgtm6T4LZuk+xdCGPnbszT637ZM+2gscPtoLHD40qKM+NKijPpO4WT5KFGk+ShRpPvx+CD78fgg+s6FbPrOhWz5yM989m4LjPUrWnj1K1p49yoBLPwImpqybets81WlOP9VpTj/VaU4/pEcVP8AFQj9apUk/zLaZPsy2mT7d9k8/3fZPP1q8Tj/+JFY//iRWP21bTj9tW04/eOJWP3jiVj9yBEc/3m4yPxouVD+Ol7U+jpe1PlspUj9bKVI/OoxSPyMwWD8jMFg/TiBKP04gSj+aj1M/mo9TPzjpLT9GKEc/Pg9FP5zqvD6c6rw+OlhJPzpYST9Itk8/+nhYP/p4WD8FxlI/BcZSP8BCTD/AQkw/AwouP2quQD9ApVs/8nLdPvJy3T776T4/++k+Pz6oNT/ZYzs/2WM7PzfuLj837i4/ecI7P3nCOz81Zfs+/CwhP1fKMz8/vIc+P7yHPpqdJT+anSU/H+ISPw64ED8OuBA/Gm0EPxptBD81mxc/NZsXP9Nn2z6Ow/4+bRHqPi5PbT4uT20+DhD0Pg4Q9D6IfsU+gsa+PoLGvj7i/ZU+4v2VPrsf2j67H9o+54iEPqb5wT7ZRow+Qp8aPkKfGj4WQZs+FkGbPqAaUj5YF2I+WBdiPgHMAD4BzAA+q+RKPqvkSj4SJ9w9Rl/fPX0ynj19Mp494jxEP2qX9Kzwum876uNHP+rjRz/q40c/A5QHPz22ST94H0w/SzqDPks6gz6C4kk/guJJP3jrSj9SIVc/UiFXP6KNTz+ijU8/OPJUPzjyVD/t9EI/QuNCP7slUz+gYKo+oGCqPkYdTT9GHU0/sB5TP/vcXD/73Fw/RiZPP0YmTz8mqE8/JqhPP03MKD+anUQ/Az8+P4O9mD6DvZg+khlBP5IZQT8Ycks/uPNZP7jzWT9z1Vk/c9VZP3usRT97rEU/TmUoP4VGNj8Fg1Q/C3zFPgt8xT6O4TQ/juE0P5IoLD9NPDM/TTwzP90WMT/dFjE/anA1P2pwNT9LZ98+mwQcPz8cNT/+YIA+/mCAPgcbHT8HGx0/ivEKP7PICT+zyAk/hXYDP4V2Az+V9BA/lfQQP5N0wD7SAPc+SD3dPnvDVz57w1c+YF7jPmBe4z6WNrU+Sr2uPkq9rj5qJYw+aiWMPpqfyz6an8s+9uNoPnDRuD7+IHo+YLj2PWC49j1CCos+QgqLPsRTLj7vlkM+75ZDPltRzz1bUc89vaIxPr2iMT4ZE7U9g6y2PU0ocz1NKHM9VpJKPwbONKxmERQ8PT9OPz0/Tj89P04/wpcGP4YZMj9UZEg/+iZ2Pvomdj72VlA/9lZQPx1eUj+SfF0/knxdP2KEVz9ihFc/rlJbP65SWz8QL0U/ADs/P804Uj/SyKY+0simPqPlUj+j5VI/kKNYP/UYYD/1GGA/YFJWP2BSVj9+TVY/fk1WP5dTOD/Xr0k/DTFHP3I3mz5yN5s+dRZGP3UWRj+qOU4/kwtaP5MLWj97+Fg/e/hYPz2LSz89i0s/up4qP7PFND8F31k/CuTBPgrkwT7Ovzc/zr83P8o2Lj/QDDU/0Aw1P76VND++lTQ/KeI4PyniOD8ou+Q+GsUZP9xIPz/d3oQ+3d6EPrIuHz+yLh8/bfAMPx4tDD8eLQw/7hIFP+4SBT+d2BE/ndgRPw51wT5NUe4+qj/fPrhuWj64blo+itDjPorQ4z4A/7Y+FYexPhWHsT6eQo8+nkKPPjXryz4168s+NkNwPtqJuj7MS4I+aNELPmjRCz5G4o4+RuKOPvK9Oz6eB04+ngdOPi6k+T0upPk9eJFDPniRQz7eUtA9BefTPdJjkD3SY5A9wlC7PhQ2DasFbSs8ACrBPgAqwT4AKsE+wwuVPvcmxT7kHsI+zkqCPs5Kgj7tM8Y+7TPGPm6Vvj6Td8Q+k3fEPjZWtD42VrQ+60jIPutIyD7yvq0+s9mhPpUPzz4j8uE+I/LhPsa/3D7Gv9w+DarZPupj2j7qY9o+SpbPPkqWzz4waN0+MGjdPgKHvj7Qxss+ixTHPtCRuj7Qkbo+qlfsPqpX7D6iv+4+IwnuPiMJ7j5z4ds+c+HbPvud6z77nes+EODgPiatzj7uNOs+w8LNPsPCzT6rKvc+qyr3PoNQ/T7C9AA/wvQAPwqO6j4Kjuo++6v4Pvur+D5Qs80+07zbPqa18D5bD6c+Ww+nPvJ5+T7yefk+lWMAP00cAz9NHAM//nzuPv587j4anvE+Gp7xPmR1uT4VHtU+UJHfPtYvyD7WL8g+4x3oPuMd6D7qV+I+IMvnPiDL5z4A6sA+AOrAPrbh1T624dU+brupPgDS1j72XKk+CjmFPgo5hT52xbQ+dsW0Pv+lkT5spZ0+bKWdPvZNcj72TXI+8uuYPvLrmD6fYDo+psxLPuZ2Cz7mdgs+wlC7PhQ2DasFbSs8ACrBPgAqwT4AKsE+wwuVPvcmxT7kHsI+zkqCPs5Kgj7tM8Y+7TPGPm6Vvj6Td8Q+k3fEPjZWtD42VrQ+60jIPutIyD7yvq0+s9mhPpUPzz4j8uE+I/LhPsa/3D7Gv9w+DarZPupj2j7qY9o+SpbPPkqWzz4waN0+MGjdPgKHvj7Qxss+ixTHPtCRuj7Qkbo+qlfsPqpX7D6iv+4+IwnuPiMJ7j5z4ds+c+HbPvud6z77nes+EODgPiatzj7uNOs+w8LNPsPCzT6rKvc+qyr3PoNQ/T7C9AA/wvQAPwqO6j4Kjuo++6v4Pvur+D5Qs80+07zbPqa18D5bD6c+Ww+nPvJ5+T7yefk+lWMAP00cAz9NHAM//nzuPv587j4anvE+Gp7xPmR1uT4VHtU+UJHfPtYvyD7WL8g+4x3oPuMd6D7qV+I+IMvnPiDL5z4A6sA+AOrAPrbh1T624dU+brupPgDS1j72XKk+CjmFPgo5hT52xbQ+dsW0Pv+lkT5spZ0+bKWdPvZNcj72TXI+8uuYPvLrmD6fYDo+psxLPuZ2Cz7mdgs+AFhnP5Yqlq2aQIQ8c1lrP3NZaz9zWWs/QlgaP73yPj/Fils/pqmhPqapoT7yrG0/8qxtP/vQZz9+X20/fl9tPwpIXT8KSF0/lcZxP5XGcT+dR1Q/8tVEPz0wWD+igdI+ooHSPlYEbT9WBG0/jS1tP2BbcT9gW3E/PRtkPz0bZD/Fym0/xcptP7J+Rz9j3lk/SzBaP9XZyj7V2co+dpthP3abYT/KymM/rahqP62oaj+4cmI/uHJiP5PzYz+T82M/aGQ7PyK9Rj9ARls/hT7UPoU+1D6tflA/rX5QP+vbQD+z/Ec/s/xHP1MhQz9TIUM/uK9PP7ivTz/MdQk/uIMoP+YwSj/QHps+0B6bPmWqNT9lqjU/+isbP1WWGj9Vlho/3mATP95gEz8Qcig/EHIoPy0R4z5SmQk/utQFP/fPhz73z4c+ZhoHP2YaBz87DdY+u8XOPrvFzj7yl6Y+8pemPoMn8z6DJ/M+XIWRPu7n1j6lD54+RckxPkXJMT7xhq0+8YatPkCObT5YQX0+WEF9Pmt3Gj5rdxo+EP9vPhD/bz4KbwA+A3sEPtyTvD3ck7w9AFhnP5Yqlq2aQIQ8c1lrP3NZaz9zWWs/QlgaP73yPj/Fils/pqmhPqapoT7yrG0/8qxtP/vQZz9+X20/fl9tPwpIXT8KSF0/lcZxP5XGcT+dR1Q/8tVEPz0wWD+igdI+ooHSPlYEbT9WBG0/jS1tP2BbcT9gW3E/PRtkPz0bZD/Fym0/xcptP7J+Rz9j3lk/SzBaP9XZyj7V2co+dpthP3abYT/KymM/rahqP62oaj+4cmI/uHJiP5PzYz+T82M/aGQ7PyK9Rj9ARls/hT7UPoU+1D6tflA/rX5QP+vbQD+z/Ec/s/xHP1MhQz9TIUM/uK9PP7ivTz/MdQk/uIMoP+YwSj/QHps+0B6bPmWqNT9lqjU/+isbP1WWGj9Vlho/3mATP95gEz8Qcig/EHIoPy0R4z5SmQk/utQFP/fPhz73z4c+ZhoHP2YaBz87DdY+u8XOPrvFzj7yl6Y+8pemPoMn8z6DJ/M+XIWRPu7n1j6lD54+RckxPkXJMT7xhq0+8YatPkCObT5YQX0+WEF9Pmt3Gj5rdxo+EP9vPhD/bz4KbwA+A3sEPtyTvD3ck7w9CHtZP1rAWK1mIxg8U5NdP1OTXT9Tk10/KPQTPwIDPz/2IVM/TgySPk4Mkj4KH2A/Ch9gPxjPXD8wD2Q/MA9kP60dVz+tHVc/gBVmP4AVZj/a/0k/Sr1CP+hiUj8Sk8c+EpPHPh4jYT8eI2E/TqNjP9KiaT/Somk/3SReP90kXj9D+GI/Q/hiP3jYPD+QKVM/7QJNPzR8vj40fL4+02tWP9NrVj/NEFw/KnxlPyp8ZT9Ka2A/SmtgP6O/WT+jv1k/BYw0P8E+Pj8VJ10/HWrSPh1q0j6yG0c/shtHP7inOj86qkE/OqpBP1OBPD9TgTw/12hGP9doRj9uTgA/LjkkPzGxQj+eepE+nnqRPqBHLT+gRy0/MroXP+IDFz/iAxc/KswOPyrMDj8V6R8/FekfP8CZ1T6HjwM/oNP6Pq20fz6ttH8+w57/PsOe/z4tBs4+mMPHPpjDxz7utKE+7rShPpMQ5j6TEOY+uk+JPqMj0D54F5Q+lhwlPpYcJT6qIqM+qiKjPgtVWj6bvGs+m7xrPuRaDj7kWg4+/pFhPv6RYT6dKPU9UA/8PbLAsj2ywLI9jhRZP4JXhax2qf87zg5dP84OXT/ODl0/1rUYPwuURj824FU/uIuPPriLjz4KXl8/Cl5fP9AbXT/jumQ/47pkP9riVz/a4lc/LpBlPy6QZT+Pbko/HnxFP8qrVD8acMM+GnDDPmB7YD9ge2A/uk1jP/5Naj/+TWo/DX1dPw19XT8uOmI/LjpiP/a9OT/KW1Q/hStLP5uQuD6bkLg+UA1VP1ANVT++tVs/iNVmP4jVZj9mPmI/Zj5iPzNpWD8zaVg/wNwzP4OAPj8QbmA/WsrTPlrK0z6P10U/j9dFP1bgOT/ONUE/zjVBP+OoOz/jqDs/AvdEPwL3RD/9bvw+c80iP2KlPz8n/44+J/+OPrXnKz+15ys/b/sWPxY3Fj8WNxY/gg4OP4IODj9atR4/WrUeP8pw0z6gQgM/7fn3PmCGez5ghns+qiz9Pqos/T6Tg8w+UTbGPlE2xj52sp8+drKfPlP/4z5T/+M+SlyHPjYPzz5FEJE+WcgePlnIHj7akaA+2pGgPvq9VD4a6WY+GulmPl65CD5euQg+Im9bPiJvWz4Dsuw9/UryPa2+qj2tvqo9jhRZP4JXhax2qf87zg5dP84OXT/ODl0/1rUYPwuURj824FU/uIuPPriLjz4KXl8/Cl5fP9AbXT/jumQ/47pkP9riVz/a4lc/LpBlPy6QZT+Pbko/HnxFP8qrVD8acMM+GnDDPmB7YD9ge2A/uk1jP/5Naj/+TWo/DX1dPw19XT8uOmI/LjpiP/a9OT/KW1Q/hStLP5uQuD6bkLg+UA1VP1ANVT++tVs/iNVmP4jVZj9mPmI/Zj5iPzNpWD8zaVg/wNwzP4OAPj8QbmA/WsrTPlrK0z6P10U/j9dFP1bgOT/ONUE/zjVBP+OoOz/jqDs/AvdEPwL3RD/9bvw+c80iP2KlPz8n/44+J/+OPrXnKz+15ys/b/sWPxY3Fj8WNxY/gg4OP4IODj9atR4/WrUeP8pw0z6gQgM/7fn3PmCGez5ghns+qiz9Pqos/T6Tg8w+UTbGPlE2xj52sp8+drKfPlP/4z5T/+M+SlyHPjYPzz5FEJE+WcgePlnIHj7akaA+2pGgPvq9VD4a6WY+GulmPl65CD5euQg+Im9bPiJvWz4Dsuw9/UryPa2+qj2tvqo9hjM1P5KvO6xFiUU7hrU4P4a1OD+GtTg/DDMGPzW3OT+phjo//S5ePv0uXj5uzDo/bsw6PxVTPD/8eUY//HlGP7rhPz+64T8/W41DP1uNQz/TMS4/GdgwP94jQT9v6KY+b+imPnhhPj94YT4/YCZEP/unTT/7p00/GeNCPxnjQj+d3EA/ndxAP/C3HD9WIDg/owMqP5hGmz6YRps+RWs0P0VrND8zfT8/xq5NP8auTT/CKE0/wihNPwZkOD8GZDg/iUsaP96VIz/IbVM/tgbAPrYGwD7CaCg/wmgoP5eoIT+b0ic/m9InP762ID++tiA/7jYnP+42Jz+DjM0+uuQPP46OJT8dIW4+HSFuPoiLET+IixE/A3IFP2OuBD9jrgQ/def0PnXn9D6tEAU/rRAFP28yrj4Vo90+D2PKPgJXUD4CV1A+c0LTPnNC0z5p2LA+smKsPrJirD7Byoo+wcqKPri4vj64uL4+W59lPsr/tT6WYG8+9qMBPvajAT6QKIU+kCiFPjKAKj5egD4+XoA+PmbP3T1mz909qPk1Pqj5NT6Ir8g9GhnMPWrIjD1qyIw9hjM1P5KvO6xFiUU7hrU4P4a1OD+GtTg/DDMGPzW3OT+phjo//S5ePv0uXj5uzDo/bsw6PxVTPD/8eUY//HlGP7rhPz+64T8/W41DP1uNQz/TMS4/GdgwP94jQT9v6KY+b+imPnhhPj94YT4/YCZEP/unTT/7p00/GeNCPxnjQj+d3EA/ndxAP/C3HD9WIDg/owMqP5hGmz6YRps+RWs0P0VrND8zfT8/xq5NP8auTT/CKE0/wihNPwZkOD8GZDg/iUsaP96VIz/IbVM/tgbAPrYGwD7CaCg/wmgoP5eoIT+b0ic/m9InP762ID++tiA/7jYnP+42Jz+DjM0+uuQPP46OJT8dIW4+HSFuPoiLET+IixE/A3IFP2OuBD9jrgQ/def0PnXn9D6tEAU/rRAFP28yrj4Vo90+D2PKPgJXUD4CV1A+c0LTPnNC0z5p2LA+smKsPrJirD7Byoo+wcqKPri4vj64uL4+W59lPsr/tT6WYG8+9qMBPvajAT6QKIU+kCiFPjKAKj5egD4+XoA+PmbP3T1mz909qPk1Pqj5NT6Ir8g9GhnMPWrIjD1qyIw9cMpeP1Txuq145mY8hfZiP4X2Yj+F9mI/T5oSPwMCOj/9rVQ/yqiaPsqomj6Ys2U/mLNlP7LHYD9Tg2c/U4NnP/DxWT/w8Vk/iilrP4opaz/NZU8/Ji1CP14pUz/z2c0+89nNPuIcZj/iHGY/HpFnP+pXbD/qV2w/ShthP0obYT/TtGc/07RnP43PQz9YE1U/aLxTP0+uxT5PrsU+W2BbP1tgWz8+EV8/nptmP56bZj+6HWA/uh1gP65ZXj+uWV4/+rE2P966QT/+CVo/wzPQPsMz0D5qEUs/ahFLP9ufPD9raUM/a2lDP0qqPj9Kqj4/eHBKP3hwSj/apAQ/bbQlP3rSRz/xkpU+8ZKVPr6uMD++rjA/flwYPwPAFz8DwBc/vkQQP75EED9gOSM/YDkjP9Vp2z4umAU/Es4AP18Ygj5fGII+dooCP3aKAj9Iq88+vhHJPr4RyT5aIKM+WiCjPlvm6j5b5uo+PUqMPuA40j5D35g+wuIrPsLiKz7ue6c+7nunPl0xYz5WYnM+VmJzPkaGFD5GhhQ+YvNnPmLzZz4ul/o9NRABPirztz0q87c9cMpeP1Txuq145mY8hfZiP4X2Yj+F9mI/T5oSPwMCOj/9rVQ/yqiaPsqomj6Ys2U/mLNlP7LHYD9Tg2c/U4NnP/DxWT/w8Vk/iilrP4opaz/NZU8/Ji1CP14pUz/z2c0+89nNPuIcZj/iHGY/HpFnP+pXbD/qV2w/ShthP0obYT/TtGc/07RnP43PQz9YE1U/aLxTP0+uxT5PrsU+W2BbP1tgWz8+EV8/nptmP56bZj+6HWA/uh1gP65ZXj+uWV4/+rE2P966QT/+CVo/wzPQPsMz0D5qEUs/ahFLP9ufPD9raUM/a2lDP0qqPj9Kqj4/eHBKP3hwSj/apAQ/bbQlP3rSRz/xkpU+8ZKVPr6uMD++rjA/flwYPwPAFz8DwBc/vkQQP75EED9gOSM/YDkjP9Vp2z4umAU/Es4AP18Ygj5fGII+dooCP3aKAj9Iq88+vhHJPr4RyT5aIKM+WiCjPlvm6j5b5uo+PUqMPuA40j5D35g+wuIrPsLiKz7ue6c+7nunPl0xYz5WYnM+VmJzPkaGFD5GhhQ+YvNnPmLzZz4ul/o9NRABPirztz0q87c96LkzP4P6C627Ws483jg3P944Nz/eODc/+ubvPtYNJz9UGDI/CU+HPglPhz7OZTo/zmU6P9uBOD/hakQ/4WpEPz5QQT8+UEE/wxZDP8MWQz/LfjI/WZIkP3LqPD86YrQ+OmK0Pr+ePT+/nj0/gspAPxtESD8bREg/vf8+P73/Pj/eXz8/3l8/P44/JT9XxDM/JWsrPyOMqz4jjKs+Ak41PwJONT/Mgz0/tu9HP7bvRz82WUU/NllFP8p9OD/KfTg/kRsZP83TIj+VuEs/+yPCPvsjwj4lDis/JQ4rP7i4Iz9b3Cg/W9woP7wWIT+8FiE/WfopP1n6KT/9xtg+d1EWP+KfLD8B14E+AdeBPgC3FT8AtxU/y9YIP0/uBz9P7gc/lrz3Ppa89z6h/Qg/of0IP7Diuj6yqOE+xcbQPhY8Wz4WPFs+7ZbdPu2W3T7Vtrg+3lOzPt5Tsz7XzY4+182OPjImyD4yJsg+Ov10PspuvD6LwX8+TtgMPk7YDD4lTY4+JU2OPrZOPT77sU0++7FNPgbz7D0G8+w97nE/Pu5xPz76ANE9jSvUPbLikj2y4pI9nxRCP+I196wEnKs7HylGPx8pRj8fKUY/CFUDPyq/Nz8N70Q/dDaEPnQ2hD6Gr0g/hq9IP0pEST/YSFQ/2EhUP5sDTj+bA04/gsVSP4LFUj/3T0I/RHRCP74iRD+dJLI+nSSyPsYkTD/GJEw/YMFRP1bIWT9WyFk/eFVQP3hVUD+QdE8/kHRPP5VELT9tZUc/On07PyMSoj4jEqI+W1NBP1tTQT9SsEk/0nJWP9JyVj/NblQ/zW5UP052RT9OdkU/Sl4kP5WPMj8zx1E/sHXCPrB1wj6ryzM/q8szP4aWKT+2TTA/tk0wP/YzLj/2My4/mzI0P5syND8rquE+7iYXP3N1Nj8X1YM+F9WDPqphHD+qYRw/jW0LPxiICj8YiAo/zcYDP83GAz8atQ8/GrUPP4PtwD52NPI+syLePjrrYT4662E+QI3jPkCN4z5Cb7g+zCyzPswssz7I1ZE+yNWRPrY+zT62Ps0+MBVzPsWmvj7W14A+7gMIPu4DCD4ygo4+MoKOPlPKNj6wrEs+sKxLPpij5z2Yo+c9tVI/PrVSPz7Grs090PfRPW45kD1uOZA90Ls7P+xDzKww4Xc7XHM/P1xzPz9ccz8/OvQDPwSiMT/dlD4/Ul5xPlJecT7tuEE/7bhBP4GERD/GfU8/xn1PPxI8Sj8SPEo/0YtLP9GLSz/QEDg/ON09P6MVPz/qsbA+6rGwPngdRj94HUY/+99MP9aJVT/WiVU/itNNP4rTTT/oC0o/6AtKP6dqKz/CgUM/RsM0P8pyoj7KcqI+nT88P50/PD+otEY/+69TP/uvUz9w8lI/cPJSP/knQT/5J0E/mBcjPwiQLD92/lM/NjTHPjY0xz4eETA/HhEwPzi1KD9+1i4/ftYuP5RSLD+UUiw/HqMwPx6jMD+jcuE+UmcSP2W7ND+NR4Q+jUeEPv7EGT/+xBk/LLsMPyb2Cz8m9gs/+ogEP/qIBD+u7ww/ru8MP6zQuj5oiek+Y9jYPnhgXj54YF4+nQ/fPp0P3z6fN7k+twS1PrcEtT557JI+eeySPup8yT7qfMk+g+NyPjJgvz7GdoE+iZoPPomaDz7d5Y0+3eWNPpy3OD59J00+fSdNPmqw8j1qsPI9CJlCPgiZQj47i9k95djePRJEmj0SRJo9qn+NPn5UhqsKWD87R9eSPkfXkj5H15I+3sBcPj2OlT4VmJI+8kdVPvJHVT42xJY+NsSWPsozkT5ACJU+QAiVPhQriD4UK4g+ZZaYPmWWmD6Z0Yc+FiR8Pnhpmz7oUu8+6FLvPrKhsj6yobI+84OwPowzrj6MM64+lDKkPpQypD4Td7E+E3exPm/tnz5Fj6A+vaKePj2Kqj49iqo+pa/BPqWvwT6jrME+upK/PrqSvz6g4qw+oOKsPlJywT5ScsE+krKzPpL5rD5VebY+a/2jPmv9oz4+ssk+PrLJPiUrzD7wQ9E+8EPRPqOruT6jq7k+RUbKPkVGyj7MtaQ+dyezPkrzzD6yDJ4+sgyePh4G0D4eBtA+1rvZPtMs5z7TLOc+nRrhPp0a4T4IqtE+CKrRPopnqT6oVM4+CFTOPqSgpD6koKQ+EojHPhKIxz6uJM8+GubbPhrm2z5hIsw+YSLMPl06vj5dOr4+gqilPgOj1D5aRJw+zjaLPs42iz4eIqw+HiKsPiC6kz5KZ6A+SmegPtv2hz7b9oc+h5uiPoeboj46AzM+5dlGPsFaAD7BWgA+qn+NPn5UhqsKWD87R9eSPkfXkj5H15I+3sBcPj2OlT4VmJI+8kdVPvJHVT42xJY+NsSWPsozkT5ACJU+QAiVPhQriD4UK4g+ZZaYPmWWmD6Z0Yc+FiR8Pnhpmz7oUu8+6FLvPrKhsj6yobI+84OwPowzrj6MM64+lDKkPpQypD4Td7E+E3exPm/tnz5Fj6A+vaKePj2Kqj49iqo+pa/BPqWvwT6jrME+upK/PrqSvz6g4qw+oOKsPlJywT5ScsE+krKzPpL5rD5VebY+a/2jPmv9oz4+ssk+PrLJPiUrzD7wQ9E+8EPRPqOruT6jq7k+RUbKPkVGyj7MtaQ+dyezPkrzzD6yDJ4+sgyePh4G0D4eBtA+1rvZPtMs5z7TLOc+nRrhPp0a4T4IqtE+CKrRPopnqT6oVM4+CFTOPqSgpD6koKQ+EojHPhKIxz6uJM8+GubbPhrm2z5hIsw+YSLMPl06vj5dOr4+gqilPgOj1D5aRJw+zjaLPs42iz4eIqw+HiKsPiC6kz5KZ6A+SmegPtv2hz7b9oc+h5uiPoeboj46AzM+5dlGPsFaAD7BWgA+HqNXP7IF0K2LQGA8w9dbP8PXWz/D11s/WF8OP+mEMz++Ok4/JNOiPiTToj5dr14/Xa9ePxhrWT+47V4/uO1eP7PgUD+z4FA/jRZjP40WYz+iVEg/GrI6P9EwSz9+QvU+fkL1PjDOYj8wzmI/dY1jP5WtZj+VrWY/0ppbP9KaWz9T+WM/U/ljP8DJQj+C5VA/Tp9QPwBi3D4AYtw+cEBcP3BAXD9au14/NidkPzYnZD/19Fs/9fRbP5iSXj+Ykl4/Sz04P17ZQT9CbVQ/YOrWPmDq1j6WlE0/lpRNPzN/Pz9DEUY/QxFGP6qEPz+qhD8/S0dNP0tHTT8acQs/zqIoP6Z7Sz+Noa0+jaGtPp4QOD+eEDg/JoYhP1dWIz9XViM/W/AbP1vwGz+4/Sw/uP0sPzt67j4SOxA/xwAPP7hKoj64SqI+6GAPP+hgDz/iW+8+7nnsPu557D7pNMU+6TTFPjK1Aj8ytQI/DwuqPt358T4tGLM+1V9lPtVfZT4lFcQ+JRXEPmNbjz69eJg+vXiYPttPUD7bT1A+jNCTPozQkz56AiQ+myktPktE+D1LRPg9HqNXP7IF0K2LQGA8w9dbP8PXWz/D11s/WF8OP+mEMz++Ok4/JNOiPiTToj5dr14/Xa9ePxhrWT+47V4/uO1eP7PgUD+z4FA/jRZjP40WYz+iVEg/GrI6P9EwSz9+QvU+fkL1PjDOYj8wzmI/dY1jP5WtZj+VrWY/0ppbP9KaWz9T+WM/U/ljP8DJQj+C5VA/Tp9QPwBi3D4AYtw+cEBcP3BAXD9au14/NidkPzYnZD/19Fs/9fRbP5iSXj+Ykl4/Sz04P17ZQT9CbVQ/YOrWPmDq1j6WlE0/lpRNPzN/Pz9DEUY/QxFGP6qEPz+qhD8/S0dNP0tHTT8acQs/zqIoP6Z7Sz+Noa0+jaGtPp4QOD+eEDg/JoYhP1dWIz9XViM/W/AbP1vwGz+4/Sw/uP0sPzt67j4SOxA/xwAPP7hKoj64SqI+6GAPP+hgDz/iW+8+7nnsPu557D7pNMU+6TTFPjK1Aj8ytQI/DwuqPt358T4tGLM+1V9lPtVfZT4lFcQ+JRXEPmNbjz69eJg+vXiYPttPUD7bT1A+jNCTPozQkz56AiQ+myktPktE+D1LRPg9DcdGP+hepK3OXRo8GTNLPxkzSz8ZM0s/6jMCP/77KD+O4T0/hUOTPoVDkz72fU4/9n1OP05NSj9ToE4/U6BOPyGmQD8hpkA/eJZSP3iWUj+omzg/VkwsP+t+PT/mGAA/5hgAP1r6VD9a+lQ/YItVP1IMWD9SDFg/aI5OP2iOTj/GM1Y/xjNWP7l3NT/nXEM/b0FFP7772T6++9k+BvZQPwb2UD+19VI/sLRXP7C0Vz/6tU8/+rVPP8YFUz/GBVM/M/MwP107OD9q+UY/wu3EPsLtxD6li0E/pYtBP/7BMj91CDk/dQg5PwUZMT8FGTE/vblAP725QD/qOQQ/AqceP88xPD/ewqM+3sKjPh7/LD8e/yw/fJ0WP2qlGj9qpRo/V3YVP1d2FT9SDCQ/UgwkPzs14z42tws/5vMLP4o7qT6KO6k+4+cJP+PnCT9+d+s+JSztPiUs7T4Bwsk+AcLJPqtB+z6rQfs+asilPlAt5z4ISa8+1u1sPtbtbD6NN74+jTe+PrOYjT5Nj5c+TY+XPsgoUT7IKFE+xS2SPsUtkj6jxio+kN02PvAdCD7wHQg+eLdIP8cttKxuYBU8hkJNP4ZCTT+GQk0/Uv4CP147KT96Oz8/5b+TPuW/kz52tlA/drZQP+PWSz/G0U8/xtFPP0lgQT9JYEE/rW1UP61tVD/SBTo/JmotP9r4PT91K/8+dSv/PnqEVj96hFY/vtlWP6oxWT+qMVk/k4JPP5OCTz9+k1c/fpNXP2opNj8QY0Q/7otGPzI12T4yNdk+IwpSPyMKUj8OrlM/82hYP/NoWD8IXlA/CF5QPyX8Uz8l/FM/Oc8wP7XqNz/yQEc/D13DPg9dwz7GMUI/xjFCP5C9Mj+eLjk/ni45P/0sMT/9LDE/DkVBPw5FQT/6+wM/7r0ePztOPD+cTKI+nEyiPjcPLT83Dy0/OMsVPyXMGT8lzBk/GqYUPxqmFD/M/SM/zP0jP5BU4j5DYgs/0sELP0pupz5Kbqc+hpUJP4aVCT+DYuk++pnqPvqZ6j4bUsc+G1LHPuu4+j7ruPo+FjGkPtrM5T4eZa4+47lpPuO5aT5uZL0+bmS9Ps9zjD4dO5Y+HTuWPq7/TT6u/00+q82QPqvNkD4tjyc+0CMzPoo9BT6KPQU+eLdIP8cttKxuYBU8hkJNP4ZCTT+GQk0/Uv4CP147KT96Oz8/5b+TPuW/kz52tlA/drZQP+PWSz/G0U8/xtFPP0lgQT9JYEE/rW1UP61tVD/SBTo/JmotP9r4PT91K/8+dSv/PnqEVj96hFY/vtlWP6oxWT+qMVk/k4JPP5OCTz9+k1c/fpNXP2opNj8QY0Q/7otGPzI12T4yNdk+IwpSPyMKUj8OrlM/82hYP/NoWD8IXlA/CF5QPyX8Uz8l/FM/Oc8wP7XqNz/yQEc/D13DPg9dwz7GMUI/xjFCP5C9Mj+eLjk/ni45P/0sMT/9LDE/DkVBPw5FQT/6+wM/7r0ePztOPD+cTKI+nEyiPjcPLT83Dy0/OMsVPyXMGT8lzBk/GqYUPxqmFD/M/SM/zP0jP5BU4j5DYgs/0sELP0pupz5Kbqc+hpUJP4aVCT+DYuk++pnqPvqZ6j4bUsc+G1LHPuu4+j7ruPo+FjGkPtrM5T4eZa4+47lpPuO5aT5uZL0+bmS9Ps9zjD4dO5Y+HTuWPq7/TT6u/00+q82QPqvNkD4tjyc+0CMzPoo9BT6KPQU+tskjP1Ncc6zFnKE7R7gnP0e4Jz9HuCc/u/PXPtxwEj/hlB0/k55ZPpOeWT4vryo/L68qPzAgKD9zOSw/czksP/o/IT/6PyE/WTYvP1k2Lz/Tqxo/SZQOP+VqIj9CYes+QmHrPrepMj+3qTI/GJAzP7qUNj+6lDY/unUuP7p1Lj9TuTM/U7kzP8wfFz8blSE/9qUlP/YUvz72FL8+YeQwP2HkMD+VoDM/39g4P9/YOD9eCjM/XgozP463Mj+OtzI/CmEWPyHoHD/C3Sw/QWaePkFmnj7rISI/6yEiP72VFD862hk/OtoZP7WrDz+1qw8/bX8gP21/ID8IYdg+bvsGP1V5Gj8wOX8+MDl/PrSzDj+0sw4/5Qr3PuCYAT/gmAE/mpT6PpqU+j54Ggg/eBoIP5EsvT5VtOs+3WXtPvavlj72r5Y+Zm7mPmZu5j46sco+kgDRPpIA0T6WALc+lgC3PqWn0T6lp9E+Wn6NPi/Svz5KLJM+XV1QPl1dUD60KZ4+tCmePvqjbD5JR4A+SUeAPvZLMD72SzA+xel3PsXpdz6Orhs+vm4oPosxAz6LMQM+tskjP1Ncc6zFnKE7R7gnP0e4Jz9HuCc/u/PXPtxwEj/hlB0/k55ZPpOeWT4vryo/L68qPzAgKD9zOSw/czksP/o/IT/6PyE/WTYvP1k2Lz/Tqxo/SZQOP+VqIj9CYes+QmHrPrepMj+3qTI/GJAzP7qUNj+6lDY/unUuP7p1Lj9TuTM/U7kzP8wfFz8blSE/9qUlP/YUvz72FL8+YeQwP2HkMD+VoDM/39g4P9/YOD9eCjM/XgozP463Mj+OtzI/CmEWPyHoHD/C3Sw/QWaePkFmnj7rISI/6yEiP72VFD862hk/OtoZP7WrDz+1qw8/bX8gP21/ID8IYdg+bvsGP1V5Gj8wOX8+MDl/PrSzDj+0sw4/5Qr3PuCYAT/gmAE/mpT6PpqU+j54Ggg/eBoIP5EsvT5VtOs+3WXtPvavlj72r5Y+Zm7mPmZu5j46sco+kgDRPpIA0T6WALc+lgC3PqWn0T6lp9E+Wn6NPi/Svz5KLJM+XV1QPl1dUD60KZ4+tCmePvqjbD5JR4A+SUeAPvZLMD72SzA+xel3PsXpdz6Orhs+vm4oPosxAz6LMQM+El5QP4rUA64duVI8UJNUP1CTVD9Qk1Q/xusIP2pBLz9LjUc/8aWbPvGlmz6Fe1c/hXtXP4W8Uj+de1g/nXtYP/PfSz/z30s/SpRcP0qUXD/R10M/hmA1P7hURz825PI+NuTyPm6XXD9ul1w/RWBdP5WIYD+ViGA/Lq1VPy6tVT/q310/6t9dP7aTPT+ndUo/Q1tLP1gr2D5YK9g+qtFWP6rRVj/zd1k/+PdeP/j3Xj8SaVc/EmlXPyJKWT8iSlk/aZwzP3utPj8631A/w6rQPsOq0D5+eUg/fnlIP2unOj/VCkE/1QpBP/YdOj/2HTo/+ANIP/gDSD+qBQc/drglP92CRj/2EKY+9hCmPr4gMz++IDM/5iIdP0lPHz9JTx8/59oXP+faFz+yTSg/sk0oP2j36D59FA0/P3sLP4g1nz6INZ8+WqsLP1qrCz9reuo+4qnoPuKp6D5wysI+cMrCPvPp/T7z6f0+gO+kPjV86j6S7qw+/u9dPv7vXT5Bj70+QY+9PgAEij7rS5M+60uTPlLGRz5Sxkc+MlKOPjJSjj7/7x4+0PwnPvYq8z32KvM9El5QP4rUA64duVI8UJNUP1CTVD9Qk1Q/xusIP2pBLz9LjUc/8aWbPvGlmz6Fe1c/hXtXP4W8Uj+de1g/nXtYP/PfSz/z30s/SpRcP0qUXD/R10M/hmA1P7hURz825PI+NuTyPm6XXD9ul1w/RWBdP5WIYD+ViGA/Lq1VPy6tVT/q310/6t9dP7aTPT+ndUo/Q1tLP1gr2D5YK9g+qtFWP6rRVj/zd1k/+PdeP/j3Xj8SaVc/EmlXPyJKWT8iSlk/aZwzP3utPj8631A/w6rQPsOq0D5+eUg/fnlIP2unOj/VCkE/1QpBP/YdOj/2HTo/+ANIP/gDSD+qBQc/drglP92CRj/2EKY+9hCmPr4gMz++IDM/5iIdP0lPHz9JTx8/59oXP+faFz+yTSg/sk0oP2j36D59FA0/P3sLP4g1nz6INZ8+WqsLP1qrCz9reuo+4qnoPuKp6D5wysI+cMrCPvPp/T7z6f0+gO+kPjV86j6S7qw+/u9dPv7vXT5Bj70+QY+9PgAEij7rS5M+60uTPlLGRz5Sxkc+MlKOPjJSjj7/7x4+0PwnPvYq8z32KvM9iO8pP4K3M63FMqI7cs8tP3LPLT9yzy0/dgbtPhiqGz82cis/JOmQPiTpkD7H/DA/x/wwP9+BLj/UpTM/1KUzP9piKz/aYis/PHc1Pzx3NT/Mnic/uFEdPwMHKz+osfU+qLH1PsbNOj/GzTo/Nxs7PzRBPj80QT4/C8UyPwvFMj+qgDs/qoA7P0k5Ij8jly8/Qo8rP8551z7Oedc+sEQ6P7BEOj+dwz8/ruhEP67oRD9quT4/ark+P/zxOz/88Ts/BUQjP62PLz8r2UA/EODWPhDg1j7ekjM/3pIzPyqnLT8PsTE/D7ExP5P5Ij+T+SI/Ua8xP1GvMT/KdwM/th8SP87wLz+XbLM+l2yzPheoJT8XqCU/DX4dP1ucIT9bnCE/P2AVPz9gFT9Nphw/TaYcP4ip5z7+wAE/bjUFP46Mpj6OjKY+c2sGP3NrBj/W1fY+xi35PsYt+T7AwsY+wMLGPoLu8z6C7vM+j12uPgMU6j6SpK0+6jp4Puo6eD7+D74+/g++PooGkj5LOJ0+SzidPhguVD4YLlQ+UkiVPlJIlT4z9TE+g2Q/PkbLCT5Gywk+Tg4lP4ZfK638Iqc7ulIoP7pSKD+6Uig/zi/SPuHGHj87SSQ/alBVPmpQVT6NQSo/jUEqP0C6KT+9izU/vYs1P8cVMz/HFTM/Eks2PxJLNj/Q2Ck/t80YP3WyOD/edaA+3nWgPkKsMD9CrDA/mho0PyhlOz8oZTs/cd0vP3HdLz/s/zI/7P8yP2o9FD8iNyE/94QkP20clD5tHJQ+UXcpP1F3KT9sEzA/Bu06PwbtOj+F6Dg/heg4P+2NLT/tjS0/WkIOPxYEIz9siz0/jvujPo77oz6D0x4/g9MeP9tfFj+SyRs/kskbP1zZFD9c2RQ/1todP9baHT/+q7k+OmANP7HkHj9NPFc+TTxXPlqECT9ahAk/8BT1PnU39D51N/Q+YEHgPmBB4D511vw+ddb8PiqfsD7NmNY+Yz+/PtrNPT7azT0+6jzIPuo8yD45Oac+yniiPsp4oj6uX4g+rl+IPsvQsz7L0LM++1NePmJEoD5W4Vk+DgntPQ4J7T0qiHY+Koh2PmJoIz566TE+eukxPoBB2T2AQdk9+x8rPvsfKz7b7rY9Na27PYbhhD2G4YQ98OMfP8pysqzapKU7YMsjP2DLIz9gyyM/7Ai6PgBN/j7uohs/SqdLPkqnSz77viY/+74mP7GPJD/aMi0/2jItP+QlKD/kJSg/0TcuP9E3Lj+H9iU/Ir4RP2WGGj+Yk9A+mJPQPrLHLT+yxy0/tvwxP0sINj9LCDY/gsEwP4LBMD9jlDA/Y5QwP6YZID+Sxh8/FS8lP0LZrT5C2a0+3qopP96qKT/2wi4/UCg1P1AoNT/nCDQ/5wg0P4JPLT+CTy0/nSMIP7NAGz/QPys/yCGxPsghsT5qmyE/apshP4I3Fz9gkhs/YJIbPyumFz8rphc/SFciP0hXIj8O4tI+u90FP90gMT+tTZ4+rU2ePo5gEz+OYBM/LewFP+jeCz/o3gs/EGoFPxBqBT+Twgs/k8ILP4lKyD5wFuQ+ILDwPjcClj43ApY+K8PqPivD6j4jH9E+jqfWPo6n1j6Cprk+gqa5Pmp32z5qd9s+oeygPp5G1T5On5w+fXtgPn17YD48uqg+PLqoPpqsgD4CbY0+Am2NPrIKSD6yCkg+JYGJPiWBiT5mfiM+R2AxPgQDAT4EAwE+Sz1BPguEn6xCa1g7xhFJPsYRST7GEUk+KubNPX75ED5TMTM+Sgs2PkoLNj7DMFU+wzBVPq2EQz4GCEY+BghGPt5BND7eQTQ+CO9WPgjvVj4CvlA+sv8cPo5OOD5cRLI+XESyPrY/gz62P4M+M/B/Pqa7dD6mu3Q+5mpuPuZqbj4qDoI+Kg6CPocxgz7tqHU+T1iCPvgoqj74KKo+22OYPttjmD5ripM+TYqMPk2KjD6qdn4+qnZ+Pspzkz7Kc5M+2K9+Pin0kT5b5nc+kBrHPpAaxz4uUrk+LlK5PhU3uj767rQ++u60PlVjmD5VY5g+clO2PnJTtj5shq0+fU2VPgizrD42Sr4+Nkq+PnCMwz5wjMM+O3nKPmLU7T5i1O0+K7/gPiu/4D6uRso+rkbKPosGxT5W+LU+muLjPiib1z4om9c+vuLIPr7iyD4aRdM+89DsPvPQ7D5ggdQ+YIHUPumZxT7pmcU+E5rKPna6tD74hMA+BPm7PgT5uz7MrbM+zK2zPpvKoj4l7rQ+Je60Pn3ufz597n8+zqKkPs6ipD5xeoM+KdaUPqAhcT6gIXE+Sz1BPguEn6xCa1g7xhFJPsYRST7GEUk+KubNPX75ED5TMTM+Sgs2PkoLNj7DMFU+wzBVPq2EQz4GCEY+BghGPt5BND7eQTQ+CO9WPgjvVj4CvlA+sv8cPo5OOD5cRLI+XESyPrY/gz62P4M+M/B/Pqa7dD6mu3Q+5mpuPuZqbj4qDoI+Kg6CPocxgz7tqHU+T1iCPvgoqj74KKo+22OYPttjmD5ripM+TYqMPk2KjD6qdn4+qnZ+Pspzkz7Kc5M+2K9+Pin0kT5b5nc+kBrHPpAaxz4uUrk+LlK5PhU3uj767rQ++u60PlVjmD5VY5g+clO2PnJTtj5shq0+fU2VPgizrD42Sr4+Nkq+PnCMwz5wjMM+O3nKPmLU7T5i1O0+K7/gPiu/4D6uRso+rkbKPosGxT5W+LU+muLjPiib1z4om9c+vuLIPr7iyD4aRdM+89DsPvPQ7D5ggdQ+YIHUPumZxT7pmcU+E5rKPna6tD74hMA+BPm7PgT5uz7MrbM+zK2zPpvKoj4l7rQ+Je60Pn3ufz597n8+zqKkPs6ipD5xeoM+KdaUPqAhcT6gIXE+lNRGP4kdEq5l30k88vdKP/L3Sj/y90o/YpwCP7KaJD8qeD0/Cv2gPgr9oD47+k0/O/pNPwrpSD+4A00/uANNPz38Pz89/D8/+gNSP/oDUj9jnjs/hToqPxqoPD96dv8+enb/PnCMVT9wjFU/e5xVPwA/Vz8AP1c/uNpMP7jaTD/GglY/xoJWP7TvOT85VEM/xthGP2PM5j5jzOY+HRhTPx0YUz8FrlQ/BdZXPwXWVz9IOE8/SDhPP9KmVD/SplQ/BysxPwXmPj9D8kg/AAPjPgAD4z6qDko/qg5KP0UAPj9SsUI/UrFCPymTOD8pkzg/q9FIP6vRSD9CaRA/w30kPxB0Rj+Ni8M+jYvDPnDEOT9wxDk/mk4nPxRnLD8UZyw/2ikkP9opJD9eHTE/Xh0xP+9uAj+O+RM/TRkaP2pUwT5qVME+CpQYPwqUGD+YjQY/07cHP9O3Bz+m6eU+punlPmZXDT9mVw0/KzHNPn8ZAz/mCM8+HvKTPh7ykz7g8t0+4PLdPiCrrD5ue7c+bnu3PtUjgT7VI4E+FoGwPhaBsD7exlw+MEFrPh7JND4eyTQ+lNRGP4kdEq5l30k88vdKP/L3Sj/y90o/YpwCP7KaJD8qeD0/Cv2gPgr9oD47+k0/O/pNPwrpSD+4A00/uANNPz38Pz89/D8/+gNSP/oDUj9jnjs/hToqPxqoPD96dv8+enb/PnCMVT9wjFU/e5xVPwA/Vz8AP1c/uNpMP7jaTD/GglY/xoJWP7TvOT85VEM/xthGP2PM5j5jzOY+HRhTPx0YUz8FrlQ/BdZXPwXWVz9IOE8/SDhPP9KmVD/SplQ/BysxPwXmPj9D8kg/AAPjPgAD4z6qDko/qg5KP0UAPj9SsUI/UrFCPymTOD8pkzg/q9FIP6vRSD9CaRA/w30kPxB0Rj+Ni8M+jYvDPnDEOT9wxDk/mk4nPxRnLD8UZyw/2ikkP9opJD9eHTE/Xh0xP+9uAj+O+RM/TRkaP2pUwT5qVME+CpQYPwqUGD+YjQY/07cHP9O3Bz+m6eU+punlPmZXDT9mVw0/KzHNPn8ZAz/mCM8+HvKTPh7ykz7g8t0+4PLdPiCrrD5ue7c+bnu3PtUjgT7VI4E+FoGwPhaBsD7exlw+MEFrPh7JND4eyTQ+yxMyPw8PJ66TYQQ8Rgc2P0YHNj9GBzY/1Z7zPrieGj92CCs/R1SMPkdUjD7S4Dg/0uA4P2PcNT94pDg/eKQ4P6jZKz+o2Ss/k1s8P5NbPD9+KSc/VUQdP/EZLD9iFwA/YhcAPzM+Qj8zPkI/Th5CP3yWQz98lkM/SY04P0mNOD84bUI/OG1CP+OhJT8BuDE/q/YzP7Ng3T6zYN0+vqFBP76hQT+DI0M/jXBGP41wRj/tPz0/7T89P+s+Qj/rPkI/RugkP3BcLz/itjg/ZmnIPmZpyD6CEjc/ghI3P6sRKz9uTy8/bk8vP9PAIz/TwCM/AQs1PwELNT9JhgQ/khYUP0yWLj+ShLM+koSzPv1LKD/9Syg/TU4XP5XaHj+V2h4/mOMXP5jjFz9WLCI/ViwiP9ri7z6XKAo/SdESP9bhxj7W4cY+/54OP/+eDj9NvAE/smMFP7JjBT8oJuU+KCblPjSzBD80swQ/2grJPuUW+D7i3Mk+uAuZPrgLmT6N6tQ+jerUPh4jqz4ZebU+GXm1Ph63gz4et4M+xkWvPsZFrz4Admg+poh5PurHRT7qx0U+WoI3PwLbPa2+nfs7X4E7P1+BOz9fgTs/+HX/PoroHj/D7i8/ZpqMPmaajD72Mz4/9jM+P2CNOj8KYD0/CmA9P1KBMD9SgTA/Z7NBP2ezQT8qpis/rjAiP4P2MD/GbPs+xmz7Pv5FRj/+RUY/iNFFP5yTRz+ck0c/Pk47Pz5OOz+CFkY/ghZGP6XoJz/1ijQ/4XM2P1Nf2z5TX9s+i0dEP4tHRD/0pUU/o9RJP6PUST/qckA/6nJAPzLbRD8y20Q/c04lP+42Lz/CAT0/R33BPkd9wT5Z9Dc/WfQ3PzuyKj+iZi8/omYvPzIiJD8yIiQ/OOo1PzjqNT9Y8gE/ZbAVP4pkLz+biao+m4mqPstRJz/LUSc/cUgUP/L7Gj/y+xo/csMTP3LDEz8yiCA/MoggPybj6T5S5wg/ghQPPwqyuj4Ksro+YIYLP2CGCz8gV/k+Knf+Pip3/j7Qx9k+0MfZPrg/AT+4PwE/rTe+PiIR8T4SusA+kz+OPpM/jj7FRc0+xUXNPq4Moz5WS6w+VkusPkJgeD5CYHg+Mh6nPjIepz7OtVY+aKZlPpYMND6WDDQ+WoI3PwLbPa2+nfs7X4E7P1+BOz9fgTs/+HX/PoroHj/D7i8/ZpqMPmaajD72Mz4/9jM+P2CNOj8KYD0/CmA9P1KBMD9SgTA/Z7NBP2ezQT8qpis/rjAiP4P2MD/GbPs+xmz7Pv5FRj/+RUY/iNFFP5yTRz+ck0c/Pk47Pz5OOz+CFkY/ghZGP6XoJz/1ijQ/4XM2P1Nf2z5TX9s+i0dEP4tHRD/0pUU/o9RJP6PUST/qckA/6nJAPzLbRD8y20Q/c04lP+42Lz/CAT0/R33BPkd9wT5Z9Dc/WfQ3PzuyKj+iZi8/omYvPzIiJD8yIiQ/OOo1PzjqNT9Y8gE/ZbAVP4pkLz+biao+m4mqPstRJz/LUSc/cUgUP/L7Gj/y+xo/csMTP3LDEz8yiCA/MoggPybj6T5S5wg/ghQPPwqyuj4Ksro+YIYLP2CGCz8gV/k+Knf+Pip3/j7Qx9k+0MfZPrg/AT+4PwE/rTe+PiIR8T4SusA+kz+OPpM/jj7FRc0+xUXNPq4Moz5WS6w+VkusPkJgeD5CYHg+Mh6nPjIepz7OtVY+aKZlPpYMND6WDDQ+mnAVPwo4Uq01lGA78pMYP/KTGD/ykxg/OpDuPspSEj9iIRM/Zp5JPmaeST6tEBo/rRAaP/jCGT/znRo/850aP7xODj+8Tg4/pT8cP6U/HD8QEQg/awQMPxvSEz8wH9M+MB/TPkLDID9CwyA/c7UgP3qaIz96miM/a9oVP2vaFT9VPB8/VTwfPwlKAD8K1hI/QpMOP7ThsD604bA+fokeP36JHj9SHiE/xTEnP8UxJz9Vqx4/VaseP43UHT+N1B0/EowHP1V9Cz/+GR8/Uh+LPlIfiz7tBw8/7QcPP8iaAj+uDQc/rg0HP7qk+D66pPg++CEMP/ghDD80u8Q+qIvqPrp4/j5yXWc+cl1nPjX+/D41/vw+DQfaPrWE5z61hOc+TSjePk0o3j4mdfQ+JnX0PkoArj7Wjdg+xUjePiAzmD4gM5g+TqXVPk6l1T5X+cA+dkjIPnZIyD4S8K4+EvCuPibMxT4mzMU+9cOPPvYuuD77vpI+1rRZPta0WT5c3po+XN6aPhNAcz6HH4E+hx+BPsMMNz7DDDc+neV4Pp3leD7YSiw+9Os3PssBFz7LARc+mnAVPwo4Uq01lGA78pMYP/KTGD/ykxg/OpDuPspSEj9iIRM/Zp5JPmaeST6tEBo/rRAaP/jCGT/znRo/850aP7xODj+8Tg4/pT8cP6U/HD8QEQg/awQMPxvSEz8wH9M+MB/TPkLDID9CwyA/c7UgP3qaIz96miM/a9oVP2vaFT9VPB8/VTwfPwlKAD8K1hI/QpMOP7ThsD604bA+fokeP36JHj9SHiE/xTEnP8UxJz9Vqx4/VaseP43UHT+N1B0/EowHP1V9Cz/+GR8/Uh+LPlIfiz7tBw8/7QcPP8iaAj+uDQc/rg0HP7qk+D66pPg++CEMP/ghDD80u8Q+qIvqPrp4/j5yXWc+cl1nPjX+/D41/vw+DQfaPrWE5z61hOc+TSjePk0o3j4mdfQ+JnX0PkoArj7Wjdg+xUjePiAzmD4gM5g+TqXVPk6l1T5X+cA+dkjIPnZIyD4S8K4+EvCuPibMxT4mzMU+9cOPPvYuuD77vpI+1rRZPta0WT5c3po+XN6aPhNAcz6HH4E+hx+BPsMMNz7DDDc+neV4Pp3leD7YSiw+9Os3PssBFz7LARc+lsc/P0+gS643mz48Yb5DP2G+Qz9hvkM/bvAAPz5mIT9rADc/KICZPiiAmT5WfkY/Vn5GP1tcQj+cZkU/nGZFP/1DOD/9Qzg/Gx9KPxsfSj/DJzQ/u5olP2A8Nz89lfo+PZX6PpYKTj+WCk4/CI9NP/0VTz/9FU8/EmZDPxJmQz/COE4/wjhOP5r0MD+YNTs/bjI/P3Pn3z5z598+ktdLP5LXSz+N6kw/9RNQP/UTUD/WuEY/1rhGP7K9TD+yvUw/4kUrP46uNz91MUE/Iz7QPiM+0D7YzEA/2MxAP2LpMz8GjTg/Bo04P8J+Lj/Cfi4/BR4/PwUePz9NQQk/6FQdPzRjOj/XUbY+11G2PkeNMD9HjTA/qzUdP52bIj+dmyI/fi0bP34tGz9uvSg/br0oP6UZ+D6Ozw0/RQUUP8oevT7KHr0+cTwSP3E8Ej8iHAE/GogCPxqIAj+rKt4+qyrePkVTBz9FUwc/KJrEPk0I+j7zYMY+fHGOPnxxjj7DttQ+w7bUPoSYpj5XiK8+V4ivPmbgeD5m4Hg+xgKpPsYCqT5FDVY+iGpjPoINMj6CDTI+lsc/P0+gS643mz48Yb5DP2G+Qz9hvkM/bvAAPz5mIT9rADc/KICZPiiAmT5WfkY/Vn5GP1tcQj+cZkU/nGZFP/1DOD/9Qzg/Gx9KPxsfSj/DJzQ/u5olP2A8Nz89lfo+PZX6PpYKTj+WCk4/CI9NP/0VTz/9FU8/EmZDPxJmQz/COE4/wjhOP5r0MD+YNTs/bjI/P3Pn3z5z598+ktdLP5LXSz+N6kw/9RNQP/UTUD/WuEY/1rhGP7K9TD+yvUw/4kUrP46uNz91MUE/Iz7QPiM+0D7YzEA/2MxAP2LpMz8GjTg/Bo04P8J+Lj/Cfi4/BR4/PwUePz9NQQk/6FQdPzRjOj/XUbY+11G2PkeNMD9HjTA/qzUdP52bIj+dmyI/fi0bP34tGz9uvSg/br0oP6UZ+D6Ozw0/RQUUP8oevT7KHr0+cTwSP3E8Ej8iHAE/GogCPxqIAj+rKt4+qyrePkVTBz9FUwc/KJrEPk0I+j7zYMY+fHGOPnxxjj7DttQ+w7bUPoSYpj5XiK8+V4ivPmbgeD5m4Hg+xgKpPsYCqT5FDVY+iGpjPoINMj6CDTI+SvfoPgL/Qq0Pg4I7aljwPmpY8D5qWPA+kpOUPkiuvj7YmeM+uFGOPrhRjj5y9vg+cvb4Pqq77z7qVfA+6lXwPm5I2z5uSNs+u1b6PrtW+j7GZ+g+JjrJPkhF3T6y5r4+sua+PoqdBD+KnQQ/xdAEP20uAz9tLgM/mID7PpiA+z5huwQ/YbsEP1PK9z59iPE+6un7PsJ2qz7Cdqs+9FEGP/RRBj8mbQc/iiAGP4ogBj/YbAE/2GwBP4HwBT+B8AU/DtrgPu4i9z7DEAA/3iLBPt4iwT6h8go/ofIKP4YSCT9mwAg/ZsAIPwLC9z4Cwvc+m8sIP5vLCD9bRd8+dsnOPggRCj/q9NI+6vTSPurdCj/q3Qo//oULP2WeET9lnhE/wg4HP8IOBz9QnAc/UJwHP70i3z5aHdk+2KH7Pj59tj4+fbY+Cpf7PgqX+z76zPY+AH34PgB9+D5JWMc+SVjHPipK7z4qSu8+1aPOPlJf5D5ju80+0KCtPtCgrT6OEtQ+jhLUPr6yuz4YVMM+GFTDPuRBkD7kQZA+vi65Pr4uuT6IL3U+m0uEPu95ST7veUk+2X4OP6MKoK3eeeo7ZXQRP2V0ET9ldBE/YNKuPsOACD+uwA4/AiVMPgIlTD5TSRM/U0kTP/WFET8r5ho/K+YaP9HtFj/R7RY/7oMcP+6DHD/fZxM/vVAEP5rmGD9uZos+bmaLPqDzFz+g8xc/qoUbP+hiIT/oYiE/vJwWP7ycFj/G/xg/xv8YPwjCAz/CLAk/CfgKP2LVdD5i1XQ+1TcQP9U3ED8FfBU/Ip4fPyKeHz9LeB4/S3geP7ZvEj+2bxI/jVTiPj7NAz/4/Rw/oI+LPqCPiz7D8gc/w/IHP707/j7kowM/5KMDPw2sAD8NrAA/06oHP9OqBz9KSqA+k+vvPjonDD/gtng+4LZ4Puuu9T7rrvU+S3fbPvuS3j77kt4+myHQPpsh0D690Oc+vdDnPgP2qT5K6rw+YPW0PrpZRT66WUU+lXC+PpVwvj6hdaA+MqibPjKomz6yXYQ+sl2EPpomsj6aJrI+olNwPh3Roj4gm2g+9cEPPvXBDz560oE+etKBPvZ6PD5tykk+bcpJPv0oAT79KAE+6ME9PujBPT4iv+A9+xnoPaJCrD2iQqw9GK/tPlAAfq13VZo766bzPuum8z7rpvM+y1u/PkIE4D7usuw+iO5APojuQD49qvc+Par3PrPg+D5QGvM+UBrzPq302D6t9Ng+lbD1PpWw9T6KX9M+NUbhPjjJ5D4O0N4+DtDePhbGBD8WxgQ/GbEDP42aAz+NmgM/c9nvPnPZ7z7OkAI/zpACP+vB0z5wy+0+KNXzPvIquj7yKro+SGEIP0hhCD810gc/xIMIP8SDCD+KLfs+ii37PoHuBT+B7gU/K6HpPpOU7j47TfU+VB6MPlQejD5q5vw+aub8PiV06T5ebuw+Xm7sPi3S1T4t0tU+gzf3PoM39z4ggMU++BTDPpIQ4j6acp4+mnKePuLd8D7i3fA+IJfcPnbA8j52wPI+nnfuPp537j6+M/I+vjPyPsyJvT5Fc9c+oOb5PmIBzj5iAc4+y+vjPsvr4z5AruE+sCfyPrAn8j5aadk+WmnZPmPg2j5j4No+S1LBPs3J0j736rw+BAalPgQGpT56Hr0+eh69PhRnpD46264+OtuuPv/Fhz7/xYc+KrirPiq4qz6tLoM+BnePPvibbT74m20+zOooPpAFNqsKSHk7Bv4vPgb+Lz4G/i8+uX4EPrSOCj7gCCg+ohAnPqIQJz6jrTY+o602PkM6KT4eeyk+HnspPhAwGD4QMBg+0GE1PtBhNT5MzRY+4EIcPiglNz7WTrk+1k65Pppmbj6aZm4+aIpePj6bUj4+m1I+zrQ9Ps60PT6YVGU+mFRlPgZ5Vj52JkQ+889rPhuArz4bgK8+bUiSPm1Ikj4mZ4g+jWd7Po1nez4B+UY+AflGPsC3ij7At4o+S4F+PiNtZD7uyls+fp56Pn6eej56Aps+egKbPjvhmT5eTpU+Xk6VPqNNeD6jTXg+RSCYPkUgmD7dpps+JvhxPlqEiz42Nqc+NjanProFrD66Baw+gmC1PmCA0j5ggNI+C3jaPgt42j5Tr7w+U6+8Ppk5qz7amLU+gKPqPk0O8z5NDvM+49HMPuPRzD4tQ+c+egcAP3oHAD+GrAM/hqwDP7731D6+99Q+YCHsPuUW0T5WW+4+bhT0Pm4U9D6w690+sOvdPstE0T4TSeA+E0ngPpnNwD6ZzcA+FsbrPhbG6z6qM+A+4/zyPkPW0T5D1tE+zOooPpAFNqsKSHk7Bv4vPgb+Lz4G/i8+uX4EPrSOCj7gCCg+ohAnPqIQJz6jrTY+o602PkM6KT4eeyk+HnspPhAwGD4QMBg+0GE1PtBhNT5MzRY+4EIcPiglNz7WTrk+1k65Pppmbj6aZm4+aIpePj6bUj4+m1I+zrQ9Ps60PT6YVGU+mFRlPgZ5Vj52JkQ+889rPhuArz4bgK8+bUiSPm1Ikj4mZ4g+jWd7Po1nez4B+UY+AflGPsC3ij7At4o+S4F+PiNtZD7uyls+fp56Pn6eej56Aps+egKbPjvhmT5eTpU+Xk6VPqNNeD6jTXg+RSCYPkUgmD7dpps+JvhxPlqEiz42Nqc+NjanProFrD66Baw+gmC1PmCA0j5ggNI+C3jaPgt42j5Tr7w+U6+8Ppk5qz7amLU+gKPqPk0O8z5NDvM+49HMPuPRzD4tQ+c+egcAP3oHAD+GrAM/hqwDP7731D6+99Q+YCHsPuUW0T5WW+4+bhT0Pm4U9D6w690+sOvdPstE0T4TSeA+E0ngPpnNwD6ZzcA+FsbrPhbG6z6qM+A+4/zyPkPW0T5D1tE+6qoxP3/UO66HrCw8SKE1P0ihNT9IoTU/UjPwPtLwEj8+Ayk/UymdPlMpnT79azg//Ws4Px8+Mz+6NDU/ujQ1P5hfKD+YXyg/rgY7P64GOz+dDCY/aD4XP4v+KT+af/0+mn/9PjuCQT87gkE/0dE/PwIIQD8CCEA/C780Pwu/ND+IEkE/iBJBP3M3KD92Ci4/2Ko0PzMC6T4zAuk+Os5CPzrOQj/KQ0I/7flCP+35Qj9IrDc/SKw3P8WMQj/FjEI/clgjP0CWLT9qgjQ/o3bSPqN20j7qTTs/6k07PwDzLz9SEDM/UhAzPwZQJz8GUCc/S0A5P0tAOT8LSAs/PRkYPzhfND9Vrss+Va7LPiNiMT8jYjE/Y1YhP76nJz++pyc/wgAhP8IAIT/+LCw//iwsP3r8Aj+OnhA/y+cdP9AA3T7QAN0+3oEcP96BHD894A8/nosRP56LET+FLQA/hS0AP3LsFD9y7BQ/eCXtPmsZCz+2VO0+5cO3PuXDtz4Vkvo+FZL6PlWszj7Ggdc+xoHXPpivoj6Yr6I+wsDSPsLA0j76tJk+I8WgPqdYhT6nWIU+6qoxP3/UO66HrCw8SKE1P0ihNT9IoTU/UjPwPtLwEj8+Ayk/UymdPlMpnT79azg//Ws4Px8+Mz+6NDU/ujQ1P5hfKD+YXyg/rgY7P64GOz+dDCY/aD4XP4v+KT+af/0+mn/9PjuCQT87gkE/0dE/PwIIQD8CCEA/C780Pwu/ND+IEkE/iBJBP3M3KD92Ci4/2Ko0PzMC6T4zAuk+Os5CPzrOQj/KQ0I/7flCP+35Qj9IrDc/SKw3P8WMQj/FjEI/clgjP0CWLT9qgjQ/o3bSPqN20j7qTTs/6k07PwDzLz9SEDM/UhAzPwZQJz8GUCc/S0A5P0tAOT8LSAs/PRkYPzhfND9Vrss+Va7LPiNiMT8jYjE/Y1YhP76nJz++pyc/wgAhP8IAIT/+LCw//iwsP3r8Aj+OnhA/y+cdP9AA3T7QAN0+3oEcP96BHD894A8/nosRP56LET+FLQA/hS0AP3LsFD9y7BQ/eCXtPmsZCz+2VO0+5cO3PuXDtz4Vkvo+FZL6PlWszj7Ggdc+xoHXPpivoj6Yr6I+wsDSPsLA0j76tJk+I8WgPqdYhT6nWIU+9oohP6efGK7adQU8GUolPxlKJT8ZSiU/C+HdPm75Bz86rxs/UuKSPlLikj7OCyg/zgsoPznbIz+KWiY/ilomP30rGz99Kxs/m9oqP5vaKj9Wbhk/uPwMP2dpGz8zJvQ+Myb0Pt69MT/evTE/YhQxP2qTMT9qkzE/SqUnP0qlJz8jgDE/I4AxP37iHD+pjh8/IOYlPy0x3j4tMd4++xc0P/sXND+1ijQ//nY1P/52NT8uHys/Lh8rP87bMz/O2zM/sLgXP7olHz+Ypyg//Y/IPv2PyD4rWC4/K1guP2v/JD92mCc/dpgnP9aqHD/Wqhw/q8wsP6vMLD9bQgQ/HpsOP7mZKj/geM0+4HjNPgQ5KD8EOSg/eAkcP8b3Ij/G9yI/ttUcP7bVHD9vnyQ/b58kPwZ9/j7QiQo/YjkaP2M74z5jO+M+1ZAYP9WQGD8UuQ8/FvIRPxbyET+KbAM/imwDP7aNEz+2jRM/Cmn2Psa5DD+aw/Q+1GDEPtRgxD7jzf8+483/PkNX2D5VJOE+VSThPj0YsD49GLA+c8XePnPF3j4Z1qc+r1+vPlU8kj5VPJI+muQfP+/vNK2ABPA73bkjP925Iz/duSM/ALvYPi1GBD9TkRk/DViQPg1YkD4BgCY/AYAmP86YIT/ywCQ/8sAkP+8KGj/vCho/WrkpP1q5KT/wfhk/MxUNP5uuGT8aW+0+GlvtPmvTLz9r0y8/HYEvP5xCMD+cQjA/Vo4mP1aOJj/Goy8/xqMvP3+dGz9Ofx0/3OIjP0qH2z5Kh9s+lsIxP5bCMT+QXTI/LuozPy7qMz8AWio/AFoqPx7aMT8e2jE/L7QSP1LyHD+T2yc/ecbHPnnGxz6Sciw/knIsP3urIj+WQiU/lkIlP1mzGj9Zsxo/ABQrPwAUKz/HeAE/UUEMP1rgKj9wLtY+cC7WPnGeJj9xniY/s7saP76AIz++gCM/rlYdP65WHT/pviM/6b4jP8pf/j5cVQk/fCwbP2Zm5j5mZuY+fscXP37HFz8DMA8/UqQSP1KkEj8KUwU/ClMFP6bjEz+m4xM/7Un+Po6hDz+F6Po+0wDNPtMAzT4AogE/AKIBP54i3j72qeg+9qnoPrbPuT62z7k+dZ7nPnWe5z6PpK4+q3+3Pv5TmD7+U5g+muQfP+/vNK2ABPA73bkjP925Iz/duSM/ALvYPi1GBD9TkRk/DViQPg1YkD4BgCY/AYAmP86YIT/ywCQ/8sAkP+8KGj/vCho/WrkpP1q5KT/wfhk/MxUNP5uuGT8aW+0+GlvtPmvTLz9r0y8/HYEvP5xCMD+cQjA/Vo4mP1aOJj/Goy8/xqMvP3+dGz9Ofx0/3OIjP0qH2z5Kh9s+lsIxP5bCMT+QXTI/LuozPy7qMz8AWio/AFoqPx7aMT8e2jE/L7QSP1LyHD+T2yc/ecbHPnnGxz6Sciw/knIsP3urIj+WQiU/lkIlP1mzGj9Zsxo/ABQrPwAUKz/HeAE/UUEMP1rgKj9wLtY+cC7WPnGeJj9xniY/s7saP76AIz++gCM/rlYdP65WHT/pviM/6b4jP8pf/j5cVQk/fCwbP2Zm5j5mZuY+fscXP37HFz8DMA8/UqQSP1KkEj8KUwU/ClMFP6bjEz+m4xM/7Un+Po6hDz+F6Po+0wDNPtMAzT4AogE/AKIBP54i3j72qeg+9qnoPrbPuT62z7k+dZ7nPnWe5z6PpK4+q3+3Pv5TmD7+U5g+oCIPP9aw4ay3crQ7PJESPzyREj88kRI/ozHIPmYa9j7gXQw/ZeRsPmXkbD7F2hQ/xdoUP3C9ET/L2RY/y9kWP++yDz/vsg8/yq0ZP8qtGT/q/w4/f3UGP/9KCz9hWLs+YVi7PpKEGz+ShBs/Y5wdP0AvID9ALyA/z3QXP890Fz9y2hs/ctobP6mACj8y4As/dDgOP4hJsD6ISbA+mX8aP5l/Gj/Kyx0/pTEiP6UxIj/dUBw/3VAcPzmcGz85nBs/Ov38Po+WBj/iBxs/RESvPkRErz4n8RU/J/EVP5aEDj9LvBE/S7wRP+iTCz/okws/dPkVP3T5FT9LOtU+PeL9Pr4YGj8nB68+JwevPj/rDz8/6w8/mWAGP7ibCz+4mws/OM0EPzjNBD+rNgw/qzYMP90O2T62M+o+W4n+Pvhksz74ZLM+yjr+Pso6/j4Nw+s+gILtPoCC7T5Ic9s+SHPbPkJD+j5CQ/o+Sh3SPgZ5+D7xiMo+XZqdPl2anT51s9Y+dbPWPvnDsj7FZ7s+xWe7PhZ8lz4WfJc+U5O+PlOTvj75Pog+ao+NPq1yZD6tcmQ+oCIPP9aw4ay3crQ7PJESPzyREj88kRI/ozHIPmYa9j7gXQw/ZeRsPmXkbD7F2hQ/xdoUP3C9ET/L2RY/y9kWP++yDz/vsg8/yq0ZP8qtGT/q/w4/f3UGP/9KCz9hWLs+YVi7PpKEGz+ShBs/Y5wdP0AvID9ALyA/z3QXP890Fz9y2hs/ctobP6mACj8y4As/dDgOP4hJsD6ISbA+mX8aP5l/Gj/Kyx0/pTEiP6UxIj/dUBw/3VAcPzmcGz85nBs/Ov38Po+WBj/iBxs/RESvPkRErz4n8RU/J/EVP5aEDj9LvBE/S7wRP+iTCz/okws/dPkVP3T5FT9LOtU+PeL9Pr4YGj8nB68+JwevPj/rDz8/6w8/mWAGP7ibCz+4mws/OM0EPzjNBD+rNgw/qzYMP90O2T62M+o+W4n+Pvhksz74ZLM+yjr+Pso6/j4Nw+s+gILtPoCC7T5Ic9s+SHPbPkJD+j5CQ/o+Sh3SPgZ5+D7xiMo+XZqdPl2anT51s9Y+dbPWPvnDsj7FZ7s+xWe7PhZ8lz4WfJc+U5O+PlOTvj75Pog+ao+NPq1yZD6tcmQ+7CkrPxJPP65+uxg8xgEvP8YBLz/GAS8/0y7sPoOEDj9mKyM/sAKWPrAClj4pZjE/KWYxP149LD+F9i4/hfYuP4KqIz+CqiM/SNE0P0jRND/lPCI/358VP0gGJT+mbvE+pm7xPh/nOT8f5zk//Yo4P5YLOT+WCzk/uB8tP7gfLT8cEjk/HBI5PzSSIj/xPCY/HDosP8MH3j7DB94+Fnw6PxZ8Oj+fZzo/jBM8P4wTPD/CKDE/wigxP7s/Oj+7Pzo/FnQbP+6HJD91ky8/5jnAPuY5wD5oUDI/aFAyP2DNJj/+ECo//hAqP+aXHz/mlx8/ZdIwP2XSMD8OfwM/0I8TP6J3LT+3yL8+t8i/PgArKT8AKyk/AFsZP90DHz/dAx8/zcsXP83LFz9SeyQ/UnskPwv8/T69Zgo/slMWP8bJ0D7GydA+EIsVPxCLFT+W8Qg/fKMJP3yjCT8FQ/Y+BUP2PgCCDz8Agg8/8NHlPnpJBz8iC+Q+SN2uPkjdrj6lUvI+pVLyPtG2yD6AyM8+gMjPPqC/nz6gv58+eknNPnpJzT7eKZQ+XvqZPr42fz6+Nn8+7CkrPxJPP65+uxg8xgEvP8YBLz/GAS8/0y7sPoOEDj9mKyM/sAKWPrAClj4pZjE/KWYxP149LD+F9i4/hfYuP4KqIz+CqiM/SNE0P0jRND/lPCI/358VP0gGJT+mbvE+pm7xPh/nOT8f5zk//Yo4P5YLOT+WCzk/uB8tP7gfLT8cEjk/HBI5PzSSIj/xPCY/HDosP8MH3j7DB94+Fnw6PxZ8Oj+fZzo/jBM8P4wTPD/CKDE/wigxP7s/Oj+7Pzo/FnQbP+6HJD91ky8/5jnAPuY5wD5oUDI/aFAyP2DNJj/+ECo//hAqP+aXHz/mlx8/ZdIwP2XSMD8OfwM/0I8TP6J3LT+3yL8+t8i/PgArKT8AKyk/AFsZP90DHz/dAx8/zcsXP83LFz9SeyQ/UnskPwv8/T69Zgo/slMWP8bJ0D7GydA+EIsVPxCLFT+W8Qg/fKMJP3yjCT8FQ/Y+BUP2PgCCDz8Agg8/8NHlPnpJBz8iC+Q+SN2uPkjdrj6lUvI+pVLyPtG2yD6AyM8+gMjPPqC/nz6gv58+eknNPnpJzT7eKZQ+XvqZPr42fz6+Nn8+kv6/Pi44w6xOUEU7zlnEPs5ZxD7OWcQ+LjN5PthzlT6UJLY+dvVYPnb1WD4HTcg+B03IPjpfvj7qyr8+6sq/PrgBsT64AbE+3V7JPt1eyT4iCLQ+cxehPohSsj5ONcM+TjXDPrjU3D641Nw+TqvaPlWC1T5VgtU+e1fJPntXyT77vdo++73aPiIjyT6olbw+febSPmUCsz5lArM+baXqPm2l6j4CZeU+aoffPmqH3z4+gcs+PoHLPt3y5j7d8uY+JN64PlKcxj7qUcs+u6iFPruohT61xus+tcbrPlZH4D4lo98+JaPfPnNkyT5zZMk+re7oPq3u6D4bI8g+I9i5Pipw7T4LO9Y+CzvWPogK/T6ICv0+vsD4PiWlBj8lpQY/xQAAP8UAAD/+0QE//tEBP8ZB2T5TddM+vMEJP+pJ8T7qSfE+O/AEPzvwBD/IMwk/7yYNP+8mDT+JLQc/iS0HP/bzCD/28wg/jr4LP074Bz+a4gc/DUH4Pg1B+D7CoQY/wqEGPyB4AD+RPAM/kTwDP2VN4z5lTeM+eQwDP3kMAz/NAdw+SMrlPqc+xj6nPsY+5wMLP2U21qz4JdE7/gwOP/4MDj/+DA4/W1S0PjVX6T51JwU/AvVnPgL1Zz5mTRA/Zk0QPy47DT/78w8/+/MPP/hQBz/4UAc/E1EUPxNRFD96UAk/MJP1PgXbAD/DhKg+w4SoPoVcFD+FXBQ/YxYWP3INFz9yDRc/TpgPP06YDz8k6RQ/JOkUP20VAz8/hQM/4tEJP8ZFlj7GRZY+d78RP3e/ET8jPRI/GkwWPxpMFj+7YhA/u2IQP4JwEj+CcBI/ZijqPhIM9j7hqwY/Kq12Piqtdj692gg/vdoIPztL+D6HVgA/h1YAP+Mo+j7jKPo+u+AIP7vgCD8iKb0++8ToPiQDCT9zeJY+c3iWPlE6Az9ROgM/AJXjPtI86j7SPOo+CjnlPgo55T4eqgA/HqoAP36byz4Iw9g+tmfkPgZDmj4GQ5o+xe3nPsXt5z7wYcg+TH3DPkx9wz7bRrg+20a4Po7r4z6O6+M+/5exPiqD0D6dVbE+iA2DPogNgz5gRMA+YETAPiSooj5a76Q+Wu+kPiacgj4mnII+KieiPionoj7Ty2M+6wtoPsNnQj7DZ0I+wNntPs7bL60g0t47wJzzPsCc8z7AnPM+yna6PnVP3D5wdes+D3hDPg94Qz6FGvc+hRr3Pjhi8z4dYPs+HWD7Pvgy7z74Mu8+LZL/Pi2S/z4o5uQ+2ibxPl0y9z72y8M+9svDPn55BT9+eQU/YwwFPyN2Bj8jdgY/zYL0Ps2C9D4YqgM/GKoDP3BL5T4+Pu0+9QXxPqLJtT6iybU+sxgIP7MYCD8HZQk/rzIMP68yDD821wE/NtcBPzirBj84qwY/kGbmPlo76z5rmwk/81KPPvNSjz6jlgE/o5YBP+vZ9z7aLfs+2i37PipB5T4qQeU+7ef/Pu3n/z4hyMI+IJXWPhC4+D5SBZs+UgWbPs0b+z7NG/s+zffxPnob/j56G/4+CkrxPgpK8T4lQ/g+JUP4Phhxyj4tsdU+s+3xPj0jvz49I78+vlbrPr5W6z77sek+KtHvPirR7z6jMOQ+ozDkPjZy6T42cuk+JvHWPvaw4j4LMtQ+COG0PgjhtD7ABNU+wATVPiiVuj6e6L8+nui/PqY/nz6mP58+GE/GPhhPxj6aa6g+AlCvPmhslz5obJc+PU0wPkIblSxTOQY7ZTo0PmU6ND5lOjQ+u7gmPkAWKz7OnTE+iiUDPoolAz7MUjM+zFIzPli/LT7zei4+83ouPnpfIj56XyI+sDs4PrA7OD4ALR0+sloqPluQUj77LpU++y6VPkJjZz5CY2c+tVNaPi6QTz4ukE8+s5cuPrOXLj5WAVw+VgFcPn75UD4dmzc+usVcPi33nT4t950+xkWKPsZFij59sIQ+Bkd2PgZHdj6yzzY+ss82PkregT5K3oE+HteLProQZT6aUGo+6n5+Pup+fj6OMpc+jjKXPj3XoT5Isp4+SLKePvaEfD72hHw+CGCVPghglT7rtpE+I7+WPmIxhD4vqZQ+L6mUPoMvqz6DL6s+2ybAPpM+0T6TPtE+9pTQPvaU0D7gNbs+4DW7PkIGsD64wLU+rgTTPoRKwj6ESsI+uEPJPrhDyT4CK+8+g6v+PoOr/j4CyQM/AskDP63C1z6twtc+uFX2Pgsk2D7gk+U+rlTiPq5U4j77++I++/viPrVq1D7DTeE+w03hPiYwwj4mMMI+Hd3rPh3d6z4NsfI+s/T6Pko+5T5KPuU+PU0wPkIblSxTOQY7ZTo0PmU6ND5lOjQ+u7gmPkAWKz7OnTE+iiUDPoolAz7MUjM+zFIzPli/LT7zei4+83ouPnpfIj56XyI+sDs4PrA7OD4ALR0+sloqPluQUj77LpU++y6VPkJjZz5CY2c+tVNaPi6QTz4ukE8+s5cuPrOXLj5WAVw+VgFcPn75UD4dmzc+usVcPi33nT4t950+xkWKPsZFij59sIQ+Bkd2PgZHdj6yzzY+ss82PkregT5K3oE+HteLProQZT6aUGo+6n5+Pup+fj6OMpc+jjKXPj3XoT5Isp4+SLKePvaEfD72hHw+CGCVPghglT7rtpE+I7+WPmIxhD4vqZQ+L6mUPoMvqz6DL6s+2ybAPpM+0T6TPtE+9pTQPvaU0D7gNbs+4DW7PkIGsD64wLU+rgTTPoRKwj6ESsI+uEPJPrhDyT4CK+8+g6v+PoOr/j4CyQM/AskDP63C1z6twtc+uFX2Pgsk2D7gk+U+rlTiPq5U4j77++I++/viPrVq1D7DTeE+w03hPiYwwj4mMMI+Hd3rPh3d6z4NsfI+s/T6Pko+5T5KPuU+P18JP31Ow63bm9E7U3MMP1NzDD9Tcww/+p7GPsOt4T6voQI/sneGPrJ3hj661A0/utQNPwSxCD/u6Qk/7ukJPz9tAD8/bQA/tQEQP7UBED9+6P8+MwbsPi0HBj+Qxso+kMbKPup7Fj/qexY/bskTP7rKEj+6yhI/LVIHPy1SBz/rlBQ/65QUP5AGBj/KLgM/KdoLPza1xz42tcc+TrIZP06yGT+CUhg/8BUXP/AVFz/wcAk/8HAJP4a/Fz+Gvxc/MokDPzjhAz/ykA0/dnutPnZ7rT72rBY/9qwWP1Z9ED9v2hE/b9oRPw3CBT8NwgU/XWQVP11kFT9Lt+c+V+wAPzgdET8yWLw+Mli8PlqnFT9apxU/Vj8OP3G5ET9xuRE/JmsLPyZrCz8XTRQ/F00UPwjc8j4z//o+xgIKP5v1xz6b9cc+BNwMPwTcDD/h2Qc/IssGPyLLBj+AIP4+gCD+PpkNDT+ZDQ0/1av7PrqYCD8mqvU+s5XIPrOVyD5qwQE/asEBP2Xg3z5W5OI+VuTiPlMwvT5TML0+/eXqPv3l6j7ZccY+CsnIPpZUsj6WVLI+P18JP31Ow63bm9E7U3MMP1NzDD9Tcww/+p7GPsOt4T6voQI/sneGPrJ3hj661A0/utQNPwSxCD/u6Qk/7ukJPz9tAD8/bQA/tQEQP7UBED9+6P8+MwbsPi0HBj+Qxso+kMbKPup7Fj/qexY/bskTP7rKEj+6yhI/LVIHPy1SBz/rlBQ/65QUP5AGBj/KLgM/KdoLPza1xz42tcc+TrIZP06yGT+CUhg/8BUXP/AVFz/wcAk/8HAJP4a/Fz+Gvxc/MokDPzjhAz/ykA0/dnutPnZ7rT72rBY/9qwWP1Z9ED9v2hE/b9oRPw3CBT8NwgU/XWQVP11kFT9Lt+c+V+wAPzgdET8yWLw+Mli8PlqnFT9apxU/Vj8OP3G5ET9xuRE/JmsLPyZrCz8XTRQ/F00UPwjc8j4z//o+xgIKP5v1xz6b9cc+BNwMPwTcDD/h2Qc/IssGPyLLBj+AIP4+gCD+PpkNDT+ZDQ0/1av7PrqYCD8mqvU+s5XIPrOVyD5qwQE/asEBP2Xg3z5W5OI+VuTiPlMwvT5TML0+/eXqPv3l6j7ZccY+CsnIPpZUsj6WVLI+Y7zXPoNB+6w6p7U7GyjdPhso3T4bKN0+loGhPrhtuz5LntU+XrNvPl6zbz54jt8+eI7fPjYb2T6iL98+oi/fPuDX1D7g19Q+RijmPkYo5j4ai9E+to3FPgLb3T4Vh54+FYeePmMS8D5jEvA+zdHtPkLI7T5CyO0+pXbaPqV22j6eRu0+nkbtPmux2j4l4tM+Te3dPg4xnz4OMZ8+Hkv1Ph5L9T5znPY+o1H2PqNR9j5ug94+boPePqrF8T6qxfE+fkbaPgV6zz7wau0+ws2oPsLNqD5KQPo+SkD6PqOi+j6g8vs+oPL7Ppsm6D6bJug+IHL6PiBy+j6BK78+w73ePnrr+z46Pq8+Oj6vPny0AD98tAA/L4sDPzIsAz8yLAM/qxP3PqsT9z5Qt/4+ULf+PkoY2T6KeNw+SMvlPjkTpT45E6U+/VvxPv1b8T74Ie8+0NLoPtDS6D5zwNk+c8DZPrDh8z6w4fM+VcDePkrc8j4iKdE+WI6qPliOqj6uS+E+rkvhPqapvz6q8sI+qvLCPooDqD6KA6g+btnRPm7Z0T5vi6Y+XQKpPr+ukD6/rpA+cqrRPla/cayxJqM7Kh/XPiof1z4qH9c+uKSdPrYZuj4A9NE+w65uPsOubj7bU9k+21PZPsZp0z56nNo+epzaPhCP0D4Qj9A+LYrgPi2K4D7dp80+QNbDPjD42T6BQpA+gUKQPugZ6D7oGeg+q13nPv3K6D79yug+ngzWPp4M1j7jyuU+48rlPnr80j4q0M8+3XLVPs0tkz7NLZM+Q/HqPkPx6j6dJu4+g/PvPoPz7z6K/Nk+ivzZPtYI6D7WCOg+hYjQPqbTxz71L+k+IDSqPiA0qj4DSvE+A0rxPjoP8z7TCfU+0wn1Pi784j4u/OI+RvbxPkb28T5BDLQ+EPPXPjVM9j5qU7c+alO3Pogj+j6II/o+qpMBP1qnAT9apwE/HfLzPh3y8z67y/c+u8v3Ploy1T6Lvdc+usLcPnYfnT52H50+msPoPprD6D7qn+Q+VbHfPlWx3z6wVdI+sFXSPgBM7D4ATOw+Mm7cPuil8T4GoMw+rXqnPq16pz5yP9s+cj/bPgPKuz70CsA+9ArAPgLAqj4CwKo+U9/QPlPf0D6cGqE+knOkPk5Aij5OQIo+cqrRPla/cayxJqM7Kh/XPiof1z4qH9c+uKSdPrYZuj4A9NE+w65uPsOubj7bU9k+21PZPsZp0z56nNo+epzaPhCP0D4Qj9A+LYrgPi2K4D7dp80+QNbDPjD42T6BQpA+gUKQPugZ6D7oGeg+q13nPv3K6D79yug+ngzWPp4M1j7jyuU+48rlPnr80j4q0M8+3XLVPs0tkz7NLZM+Q/HqPkPx6j6dJu4+g/PvPoPz7z6K/Nk+ivzZPtYI6D7WCOg+hYjQPqbTxz71L+k+IDSqPiA0qj4DSvE+A0rxPjoP8z7TCfU+0wn1Pi784j4u/OI+RvbxPkb28T5BDLQ+EPPXPjVM9j5qU7c+alO3Pogj+j6II/o+qpMBP1qnAT9apwE/HfLzPh3y8z67y/c+u8v3Ploy1T6Lvdc+usLcPnYfnT52H50+msPoPprD6D7qn+Q+VbHfPlWx3z6wVdI+sFXSPgBM7D4ATOw+Mm7cPuil8T4GoMw+rXqnPq16pz5yP9s+cj/bPgPKuz70CsA+9ArAPgLAqj4CwKo+U9/QPlPf0D6cGqE+knOkPk5Aij5OQIo+auuTPmqCzCvztW47Yj+YPmI/mD5iP5g+U7hvPh7BjT4ZNZs+dqw0PnasND7ra5k+62uZPp7jlj4+eZ8+PnmfPtdLnD7XS5w+QIGhPkCBoT6dAJg+KZOWPibooz4AvDQ+ALw0PsIbpD7CG6Q+qiKlPlI0qD5SNKg+j9CYPo/QmD6NfaI+jX2iPouclj7LR5Y+q/STPtCnQj7Qp0I+MECkPjBApD7eE6o+a8WtPmvFrT4Ta5w+E2ucPmYHoj5mB6I+L8ubPi+iiT5X268+RR6QPkUekD7FfK8+xXyvPvTkuT6aZbs+mmW7Pg/Trz4P068+xmWyPsZlsj6NaH4+XiaoPuysuj5QLIE+UCyBPt6ktz7epLc+CmfLPn7Wwj5+1sI+3hGxPt4RsT6zcrI+s3KyPsaTnj7ufaA+81SSPt1JPT7dST0+eLGhPnixoT6b658+ReCXPkXglz437oo+N+6KPuRboj7kW6I+5NqSPnJBrD7yu34+LINDPiyDQz5d8Y8+XfGPPh4PZT6tkm0+rZJtPvLBWT7ywVk+/2eKPv9nij4T5TM+TN45PrJJCz6ySQs+auuTPmqCzCvztW47Yj+YPmI/mD5iP5g+U7hvPh7BjT4ZNZs+dqw0PnasND7ra5k+62uZPp7jlj4+eZ8+PnmfPtdLnD7XS5w+QIGhPkCBoT6dAJg+KZOWPibooz4AvDQ+ALw0PsIbpD7CG6Q+qiKlPlI0qD5SNKg+j9CYPo/QmD6NfaI+jX2iPouclj7LR5Y+q/STPtCnQj7Qp0I+MECkPjBApD7eE6o+a8WtPmvFrT4Ta5w+E2ucPmYHoj5mB6I+L8ubPi+iiT5X268+RR6QPkUekD7FfK8+xXyvPvTkuT6aZbs+mmW7Pg/Trz4P068+xmWyPsZlsj6NaH4+XiaoPuysuj5QLIE+UCyBPt6ktz7epLc+CmfLPn7Wwj5+1sI+3hGxPt4RsT6zcrI+s3KyPsaTnj7ufaA+81SSPt1JPT7dST0+eLGhPnixoT6b658+ReCXPkXglz437oo+N+6KPuRboj7kW6I+5NqSPnJBrD7yu34+LINDPiyDQz5d8Y8+XfGPPh4PZT6tkm0+rZJtPvLBWT7ywVk+/2eKPv9nij4T5TM+TN45PrJJCz6ySQs+GFXePk1UhK3KeGM72OPjPtjj4z7Y4+M+ykeiPibMtD5O1tQ+zZ5iPs2eYj4W6+U+FuvlPuu/3D57juA+e47gPrPn0j6z59I+Ou3qPjrt6j61NtI+ZRfIPioL3D4ep6A+HqegPqNT9D6jU/Q+MPjvPu1n7j7tZ+4+A4/ZPgOP2T4DLvA+Ay7wPkVM3D629NE+i0jhPoQPpD6ED6Q+Q3H5PkNx+T4lAvg+aP71Pmj+9T67K90+uyvdPiZU9T4mVPU+fULYPpK6zj6uV+c+GaqWPhmqlj4TQPg+E0D4Pj3N8j56OvQ+ejr0Plp24T5aduE+egn4PnoJ+D5b8Ls+pkzcPuvd9D7ArKU+wKylPlaM+j5WjPo+CmD3Ph4C+j4eAvo+tV/tPrVf7T7+PPk+/jz5PuZD0z6iUtU+OqrjPsdmoz7HZqM+gHrrPoB66z7eXuU+vdPgPr3T4D5yCtg+cgrYPn7m7z5+5u8+sA3dPjZi7j61HdQ+2CmwPtgpsD5ue+A+bnvgPiVGwT5Sw8I+UsPCPtMtqz7TLas+omTWPqJk1j4j060+Bs2xPrIKmj6yCpo+GFXePk1UhK3KeGM72OPjPtjj4z7Y4+M+ykeiPibMtD5O1tQ+zZ5iPs2eYj4W6+U+FuvlPuu/3D57juA+e47gPrPn0j6z59I+Ou3qPjrt6j61NtI+ZRfIPioL3D4ep6A+HqegPqNT9D6jU/Q+MPjvPu1n7j7tZ+4+A4/ZPgOP2T4DLvA+Ay7wPkVM3D629NE+i0jhPoQPpD6ED6Q+Q3H5PkNx+T4lAvg+aP71Pmj+9T67K90+uyvdPiZU9T4mVPU+fULYPpK6zj6uV+c+GaqWPhmqlj4TQPg+E0D4Pj3N8j56OvQ+ejr0Plp24T5aduE+egn4PnoJ+D5b8Ls+pkzcPuvd9D7ArKU+wKylPlaM+j5WjPo+CmD3Ph4C+j4eAvo+tV/tPrVf7T7+PPk+/jz5PuZD0z6iUtU+OqrjPsdmoz7HZqM+gHrrPoB66z7eXuU+vdPgPr3T4D5yCtg+cgrYPn7m7z5+5u8+sA3dPjZi7j61HdQ+2CmwPtgpsD5ue+A+bnvgPiVGwT5Sw8I+UsPCPtMtqz7TLas+omTWPqJk1j4j060+Bs2xPrIKmj6yCpo+njp2Pqs3XquSlxc7OnZ9Pjp2fT46dn0+FwQ2PpsQSj79wHM+rNEuPqzRLj5WvoE+Vr6BPv66dj51dXs+dXV7Pmq/aT5qv2k+l7SDPpe0gz7ojWo+VbduPn4ReD5dUWo+XVFqPvG+kT7xvpE+sEKQPrs6jT67Oo0+y/+CPsv/gj4Vno8+FZ6PPuP+hT5YCXo+V3eKPluxdj5bsXY+20KcPttCnD7N9po+NSmXPjUplz51KoU+dSqFPhx/mD4cf5g+yIeBPtqDeT4UcY4+vYhdPr2IXT6H5qQ+h+akPksvpT65eaM+uXmjPljQjz5Y0I8+vs6jPr7Ooz6ebIY+RsiEPkPTqD70uMo+9LjKPn8fwT5/H8E+sljOPu1m2z7tZts+GF/WPhhf1j7Hicw+x4nMPj4vuT7iGKs+I2PXPg40vT4ONL0+bVrXPm1a1z7KhOI+u7flPru35T4lQ+0+JUPtPt3+7j7d/u4+2m8IP4u3AD+KgQU/uPD6Prjw+j7S0wM/0tMDP/rxAD8yvQA/Mr0APzg6+j44Ovo+aHsNP2h7DT9DOwM/BlgFPwoT8z4KE/M+3pvxPumRQKzYbeM6Cyr3Pgsq9z4LKvc+BluVPqx4vT46xOk+FsNXPhbDVz5+f/s+fn/7PsvU9D57Ef8+exH/PhCy+D4Qsvg+0roCP9K6Aj8jqO4+iI7RPrWf9D7zfqE+836hPvV9BD/1fQQ/+m0FP1ZABj9WQAY/Zp//Pmaf/z5GswQ/RrMEP8IQAD+T1OI+o8H8PqLPnT6iz50+OlEFPzpRBT/8rwY/MIQHPzCEBz8P4wA/D+MAP1JyBT9ScgU/qhnZPg1V1T6+fwA/0KaIPtCmiD7quwE/6rsBP5Yk+D7KN/o+yjf6Pq2d7j6tne4+WzUCP1s1Aj/NFrA+4qjkPqmxCD8ReI8+EXiPPhPb/D4T2/w+5STtPgPy7j4D8u4+82zpPvNs6T5Ah/c+QIf3PvnGvD4GBcw+drrXPvUVkz71FZM+MgbkPjIG5D51rtQ+4gXNPuIFzT72JMg+9iTIPiWt5T4lreU+XubHPlIG3j6ae8s+bMSmPmzEpj5d/dU+Xf3VPor5tT7SA7Y+0gO2PsRqnj7Eap4+9XvJPvV7yT45V6k+GhGsPrKwlj6ysJY+XQ6NPj3CH61mb/s6o9OQPqPTkD6j05A+i/FYPn5XYT4H94Q+VHQePlR0Hj5fOZI+XzmSPv43ij52c4o+dnOKPtpBfz7aQX8+1sWTPtbFkz7drH4+pRiCPr5vjT4JpZo+CaWaPvDHpj7wx6Y+cDmfPvNMmj7zTJo+7liJPu5YiT5wS6E+cEuhPk6Klz5uSYk+yxqcPvKTpD7yk6Q+Xem3Pl3ptz7WPbA+k5SoPpOUqD7z/Iw+8/yMPrrLsD66y7A+nY+dPojEkT671Zo+sAZpPrAGaT5WK7s+Viu7PqNVuT5V9rc+Vfa3PsMXoj7DF6I+4wS6PuMEuj7UG58+tX+mPksKrT7ZS6I+2UuiPoMCyz6DAss+bkzOPoCo3j6AqN4+ndXePp3V3j5OEdc+ThHXPmRxwD7aC78+uqTnPiVyzT4lcs0+PljhPj5Y4T5qDO4+WqH1Plqh9T77JwA/+ycAP1DU8T5Q1PE+SHQBPwM08z4rbwA/TZvvPk2b7z5rU/w+a1P8Psr+6j5rfu0+a37tPquB2D6rgdg+VrsAP1a7AD+odfo+6QAAP3sR7D57Eew+Sw0bPjZKFK0mUUU7zesgPs3rID7N6yA+27/yPX1c6D3eUhI+OxEUPjsRFD4eniQ+Hp4kPnJhGD6TkRY+k5EWPpa8CD6WvAg+RPQlPkT0JT4d5BA+vlwTPv/3JT6blJM+m5STPgB/Vj4Af1Y+hh5EPsP4NT7D+DU+dpscPnabHD5KSUs+SklLPlbtTz5uby0+hgxXPlUZoD5VGaA+boGEPm6BhD4dTnQ+9nBePvZwXj6uMSo+rjEqPk1reD5Na3g++gtnPhaYSD4Mn0I+IPdXPiD3Vz5iZY8+YmWPPmpEkD7l2ow+5dqMPsvgcj7L4HI+eoWOPnqFjj6iJJA+lQeCPgIWdz62faU+tn2lPrRfqD60X6g+mdSvPr1lzT69Zc0+BWLbPgVi2z4tE8A+LRPAPtFkvj6dBbs+GxrsPpNN6z6TTes+fsPXPn7D1z4lmfA+Gs0CPxrNAj9r1w0/a9cNPwC17T4Ate0+sIoFPxYR5z6ubgg/Oj4HPzo+Bz9bTwA/W08AP2Cz8j77tv8++7b/PkV42z5FeNs+nDYEP5w2BD9fCgw/LksRPz4cBz8+HAc/Sw0bPjZKFK0mUUU7zesgPs3rID7N6yA+27/yPX1c6D3eUhI+OxEUPjsRFD4eniQ+Hp4kPnJhGD6TkRY+k5EWPpa8CD6WvAg+RPQlPkT0JT4d5BA+vlwTPv/3JT6blJM+m5STPgB/Vj4Af1Y+hh5EPsP4NT7D+DU+dpscPnabHD5KSUs+SklLPlbtTz5uby0+hgxXPlUZoD5VGaA+boGEPm6BhD4dTnQ+9nBePvZwXj6uMSo+rjEqPk1reD5Na3g++gtnPhaYSD4Mn0I+IPdXPiD3Vz5iZY8+YmWPPmpEkD7l2ow+5dqMPsvgcj7L4HI+eoWOPnqFjj6iJJA+lQeCPgIWdz62faU+tn2lPrRfqD60X6g+mdSvPr1lzT69Zc0+BWLbPgVi2z4tE8A+LRPAPtFkvj6dBbs+GxrsPpNN6z6TTes+fsPXPn7D1z4lmfA+Gs0CPxrNAj9r1w0/a9cNPwC17T4Ate0+sIoFPxYR5z6ubgg/Oj4HPzo+Bz9bTwA/W08AP2Cz8j77tv8++7b/PkV42z5FeNs+nDYEP5w2BD9fCgw/LksRPz4cBz8+HAc/poafPmigva3yw0A7Gn2kPhp9pD4afaQ+IJVrPppufT6TnJk+w3NNPsNzTT6qeqY+qnqmPqIznj4HVZ8+B1WfPg4TlD4OE5Q+IsqpPiLKqT6Xw5k+4xGRPhjXoD6214I+tteCPtvmtT7b5rU+WjawPkeJrD5Hiaw+TTmcPk05nD7e2rE+3tqxPkzCqD5KUpw+5vGsPjMOjj4zDo4+LkK/Pi5Cvz4ez7o+ry22Pq8ttj6ptp4+qbaePp72uT6e9rk+sJqlPhKXmj5JEKk+EE9+PhBPfj4+hcQ+PoXEPvhqwT6RtMA+kbTAPjW5rz41ua8+etzDPnrcwz7Vfp0+tDurPh9Rvj5W37Y+Vt+2PiMO1z4jDtc+XUDaPjUO4z41DuM+wuvgPsLr4D7uG+A+7hvgPg760D5Y4cY+m13fPi6ssz4urLM+GgfjPhoH4z5tf+I+KDDiPigw4j4qfOU+KnzlPsOj8j7Do/I+0Nj4Ptpa8z4m2vU+M+/aPjPv2j4VpPk+FaT5PrOY5T4lQuc+JULnPu661D7uutQ+e2z5Pnts+T5THug+rYrqPg0B2D4NAdg+poafPmigva3yw0A7Gn2kPhp9pD4afaQ+IJVrPppufT6TnJk+w3NNPsNzTT6qeqY+qnqmPqIznj4HVZ8+B1WfPg4TlD4OE5Q+IsqpPiLKqT6Xw5k+4xGRPhjXoD6214I+tteCPtvmtT7b5rU+WjawPkeJrD5Hiaw+TTmcPk05nD7e2rE+3tqxPkzCqD5KUpw+5vGsPjMOjj4zDo4+LkK/Pi5Cvz4ez7o+ry22Pq8ttj6ptp4+qbaePp72uT6e9rk+sJqlPhKXmj5JEKk+EE9+PhBPfj4+hcQ+PoXEPvhqwT6RtMA+kbTAPjW5rz41ua8+etzDPnrcwz7Vfp0+tDurPh9Rvj5W37Y+Vt+2PiMO1z4jDtc+XUDaPjUO4z41DuM+wuvgPsLr4D7uG+A+7hvgPg760D5Y4cY+m13fPi6ssz4urLM+GgfjPhoH4z5tf+I+KDDiPigw4j4qfOU+KnzlPsOj8j7Do/I+0Nj4Ptpa8z4m2vU+M+/aPjPv2j4VpPk+FaT5PrOY5T4lQuc+JULnPu661D7uutQ+e2z5Pnts+T5THug+rYrqPg0B2D4NAdg+ygZqPlIOEqzT1kw7FhpzPhYacz4WGnM+SzkyPrvbRD46E24+9tQ7PvbUOz5Qx3c+UMd3PmsKbD6dgnI+nYJyPqpaZD6qWmQ+oOJ/PqDifz6Vw2w+w8dqPo3seD4lKCo+JSgqPjJxhz4ycYc+oEqFPnsqhD57KoQ+3nZxPt52cT7eQYU+3kGFPoazej5K+3A+yOOAPiJhQT4iYUE+MJuMPjCbjD62Eow+G+WKPhvlij7IknM+yJJzPlARiT5QEYk+qO50PgpKXz70IYQ+yLVjPsi1Yz4wGpY+MBqWPk/Ylz6RbpY+kW6WPpR1iT6UdYk+2SCWPtkglj7N8WU+Ncl6PhmzmT7eDrE+3g6xPiZorT4maK0+Mo+6Ptf/vj7X/74+UvC7PlLwuz6+07U+vtO1PsFKsT6Ld6U+7rSzPvvekz773pM+rli4Pq5YuD4x97c+Moy2PjKMtj4LPro+Cz66PqVUyD6lVMg+Db/ZPq6v0z7GqNI+phfAPqYXwD7DJ9U+wyfVPnYWyD4F3so+Bd7KPm2Axz5tgMc+9hrjPvYa4z7K7cY+gnjLPohMtT6ITLU+9jBoPuasu6wJxEc786dxPvOncT7zp3E+ZmcjPlcFND7lJ2Y+3VY7Pt1WOz4bU3c+G1N3PqA+aT7Sxm4+0sZuPs1bYD7NW2A+G9F+PhvRfj6oW2w+/v9mPvL2cD5qITg+aiE4Pka1hz5GtYc+s8iDPsxWgT7MVoE+Hq9rPh6vaz5rx4Q+a8eEPrsEfj5KCm0+6HqBPjKITT4yiE0+gwKOPoMCjj7lcIs+FKWIPhSliD6uDG4+rgxuPq3biT6t24k+5mxzPiWVXD5Gin0+Vc1JPlXNST7CW5Q+wluUPv+skz5b+pE+W/qRPnrxhD568YQ+giSUPoIklD4qcmE+uCp6PjO0lT4WUqU+FlKlPjvbpz4726c+oVGxPhm8tT4ZvLU+U2m0PlNptD7GvK8+xryvPtJIqj7bFZ8+9OCsPpN5iz6TeYs+9rSxPva0sT7scK8+ut+rPrrfqz5fJq8+XyavPieLwD4ni8A+XSjOPkIPxj7WY80+bZ66Pm2euj52xc0+dsXNPhNPwj7eXcE+3l3BPho6vj4aOr4+RubcPkbm3D50374+MvTDPopQrj6KUK4+9jBoPuasu6wJxEc786dxPvOncT7zp3E+ZmcjPlcFND7lJ2Y+3VY7Pt1WOz4bU3c+G1N3PqA+aT7Sxm4+0sZuPs1bYD7NW2A+G9F+PhvRfj6oW2w+/v9mPvL2cD5qITg+aiE4Pka1hz5GtYc+s8iDPsxWgT7MVoE+Hq9rPh6vaz5rx4Q+a8eEPrsEfj5KCm0+6HqBPjKITT4yiE0+gwKOPoMCjj7lcIs+FKWIPhSliD6uDG4+rgxuPq3biT6t24k+5mxzPiWVXD5Gin0+Vc1JPlXNST7CW5Q+wluUPv+skz5b+pE+W/qRPnrxhD568YQ+giSUPoIklD4qcmE+uCp6PjO0lT4WUqU+FlKlPjvbpz4726c+oVGxPhm8tT4ZvLU+U2m0PlNptD7GvK8+xryvPtJIqj7bFZ8+9OCsPpN5iz6TeYs+9rSxPva0sT7scK8+ut+rPrrfqz5fJq8+XyavPieLwD4ni8A+XSjOPkIPxj7WY80+bZ66Pm2euj52xc0+dsXNPhNPwj7eXcE+3l3BPho6vj4aOr4+RubcPkbm3D50374+MvTDPopQrj6KUK4+J4pBPjujTC0yK4U7ctVKPnLVSj5y1Uo+LvQNPpbdIz5LWU0+entLPnp7Sz4zVFI+M1RSPr0yRj6NUU4+jVFOPjLDQj4yw0I+TZdZPk2XWT6YAUs+6a1KPgjzUD59CgQ+fQoEPrLbZD6y22Q+llZiPo0FYz6NBWM+JolTPiaJUz6yfmM+sn5jPlOTTz6gD1E+GrhXPrF2Ez6xdhM+c/xtPnP8bT7+f24+AsBuPgLAbj5S3FQ+UtxUPsDCaT7Awmk+p/NKPv5RPj6iF18+QwpNPkMKTT4oo38+KKN/PkKJgD52oX0+dqF9Po0iZz6NImc+tdp+PrXafj4YIT0+ROBAPnJYhj6aebI+mnmyPrrymD668pg++kSnPgbVqj4G1ao+zF6rPsxeqz4676A+Ou+gPh9gmz4BiZs+zbOiPvMWij7zFoo+0h6kPtIepD6SlKE+jreePo63nj5ampc+WpqXPq70rj6u9K4++/O9Pta0uT5k5rU+ZgOmPmYDpj6MzLc+jMy3PsxKrj7yk7Q+8pO0PgHMtT4BzLU+fHbIPnx2yD44HZ4+L/GjPjRFjD40RYw+J4pBPjujTC0yK4U7ctVKPnLVSj5y1Uo+LvQNPpbdIz5LWU0+entLPnp7Sz4zVFI+M1RSPr0yRj6NUU4+jVFOPjLDQj4yw0I+TZdZPk2XWT6YAUs+6a1KPgjzUD59CgQ+fQoEPrLbZD6y22Q+llZiPo0FYz6NBWM+JolTPiaJUz6yfmM+sn5jPlOTTz6gD1E+GrhXPrF2Ez6xdhM+c/xtPnP8bT7+f24+AsBuPgLAbj5S3FQ+UtxUPsDCaT7Awmk+p/NKPv5RPj6iF18+QwpNPkMKTT4oo38+KKN/PkKJgD52oX0+dqF9Po0iZz6NImc+tdp+PrXafj4YIT0+ROBAPnJYhj6aebI+mnmyPrrymD668pg++kSnPgbVqj4G1ao+zF6rPsxeqz4676A+Ou+gPh9gmz4BiZs+zbOiPvMWij7zFoo+0h6kPtIepD6SlKE+jreePo63nj5ampc+WpqXPq70rj6u9K4++/O9Pta0uT5k5rU+ZgOmPmYDpj6MzLc+jMy3PsxKrj7yk7Q+8pO0PgHMtT4BzLU+fHbIPnx2yD44HZ4+L/GjPjRFjD40RYw+PqOBPg9OpK2d+Ps66oWGPuqFhj7qhYY+4oo9Pr5MSz5NmH4+XiE/Pl4hPz5j0Yg+Y9GIPkYfgT5rn4I+a5+CPkh1cj5IdXI+of6LPqH+iz40e4A+2NV5PsIbhD47hFs+O4RbPs2xlj7NsZY+2kqSPs76jj7O+o4+gXqBPoF6gT7dVJM+3VSTPiDVjD7DmoE+BR6QPiKObz4ijm8+zQmfPs0Jnz4CK5s+WnuXPlp7lz7DJ4M+wyeDPtUnmj7VJ5o+zGmIPhrSeD5/1oo+Zt9gPmbfYD6+e6U+vnulPm2Moz5kUaI+ZFGiPj87lD4/O5Q+vXelPr13pT5aXII+d7iMPq1apD6WfbI+ln2yPqXBuj6lwbo+vtvBPvwLyz78C8s+YRnLPmEZyz6KpcU+iqXFPpuavD6+WrQ+mC3JPu5gpz7uYKc+5f7IPuX+yD4Ipsg+tTPKPrUzyj5TeM0+U3jNPsDp2D7A6dg+cvjlPpaD3D5m0OM+YE/SPmBP0j4b2OM+G9jjPvs21j6NlNk+jZTZPk5rzz5Oa88+vcvwPr3L8D7ABdQ+TXPbPs93wz7Pd8M+PqOBPg9OpK2d+Ps66oWGPuqFhj7qhYY+4oo9Pr5MSz5NmH4+XiE/Pl4hPz5j0Yg+Y9GIPkYfgT5rn4I+a5+CPkh1cj5IdXI+of6LPqH+iz40e4A+2NV5PsIbhD47hFs+O4RbPs2xlj7NsZY+2kqSPs76jj7O+o4+gXqBPoF6gT7dVJM+3VSTPiDVjD7DmoE+BR6QPiKObz4ijm8+zQmfPs0Jnz4CK5s+WnuXPlp7lz7DJ4M+wyeDPtUnmj7VJ5o+zGmIPhrSeD5/1oo+Zt9gPmbfYD6+e6U+vnulPm2Moz5kUaI+ZFGiPj87lD4/O5Q+vXelPr13pT5aXII+d7iMPq1apD6WfbI+ln2yPqXBuj6lwbo+vtvBPvwLyz78C8s+YRnLPmEZyz6KpcU+iqXFPpuavD6+WrQ+mC3JPu5gpz7uYKc+5f7IPuX+yD4Ipsg+tTPKPrUzyj5TeM0+U3jNPsDp2D7A6dg+cvjlPpaD3D5m0OM+YE/SPmBP0j4b2OM+G9jjPvs21j6NlNk+jZTZPk5rzz5Oa88+vcvwPr3L8D7ABdQ+TXPbPs93wz7Pd8M+U6LaPbLvgapCrGw69n/hPfZ/4T32f+E9XWmtPfERoT3q3dA9cPS+PXD0vj1S+uI9UvriPX4q1D0DqM09A6jNPRL7tz0S+7c9ukzkPbpM5D2e78c99jjRPYta6j22YTs+tmE7PqZzFT6mcxU+YjcJPl7n/T1e5/092p3bPdqd2z0YTg4+GE4OPrbZCT5+Xfc9+mgfPvg2VD74NlQ+7U00Pu1NND6/yiU+lpkXPpaZFz5bgOM9W4DjPQuNKD4LjSg+SAMRPspeCz5FdAQ+vEoIPrxKCD42m0U+NptFPpLqQj4ONT0+DjU9PjpvHD46bxw++dJAPvnSQD79Dz4+pSMkPlKVKj7ZiJ4+2YiePtrNfz7azX8+uVmHPqb6oD6m+qA+m8avPpvGrz4NdJc+DXSXPvx3oD5Px4k+sl7CPnBbxj5wW8Y+EVe2PhFXtj6YhMs+wpvePsKb3j7/EAE//xABPyDk1z4g5Nc+rCQIP4ro2z7CeQs/ezgLP3s4Cz873gI/O94CP0JcBD9GHAg/RhwIP2p+/D5qfvw+D1MNPw9TDT/19CU/MGwmP43iIj+N4iI/LljaPaVeraqw9UE6M5DhPTOQ4T0zkOE9DhipPZYYnj2oeNI9ClnJPQpZyT1Q4uM9UOLjPYYI1D2wb849sG/OPR03uD0dN7g9nbLlPZ2y5T0QCs09OIHYPdBW7D0QMVY+EDFWPv1gGz79YBs+bsQPPo00BD6NNAQ+OiLnPToi5z06GRQ+OhkUPp/1ET6+FwA+AHkoPlZSbD5WUmw+5S8/PuUvPz69iC8+lgIgPpYCID5COfA9QjnwPeZgMj7mYDI+07EXPrSsEj72hAg+sv0VPrL9FT4IKlI+CCpSPihzTj61iUc+tYlHPhsTJD4bEyQ+ACtNPgArTT7ejEw+5qUqPmmSOD6Ktao+irWqPuTmhj7k5oY+aRyPPgjbrD4I26w+/2S8Pv9kvD6KU6A+ilOgPhdYqT5L8pQ+YtDQPl612j5etdo+VvW9Plb1vT6SNdQ+w4nrPsOJ6z7j7Qc/4+0HPzMX3j4zF94+RQgMP+A34z4lpw8/57MTP+ezEz8FEwU/BRMFP7BNBz9ZSww/WUsMPz85Aj8/OQI/hvETP4bxEz8CdSM/JKgoP8pZHz/KWR8/fafRPUzjNalDHmI6EPTXPRD01z0Q9Nc90sCmPXNmnD3nysY9FoutPRaLrT3GTNk9xkzZPUXFyz0rycQ9K8nEPb4UsD2+FLA9+PnZPfj52T1dQ7w9CtbDPU0V4D2QSjY+kEo2PhKeDz4Sng8+9U8DPjK08j0ytPI9sgXSPbIF0j144gg+eOIIPjoyBD6NXe49jjkZPkN+Tj5Dfk4+QzguPkM4Lj4OICA+LhQSPi4UEj6wFds9sBXbPU4LIz5OCyM+x9YMPmEnBz6wNQA+nJcBPpyXAT4Yfz4+GH8+PorpOz4QBzc+EAc3Pt0qFz7dKhc+ELM5PhCzOT52xDg+8V0gPvIOIj76WJc++liXPqOQdj6jkHY+kumBPiwzmz4sM5s+MgWqPjIFqj52LpI+di6SPtskmj5VpIM+63y8Ptz0wD7c9MA+kWmxPpFpsT6qrsY+5bTZPuW02T5epf0+XqX9PkCe0j5AntI+m5MEP3Z81j6SKgg/CicHPwonBz99m/8+fZv/PhbFAD+GVwQ/hlcEPyPI8z4jyPM+j10IP49dCD84jSQ/mwEkPyhRIj8oUSI/fafRPUzjNalDHmI6EPTXPRD01z0Q9Nc90sCmPXNmnD3nysY9FoutPRaLrT3GTNk9xkzZPUXFyz0rycQ9K8nEPb4UsD2+FLA9+PnZPfj52T1dQ7w9CtbDPU0V4D2QSjY+kEo2PhKeDz4Sng8+9U8DPjK08j0ytPI9sgXSPbIF0j144gg+eOIIPjoyBD6NXe49jjkZPkN+Tj5Dfk4+QzguPkM4Lj4OICA+LhQSPi4UEj6wFds9sBXbPU4LIz5OCyM+x9YMPmEnBz6wNQA+nJcBPpyXAT4Yfz4+GH8+PorpOz4QBzc+EAc3Pt0qFz7dKhc+ELM5PhCzOT52xDg+8V0gPvIOIj76WJc++liXPqOQdj6jkHY+kumBPiwzmz4sM5s+MgWqPjIFqj52LpI+di6SPtskmj5VpIM+63y8Ptz0wD7c9MA+kWmxPpFpsT6qrsY+5bTZPuW02T5epf0+XqX9PkCe0j5AntI+m5MEP3Z81j6SKgg/CicHPwonBz99m/8+fZv/PhbFAD+GVwQ/hlcEPyPI8z4jyPM+j10IP49dCD84jSQ/mwEkPyhRIj8oUSI/",
          "dtype": "f4",
          "shape": "108, 108"
         }
        }
       ],
       "layout": {
        "margin": {
         "b": 10,
         "l": 10,
         "r": 10,
         "t": 30
        },
        "paper_bgcolor": "rgba(255,255,255,255)",
        "plot_bgcolor": "rgba(255,255,255,255)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "automargin": true,
         "pad": {
          "t": 5
         },
         "text": "w3907soi",
         "x": 0.5,
         "xref": "paper",
         "yanchor": "top",
         "yref": "container"
        },
        "xaxis": {
         "showticklabels": false
        },
        "yaxis": {
         "scaleanchor": "x",
         "scaleratio": 1,
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/5 Computing RSA w/ CKA: 100%|██████████| 11664/11664 [00:33<00:00, 705.13it/s]"
     ]
    }
   ],
   "source": [
    "def custom_hover_text(x, y, value):\n",
    "    return f\"CKA: {value:.3}<br>Base: {list(base_activations.keys())[x]}<br>Pretrained: {list(pretrained_activations.keys())[y]}<br>({x}, {y})\"\n",
    "\n",
    "\n",
    "nrows, ncols = sim_matrix.shape\n",
    "customdata = np.empty((nrows, ncols), dtype=object)\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        customdata[i, j] = custom_hover_text(i, j, sim_matrix[i, j])\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=sim_matrix,\n",
    "        colorscale=\"Viridis\",\n",
    "        customdata=customdata,\n",
    "        hovertemplate=\"%{customdata}<extra></extra>\",\n",
    "        showscale=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showticklabels=False),\n",
    "    yaxis=dict(showticklabels=False, scaleanchor=\"x\", scaleratio=1),\n",
    "    plot_bgcolor=\"rgba(255,255,255,255)\",\n",
    "    paper_bgcolor=\"rgba(255,255,255,255)\",\n",
    "    margin=dict(t=30, b=10, l=10, r=10),\n",
    "    title=dict(\n",
    "        text=best_model.ID,\n",
    "        xref=\"paper\",\n",
    "        yref=\"container\",\n",
    "        yanchor=\"top\",\n",
    "        x=0.5,\n",
    "        automargin=True,\n",
    "        pad=dict(t=5),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malbert-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
