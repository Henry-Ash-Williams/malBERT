{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5cbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5,6,7\"\n",
    "\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f644e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 9230196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 2291009\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample['text']\n",
    "    labels = sample['label']\n",
    "    \n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized['input_ids'])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened['label'].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../MalBERTa\")\n",
    "dataset = datasets.load_from_disk(\"../data/raw\")\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n",
    "\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de0acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset_size = 0.1\n",
    "# processed_dataset['test'] = processed_dataset['test'].shuffle().select(range(int(len(processed_dataset['test']) * subset_size)))\n",
    "# processed_dataset['train'] = processed_dataset['train'].shuffle().select(range(int(len(processed_dataset['train']) * subset_size)))\n",
    "\n",
    "# processed_dataset\n",
    "(len(processed_dataset['train']) // 256) // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bcda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MalBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlainon\u001b[0m (\u001b[33mhenry-williams\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/hw452/programming/MalBERT/wandb/run-20250526_105258-ex6mf30c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/henry-williams/huggingface/runs/ex6mf30c' target=\"_blank\">./MalBERTa-classifier</a></strong> to <a href='https://wandb.ai/henry-williams/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/henry-williams/huggingface' target=\"_blank\">https://wandb.ai/henry-williams/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/henry-williams/huggingface/runs/ex6mf30c' target=\"_blank\">https://wandb.ai/henry-williams/huggingface/runs/ex6mf30c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12019' max='12019' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12019/12019 37:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3605</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.370647</td>\n",
       "      <td>0.825373</td>\n",
       "      <td>0.837079</td>\n",
       "      <td>0.825373</td>\n",
       "      <td>0.821213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7210</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.362221</td>\n",
       "      <td>0.830187</td>\n",
       "      <td>0.844075</td>\n",
       "      <td>0.830187</td>\n",
       "      <td>0.825777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10815</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.362119</td>\n",
       "      <td>0.829934</td>\n",
       "      <td>0.845364</td>\n",
       "      <td>0.829934</td>\n",
       "      <td>0.825212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12019, training_loss=0.350899040227881, metrics={'train_runtime': 2283.5552, 'train_samples_per_second': 4042.029, 'train_steps_per_second': 5.263, 'total_flos': 1875488398783488.0, 'train_loss': 0.350899040227881, 'epoch': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./MalBERTa\")\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=256, \n",
    "    per_device_eval_batch_size=512, \n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=(len(processed_dataset['train']) // 256) // 10,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
