{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b548b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrywilliams/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-rusreqta:v1, 87.83MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import datasets \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "from os import PathLike\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('henry-williams/opcode-malberta/model-rusreqta:v1', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('mps')\n",
    "dataset = datasets.load_from_disk('../data/raw')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(artifact_dir)\n",
    "model = RobertaForSequenceClassification.from_pretrained(artifact_dir).to(device)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset['test'] = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b645b103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0050, -0.0322, -0.0146,  ...,  0.0288,  0.0094,  0.0116],\n",
       "         [ 0.0111, -0.0075,  0.0030,  ...,  0.0265, -0.0135, -0.0014],\n",
       "         [-0.0062, -0.0173, -0.0111,  ..., -0.0237,  0.0068, -0.0183],\n",
       "         ...,\n",
       "         [-0.0072, -0.0065,  0.0043,  ...,  0.0236, -0.0300,  0.0026],\n",
       "         [-0.0082,  0.0470,  0.0351,  ...,  0.0017,  0.0049, -0.0017],\n",
       "         [ 0.0099, -0.0305, -0.0337,  ..., -0.0075,  0.0059, -0.0183]],\n",
       "        device='mps:0'),\n",
       " Parameter containing:\n",
       " tensor([-3.5597e-03, -2.5868e-03,  1.0728e-03, -2.7034e-03, -3.6080e-03,\n",
       "          3.0296e-03, -4.0349e-03, -3.5384e-03, -2.3419e-03, -3.5672e-03,\n",
       "          2.2834e-03, -9.4198e-04,  2.4381e-03,  3.2620e-03, -1.9623e-03,\n",
       "          3.8463e-04,  1.6879e-03, -2.9276e-03, -1.0824e-04,  3.3817e-03,\n",
       "          1.0290e-03, -4.6376e-04,  2.0491e-03, -9.3507e-04,  1.2426e-03,\n",
       "          8.8885e-04, -5.2108e-04,  3.9557e-03, -2.1033e-03,  3.6117e-03,\n",
       "          1.2645e-03, -2.5963e-03, -5.1908e-03,  1.3568e-03, -1.1574e-04,\n",
       "          4.8547e-03, -2.8265e-03,  1.4102e-03,  8.8645e-04, -1.7716e-03,\n",
       "          9.6925e-04, -3.1486e-03,  1.6859e-03,  4.9021e-04,  2.1404e-03,\n",
       "         -5.2602e-04, -3.3172e-03,  4.9055e-03, -6.3884e-04, -2.1711e-03,\n",
       "          2.5790e-03,  4.3568e-03,  7.2190e-04,  2.1348e-03,  2.4260e-03,\n",
       "         -1.1447e-03, -2.6716e-03,  6.6483e-04, -3.9444e-03, -5.0313e-03,\n",
       "         -1.7309e-03,  2.8514e-05, -2.1022e-03, -2.0751e-03, -1.2220e-03,\n",
       "          2.3279e-03, -1.8668e-03,  3.8905e-03,  1.2619e-03,  1.6597e-03,\n",
       "          1.8155e-04, -6.3431e-05,  8.3546e-04,  8.1754e-04, -2.0399e-03,\n",
       "          1.3691e-03,  3.1378e-03, -7.4728e-04, -7.2223e-04,  1.0723e-03,\n",
       "          2.8918e-03, -1.7749e-03,  2.7865e-03, -7.1533e-04,  1.2345e-04,\n",
       "          1.5082e-03, -4.3708e-03, -1.6101e-03,  6.5420e-04, -1.2407e-03,\n",
       "          2.0773e-03,  2.2785e-03, -1.2719e-04,  1.5002e-03, -1.8848e-03,\n",
       "          3.4835e-03, -4.6064e-04, -3.1663e-03, -1.0244e-03, -8.7165e-04,\n",
       "         -9.8608e-04,  1.4271e-03,  9.7793e-04,  6.5814e-04, -9.8656e-05,\n",
       "          1.5222e-03, -3.1857e-03, -3.0497e-03, -3.4802e-04,  1.4649e-03,\n",
       "          1.7521e-03,  7.4762e-04, -9.3880e-04,  2.7946e-03, -2.2803e-03,\n",
       "          9.9746e-04,  3.4254e-03, -3.2914e-03, -1.7214e-03, -3.3314e-03,\n",
       "         -1.7131e-03, -1.8388e-03, -1.8084e-03,  6.3245e-03,  1.2921e-03,\n",
       "         -8.0847e-04, -2.0356e-03, -1.8650e-03, -3.2006e-03,  1.2233e-03,\n",
       "         -4.7245e-04,  8.8719e-04, -8.4961e-04,  5.0071e-03, -1.6988e-03,\n",
       "          2.4312e-03, -1.4627e-03,  1.5867e-03, -3.3873e-04, -1.7088e-04,\n",
       "         -5.4083e-03, -4.0805e-03, -2.6278e-03,  2.1619e-03, -1.2066e-03,\n",
       "          5.5148e-03,  3.5668e-03,  2.4624e-03,  1.7189e-03, -4.8573e-03,\n",
       "          3.4926e-03,  4.5173e-04,  2.1790e-03,  8.2681e-05, -6.7081e-04,\n",
       "          1.1880e-03, -2.8440e-03, -5.8546e-04,  1.1711e-03,  1.5133e-03,\n",
       "         -8.5416e-04, -1.0004e-03, -2.8955e-03,  3.7111e-04,  7.6240e-04,\n",
       "          3.3360e-03, -1.7309e-03, -1.4591e-03, -1.5538e-03, -5.5721e-04,\n",
       "          3.8796e-03,  8.3251e-04,  2.9657e-03, -6.9829e-03, -2.0559e-03,\n",
       "          1.5439e-03, -1.6852e-03,  3.6237e-03, -2.0545e-03, -1.2950e-03,\n",
       "         -5.9487e-03, -3.8374e-03, -5.9436e-03,  1.2448e-03, -4.4688e-03,\n",
       "          3.6668e-03, -4.0665e-03,  2.2230e-03, -1.0192e-03, -9.7226e-04,\n",
       "         -1.3452e-03, -8.4899e-04, -1.8367e-03,  3.9304e-03,  1.5999e-04,\n",
       "         -1.2545e-03,  2.6593e-03,  1.2590e-03,  2.4121e-03, -6.5026e-03,\n",
       "          7.0101e-04, -2.1578e-04,  9.2270e-04,  5.9074e-04, -7.4938e-04,\n",
       "         -2.3182e-03,  3.2982e-03, -7.3149e-04, -1.0259e-03,  1.2287e-03,\n",
       "         -3.3817e-03,  2.5931e-03,  1.3317e-03, -2.5979e-04,  1.8427e-03,\n",
       "          3.9180e-03, -1.7768e-03,  3.0609e-04,  9.8881e-04, -9.8232e-05,\n",
       "         -2.6011e-03,  3.6573e-03, -1.5394e-03,  3.2309e-03, -3.8801e-03,\n",
       "         -1.4443e-03, -2.3680e-03,  2.7161e-03,  3.8226e-03, -1.7372e-03,\n",
       "          7.4686e-04,  2.7243e-03, -5.3958e-03,  1.4158e-03,  4.7504e-03,\n",
       "          9.3492e-04, -4.5401e-03, -2.6994e-03,  7.0278e-04, -5.3078e-03,\n",
       "         -1.5535e-03,  2.1641e-03, -1.4807e-03, -1.5410e-03,  5.0295e-03,\n",
       "          9.8293e-06, -4.0564e-03, -6.9594e-04,  4.7714e-03,  1.2550e-05,\n",
       "         -3.2313e-03,  8.6281e-04,  7.9379e-04, -1.9044e-03, -1.4603e-03,\n",
       "          2.3715e-03,  3.0223e-03,  7.0784e-03,  1.1083e-03, -2.9504e-04,\n",
       "         -3.0111e-04, -9.1975e-04, -1.5975e-03,  2.3245e-04, -1.0862e-03,\n",
       "          1.8416e-04,  3.4813e-03, -1.3481e-03,  1.8268e-03, -1.0419e-03,\n",
       "          1.2278e-03, -3.1678e-04, -2.9226e-04, -5.3847e-04,  1.7360e-03,\n",
       "         -1.9409e-03, -4.6801e-03,  1.9283e-03, -9.1684e-04, -6.2130e-04,\n",
       "         -4.7283e-03,  1.2968e-03, -1.3526e-03, -2.9150e-03,  1.2730e-03,\n",
       "          8.4725e-04, -1.3671e-03,  1.3619e-03,  5.6432e-03,  3.1586e-03,\n",
       "         -6.3260e-04,  8.9092e-04, -1.8954e-03, -1.9381e-03,  3.6251e-03,\n",
       "         -1.9501e-03,  1.6153e-03, -2.4481e-04,  1.9098e-03,  3.6442e-04,\n",
       "          7.9902e-04, -1.7915e-03,  7.1792e-04, -1.9700e-03,  5.8911e-03,\n",
       "          2.0689e-04,  3.3050e-03, -8.3850e-04,  2.4639e-03, -2.9146e-03,\n",
       "          6.0343e-03,  5.0741e-04, -1.9422e-03, -1.2823e-03,  3.2593e-03,\n",
       "          2.0527e-03,  2.3313e-03, -1.2720e-03,  4.2571e-03,  2.5941e-03,\n",
       "          1.4032e-03, -1.9832e-03,  1.6801e-03,  2.1037e-03,  3.4228e-04,\n",
       "          3.1908e-03,  2.3067e-03,  3.6813e-03, -2.0154e-03,  1.7117e-03,\n",
       "          2.9123e-04,  1.3179e-03,  2.4279e-04,  1.1522e-03, -4.3187e-03,\n",
       "         -1.2259e-03,  1.2461e-03, -5.0950e-03, -9.9122e-04,  4.8876e-03,\n",
       "          1.2949e-03,  1.4822e-03, -3.4244e-03,  1.9764e-04, -5.7648e-03,\n",
       "          1.4425e-03,  1.4745e-03,  1.4750e-03,  9.3990e-04,  2.3544e-03,\n",
       "          1.1249e-03, -2.5919e-03, -1.0778e-03,  1.3716e-03,  6.8110e-03,\n",
       "          5.0701e-05,  9.2308e-04,  2.2894e-03, -3.1900e-03,  2.6182e-03,\n",
       "         -3.8070e-03, -5.1398e-03,  1.6703e-03, -2.7737e-03,  1.9041e-03,\n",
       "          1.1725e-04, -5.4126e-04,  2.6489e-03, -6.0535e-04, -2.2282e-03,\n",
       "          2.6871e-03, -3.6071e-04,  8.8767e-04,  1.7040e-03, -1.4458e-03,\n",
       "         -2.0334e-03,  3.0745e-03, -2.6047e-05, -4.6490e-03,  3.0774e-03,\n",
       "          2.2270e-03, -9.6285e-04,  8.5474e-04, -2.3104e-03, -8.5045e-04,\n",
       "         -2.6399e-03,  5.0915e-03, -1.5347e-03,  2.7179e-03,  6.7154e-03,\n",
       "         -2.9702e-04, -1.1914e-03,  2.7174e-03,  1.9913e-03,  2.5050e-04,\n",
       "         -1.3232e-03,  1.4236e-03,  1.4573e-03, -2.8078e-04,  2.1320e-03,\n",
       "          1.9698e-03, -2.4580e-03, -3.1217e-03,  1.6611e-03,  1.8174e-03,\n",
       "          3.5141e-03, -7.2856e-04, -2.1010e-04, -1.9005e-03,  1.2958e-03,\n",
       "         -2.4676e-03, -5.2126e-04, -3.5857e-04, -3.7940e-03, -4.0428e-03,\n",
       "         -8.7377e-04,  7.8860e-04,  9.7929e-04,  1.4394e-04, -8.2453e-04,\n",
       "          2.2072e-03,  4.1253e-03,  3.3988e-03,  1.7642e-03, -3.1378e-03,\n",
       "         -2.8531e-05, -1.5099e-03,  2.1562e-03,  1.0257e-03,  1.5159e-03,\n",
       "         -1.8731e-03, -6.3544e-04,  1.3357e-03,  3.7392e-03,  1.1279e-03,\n",
       "          3.6332e-03,  2.0735e-03,  2.9481e-04,  2.0249e-03, -3.0460e-03,\n",
       "          6.6369e-03, -1.4003e-03, -5.3918e-03, -1.3348e-04, -1.4818e-03,\n",
       "         -3.5432e-03,  3.3797e-04,  3.8619e-03,  1.3698e-04, -1.5717e-03,\n",
       "          1.8399e-03,  1.6744e-03, -7.4653e-03,  2.3718e-03,  3.4895e-03,\n",
       "          2.1372e-03, -8.1324e-04, -6.7315e-03,  1.9574e-03,  3.9888e-03,\n",
       "         -2.9981e-03, -6.4957e-05,  1.6080e-03,  6.0734e-06, -6.4654e-04,\n",
       "          7.8611e-04,  6.4407e-03,  1.1023e-03,  1.8429e-03,  8.0414e-04,\n",
       "         -2.0222e-03, -2.9027e-03,  1.2907e-03,  3.4661e-03,  6.3013e-03,\n",
       "          4.4467e-04, -1.6150e-03, -1.0088e-03, -2.6218e-04, -2.2369e-03,\n",
       "          9.5904e-04,  1.7353e-03, -2.1888e-04, -7.6788e-04, -1.4694e-03,\n",
       "         -2.5772e-03, -2.1659e-03, -1.9696e-03,  4.9150e-03, -1.7752e-03,\n",
       "          2.8142e-03,  1.2301e-03,  8.8377e-04, -2.2175e-03,  2.4472e-03,\n",
       "          2.8650e-03, -7.7405e-04, -3.8794e-03,  3.5248e-03,  2.1007e-03,\n",
       "         -4.3987e-03, -8.4784e-04,  1.2900e-03, -4.7741e-03, -2.5541e-03,\n",
       "          5.0069e-03,  6.3032e-04, -3.8285e-04, -2.2592e-03, -1.4472e-03,\n",
       "         -1.4660e-03,  7.4320e-03], device='mps:0'),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0005, -0.0186, -0.0086,  ..., -0.0120,  0.0020,  0.0032],\n",
       "         [ 0.0212,  0.0042, -0.0093,  ...,  0.0042,  0.0023, -0.0094]],\n",
       "        device='mps:0'),\n",
       " Parameter containing:\n",
       " tensor([ 0.0026, -0.0026], device='mps:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fa925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(sample):\n",
    "    input = tokenizer(\n",
    "        sample,\n",
    "        padding='max_length',\n",
    "        max_length=32,\n",
    "        return_overflowing_tokens=True,\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    input_ids = torch.tensor(input['input_ids'])\n",
    "    attention_mask = torch.tensor(input['attention_mask'])\n",
    "    full_logits = []\n",
    "\n",
    "    for ids, mask in zip(input_ids.split(BATCH_SIZE), attention_mask.split(BATCH_SIZE)):\n",
    "        torch.mps.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids.to(device), mask.to(device)).logits\n",
    "        full_logits.append(logits)\n",
    "\n",
    "    logits = torch.vstack(full_logits)\n",
    "    return logits\n",
    "\n",
    "def make_ds():\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset['test'], position=0)):\n",
    "        logits = make_preds(sample['text']).unsqueeze(0)\n",
    "        \n",
    "        predictions.append(logits)\n",
    "        actuals.append(sample['label'])\n",
    "    return predictions, actuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66275794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionsDataset(Dataset):\n",
    "    def __init__(self, preds, labels):\n",
    "        if len(preds) != len(labels):\n",
    "            raise ValueError(\"Mismatch in size between x and y\")\n",
    "\n",
    "        self.preds = preds \n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.preds[index], self.labels[index]\n",
    "\n",
    "if not os.path.exists('logits-ds.pt'):\n",
    "    predictions, actuals = make_ds()\n",
    "    d = PredictionsDataset(predictions, actuals)\n",
    "    torch.save(d, 'logits-ds.pt')\n",
    "else: \n",
    "    d = torch.load('logits-ds.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reducer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Reducer, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        z = self.fc(out[:, -1, :])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reducer(2, 32, 3)\n",
    "r = r.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(r.parameters())\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for logits, label in tqdm(d):\n",
    "    torch.mps.empty_cache()\n",
    "    actual = F.one_hot(torch.tensor(torch.tensor(label)), 2).unsqueeze(0).to(torch.float32).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    predicted = r(logits)\n",
    "    loss = criterion(predicted, actual)\n",
    "    loss_history.append(loss)\n",
    "    loss.backward() \n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(loss_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
