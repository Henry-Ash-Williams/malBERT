{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b548b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrywilliams/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-rusreqta:v1, 87.83MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import datasets \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "from os import PathLike\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('henry-williams/opcode-malberta/model-rusreqta:v1', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('mps')\n",
    "dataset = datasets.load_from_disk('../data/raw')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(artifact_dir)\n",
    "model = RobertaForSequenceClassification.from_pretrained(artifact_dir).to(device)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "dataset['test'] = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fa925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(sample):\n",
    "    input = tokenizer(\n",
    "        sample,\n",
    "        padding='max_length',\n",
    "        max_length=32,\n",
    "        return_overflowing_tokens=True,\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    input_ids = torch.tensor(input['input_ids'])\n",
    "    attention_mask = torch.tensor(input['attention_mask'])\n",
    "    full_logits = []\n",
    "\n",
    "    for ids, mask in zip(input_ids.split(BATCH_SIZE), attention_mask.split(BATCH_SIZE)):\n",
    "        torch.mps.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids.to(device), mask.to(device)).logits\n",
    "        full_logits.append(logits)\n",
    "\n",
    "    logits = torch.vstack(full_logits)\n",
    "    return logits\n",
    "\n",
    "def make_ds():\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset['test'], position=0)):\n",
    "        logits = make_preds(sample['text']).unsqueeze(0)\n",
    "        \n",
    "        predictions.append(logits)\n",
    "        actuals.append(sample['label'])\n",
    "    return predictions, actuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66275794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionsDataset(Dataset):\n",
    "    def __init__(self, preds, labels):\n",
    "        if len(preds) != len(labels):\n",
    "            raise ValueError(\"Mismatch in size between x and y\")\n",
    "\n",
    "        self.preds = preds \n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.preds[index], self.labels[index]\n",
    "\n",
    "if not os.path.exists('logits-ds.pt'):\n",
    "    predictions, actuals = make_ds()\n",
    "    d = PredictionsDataset(predictions, actuals)\n",
    "    torch.save(d, 'logits-ds.pt')\n",
    "else: \n",
    "    d = torch.load('logits-ds.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reducer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Reducer, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        z = self.fc(out[:, -1, :])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reducer(2, 32, 3)\n",
    "r = r.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(r.parameters())\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for logits, label in tqdm(d):\n",
    "    torch.mps.empty_cache()\n",
    "    actual = F.one_hot(torch.tensor(torch.tensor(label)), 2).unsqueeze(0).to(torch.float32).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    predicted = r(logits)\n",
    "    loss = criterion(predicted, actual)\n",
    "    loss_history.append(loss)\n",
    "    loss.backward() \n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(loss_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
