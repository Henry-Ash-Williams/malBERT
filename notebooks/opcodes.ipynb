{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, trainers, models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "DATA_PATH = \"/Volumes/malware-dataset/opcodes/processed-data\"\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: os.PathLike, full_path: bool = True) -> List[str]:\n",
    "    all_files = os.listdir(path)\n",
    "\n",
    "    if full_path:\n",
    "        return [\n",
    "            os.path.join(path, file)\n",
    "            for file in all_files\n",
    "            if file.endswith(\".txt\") and not file.startswith(\"._\")\n",
    "        ]\n",
    "    else:\n",
    "        return all_files\n",
    "\n",
    "\n",
    "def get_labels(filenames):\n",
    "    return [1 if \"VirusShare\" in filename else 0 for filename in filenames]\n",
    "\n",
    "\n",
    "paths = get_data(DATA_PATH)\n",
    "labels = get_labels(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeDataset(Dataset):\n",
    "    def __init__(self, paths: List[str], labels: List[str]):\n",
    "        assert len(paths) == len(labels), \"Mismatch between number of files and labels\"\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx <= len(self), \"Index out of range\"\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with open(self.paths[idx], \"r\") as file:\n",
    "            content = file.readlines()\n",
    "\n",
    "        return \" \".join([opcode.rstrip() for opcode in content]), label\n",
    "\n",
    "\n",
    "opcode_dataset = OpcodeDataset(paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "if not os.path.exists(\"../MalBERTa\"):\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.WordLevelTrainer(\n",
    "        vocab_size=1293,\n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.train(paths, trainer)\n",
    "    tokenizer.post_processor = RobertaProcessing(\n",
    "        cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        sep=(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    )\n",
    "    tokenizer.save(\"../MalBERTa/tokenizer.json\")\n",
    "\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=\"../MalBERTa/tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "    )\n",
    "    hf_tokenizer.save_pretrained(\"../MalBERTa\")\n",
    "    tokenizer = hf_tokenizer\n",
    "else:\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../MalBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Original Dataset: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5552</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1388</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Original Dataset: \u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m5552\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m1388\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Class Distribution:\n",
       "        Train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Counter</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2787</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2765</span><span style=\"font-weight: bold\">})</span>\n",
       "        Test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Counter</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">704</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">684</span><span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Class Distribution:\n",
       "        Train: \u001b[1;35mCounter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m2787\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m2765\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "        Test: \u001b[1;35mCounter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m704\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m684\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dataset_generator():\n",
    "    for text, label in tqdm(opcode_dataset):\n",
    "        yield {\"text\": text, \"label\": label}\n",
    "\n",
    "\n",
    "if not os.path.exists(\"../data/raw\"):\n",
    "    dataset = datasets.Dataset.from_generator(dataset_generator)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"../data/raw\")\n",
    "else:\n",
    "    dataset = datasets.load_from_disk(\"../data/raw\")\n",
    "print(f\"Original Dataset: {dataset}\")\n",
    "print(\n",
    "    f\"Class Distribution:\\n\\tTrain: {Counter(dataset['train']['label'])}\\n\\tTest: {Counter(dataset['test']['label'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 2627\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_generator():\n",
    "    obfuscated_benign_path = (\n",
    "        \"/Volumes/malware-dataset/obfuscated-benign/disassembly/opcode-sequences\"\n",
    "    )\n",
    "    files = [\n",
    "        os.path.join(obfuscated_benign_path, file)\n",
    "        for file in os.listdir(obfuscated_benign_path)\n",
    "        if file.endswith(\".txt\")\n",
    "    ]\n",
    "    labels = [0] * len(files)\n",
    "\n",
    "    ds = OpcodeDataset(files, labels)\n",
    "\n",
    "    for text, label in ds:\n",
    "        yield {\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "obfuscated_benign = datasets.Dataset.from_generator(new_generator)\n",
    "obfuscated_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 5518/5518 [00:06<00:00, 822.84 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 1380/1380 [00:01<00:00, 847.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New dataset: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5518</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1380</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New dataset: \u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m5518\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m, \u001b[32m'label'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m1380\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Class Distribution:\n",
       "        Train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Counter</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2773</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2745</span><span style=\"font-weight: bold\">})</span>\n",
       "        Test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Counter</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">704</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">676</span><span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Class Distribution:\n",
       "        Train: \u001b[1;35mCounter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m2773\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m2745\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "        Test: \u001b[1;35mCounter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m704\u001b[0m, \u001b[1;36m0\u001b[0m: \u001b[1;36m676\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge the two datasets\n",
    "def is_malicious(args):\n",
    "    return args[\"label\"] != 0\n",
    "\n",
    "\n",
    "merged = datasets.concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n",
    "\n",
    "new_dataset = datasets.concatenate_datasets([merged, obfuscated_benign])\n",
    "class_dist = Counter(new_dataset[\"label\"])\n",
    "mal = new_dataset.filter(is_malicious)\n",
    "ben = new_dataset.filter(lambda s: not is_malicious(s))\n",
    "\n",
    "subset_ben = ben.shuffle().select(range(min(class_dist.values())))\n",
    "new_dataset = datasets.concatenate_datasets([mal, subset_ben])\n",
    "Counter(new_dataset[\"label\"])\n",
    "\n",
    "new_dataset = new_dataset.train_test_split(test_size=0.2)\n",
    "new_dataset.save_to_disk(\"../data/partially_obfuscated_benign\")\n",
    "print(f\"New dataset: {new_dataset}\")\n",
    "print(\n",
    "    f\"Class Distribution:\\n\\tTrain: {Counter(new_dataset['train']['label'])}\\n\\tTest: {Counter(new_dataset['test']['label'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample[\"text\"]\n",
    "    labels = sample[\"label\"]\n",
    "\n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized[\"input_ids\"])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened[\"label\"].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=MAX_LENGTH + 2,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=2048,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "train_ds = processed_dataset[\"train\"].remove_columns(\"label\")\n",
    "test_ds = processed_dataset[\"test\"].remove_columns(\"label\")\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(token_ids):\n",
    "    X = data_collator(torch.tensor(token_ids[\"input_ids\"]))\n",
    "    preds = trainer.predict(X[\"input_ids\"])\n",
    "\n",
    "    Y_hat = tokenizer.batch_decode(preds.predictions.argmax(-1))\n",
    "    Y = tokenizer.batch_decode(token_ids[\"input_ids\"])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "            \"Input\": tokenizer.batch_decode(X[\"input_ids\"]),\n",
    "            \"Predicted\": Y_hat,\n",
    "            \"Actual\": Y,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data = test_ds.select(range(10))\n",
    "predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"./MalBERTa\")\n",
    "data = processed_dataset[\"train\"][0]\n",
    "\n",
    "input_ids = torch.tensor(data[\"input_ids\"]).unsqueeze(0)\n",
    "attention_mask = torch.tensor(data[\"attention_mask\"]).unsqueeze(0)\n",
    "label = torch.tensor(data[\"label\"])\n",
    "\n",
    "model(input_ids, attention_mask, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(\n",
    "            labels, predictions, average=\"weighted\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            labels, predictions, average=\"weighted\", zero_division=0\n",
    "        ),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier(processed_dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malbert-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
