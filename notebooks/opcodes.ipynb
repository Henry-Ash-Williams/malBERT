{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrywilliams/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, trainers, models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os \n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ['WANDB_MODE'] = \"offline\"\n",
    "\n",
    "DATA_PATH = \"/Volumes/New Volume/malware-detection-dataset/opcodes/processed-data\"\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: os.PathLike, full_path: bool = True) -> List[str]:\n",
    "    all_files = os.listdir(path)\n",
    "    \n",
    "    if full_path:\n",
    "        return [os.path.join(path, file) for file in all_files if file.endswith('.txt') and not file.startswith(\"._\")]\n",
    "    else: \n",
    "        return all_files\n",
    "\n",
    "def get_labels(filenames):\n",
    "    return [1 if \"VirusShare\" in filename else 0 for filename in filenames]\n",
    "\n",
    "paths = get_data(DATA_PATH)\n",
    "labels = get_labels(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeDataset(Dataset): \n",
    "    def __init__(self, paths: List[str], labels: List[str]):\n",
    "        assert len(paths) == len(labels), \"Mismatch between number of files and labels\"\n",
    "        self.paths = paths \n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx <= len(self), \"Index out of range\"\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with open(self.paths[idx], 'r') as file: \n",
    "            content = file.readlines() \n",
    "            \n",
    "        return ' '.join([opcode.rstrip() for opcode in content]), label\n",
    "\n",
    "opcode_dataset = OpcodeDataset(paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "if not os.path.exists('../MalBERTa'):\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.WordLevelTrainer(\n",
    "        vocab_size=1293, \n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ], \n",
    "    )\n",
    "    tokenizer.train(paths, trainer)\n",
    "    tokenizer.post_processor = RobertaProcessing(\n",
    "        cls=(\"<s>\", tokenizer.token_to_id('<s>')),\n",
    "        sep=(\"</s>\", tokenizer.token_to_id('</s>'))\n",
    "    )\n",
    "    tokenizer.save('../MalBERTa/tokenizer.json')\n",
    "\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=\"../MalBERTa/tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )\n",
    "    hf_tokenizer.save_pretrained(\"../MalBERTa\")\n",
    "    tokenizer = hf_tokenizer\n",
    "else: \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../MalBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    for text, label in tqdm(opcode_dataset): \n",
    "        yield {\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "if not os.path.exists('./data/raw'):\n",
    "    dataset = datasets.Dataset.from_generator(dataset_generator)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"data/raw\")\n",
    "else: \n",
    "    dataset = datasets.load_from_disk(\"./data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample['text']\n",
    "    labels = sample['label']\n",
    "    \n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized['input_ids'])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened['label'].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    max_position_embeddings=MAX_LENGTH + 2, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=2048,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_ds = processed_dataset['train'].remove_columns('label')\n",
    "test_ds = processed_dataset['test'].remove_columns('label')\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(token_ids):\n",
    "    X = data_collator(torch.tensor(token_ids['input_ids']))\n",
    "    preds = trainer.predict(X['input_ids'])\n",
    "    \n",
    "    Y_hat = tokenizer.batch_decode(preds.predictions.argmax(-1))\n",
    "    Y = tokenizer.batch_decode(token_ids['input_ids'])\n",
    "\n",
    "    df = pd.DataFrame(data={\n",
    "        \"Input\": tokenizer.batch_decode(X['input_ids']),\n",
    "        \"Predicted\": Y_hat,\n",
    "        \"Actual\": Y,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "    \n",
    "data = test_ds.select(range(10))\n",
    "predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./MalBERTa\")\n",
    "data = processed_dataset['train'][0]\n",
    "\n",
    "input_ids = torch.tensor(data['input_ids']).unsqueeze(0)\n",
    "attention_mask = torch.tensor(data['attention_mask']).unsqueeze(0)\n",
    "label = torch.tensor(data['label'])\n",
    "\n",
    "model(input_ids, attention_mask, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=256, \n",
    "    per_device_eval_batch_size=256, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier(processed_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
