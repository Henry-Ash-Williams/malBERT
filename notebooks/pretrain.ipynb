{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7632a4",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af556e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import pandas as pd \n",
    "from tqdm import TqdmWarning\n",
    "from tokenizers import ByteLevelBPETokenizer \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "import json \n",
    "import pickle\n",
    "import warnings\n",
    "import subprocess \n",
    "\n",
    "# Hide all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "\n",
    "# Set up weights & biases \n",
    "os.environ[\"WANDB_PROJECT\"] = \"malbert-hf\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "\n",
    "# Config options \n",
    "MAX_LENGTH = 10         \t    \t# Max number of tokens in an instruction\n",
    "VOCAB_SIZE = 10000      \t    \t# Number of tokens \n",
    "SUBSET_SIZE = 0.1       \t    \t# Size of dataset as a fraction of the total number of files (0 - 1)\n",
    "HIDDEN_SIZE = 768       \t    \t# Size of hidden layers \n",
    "NUM_HIDDEN = 12         \t    \t# Number of hidden layers\n",
    "NUM_ATTENTION = 12      \t    \t# Number of attention heads \n",
    "INTERMEDIATE_SIZE = 3072 \t    \t# Size of intermediate layers\n",
    "HIDDEN_ACT = \"gelu\"     \t    \t# Activation function used in the hidden layers\n",
    "HIDDEN_DROPOUT_PROB = 0.1      \t\t# Dropout probability in hidden layers\n",
    "ATTENTION_DROPOUT_PROB = 0.1      \t# Dropout probability in attention mechanisms\n",
    "INIT_RANGE = 0.02       \t    \t# Variance of initialisation \n",
    "LAYER_NORM_EPS = 1e-12  \t    \t# Epsilon value used by layernorm \n",
    "\n",
    "EPOCHS = 20                         # Number of training epochs\n",
    "\n",
    "\n",
    "# Directory containing files of disassembled executables\n",
    "DATASET_BASE = \"/Volumes/New Volume/malware-detection-dataset/opcodes/disasm\"\n",
    "\n",
    "# Evaluation samples\n",
    "EVAL_DS_PATH = \"./data.pickle\"\n",
    "\n",
    "with open(os.path.join(DATASET_BASE, 'labels.json'), 'r') as dataset_file:\n",
    "    dataset = json.load(dataset_file)\n",
    "files = [os.path.join(DATASET_BASE, name) for name in dataset.keys()]\n",
    "labels = list(dataset.values())\n",
    "\n",
    "files, _, labels, _ = train_test_split(files, labels, test_size=1 - SUBSET_SIZE)\n",
    "def get_no_lines(file):\n",
    "    return int(subprocess.run([\"wc\", \"-l\", file], capture_output=True).stdout.decode().lstrip().split(\" \")[0])\n",
    "\n",
    "# print(f\"{len(files)} files will be used in training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070739cb",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Train a new tokenizer if we haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940d844",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir MalBERT\n",
    "\n",
    "if not os.path.exists(\"./MalBERT\"):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=files, vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    tokenizer.save_model('MalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556eadc",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Set up the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212fec0",
   "metadata": {},
   "source": [
    "## Un-tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('./MalBERT', max_len=MAX_LENGTH)\n",
    "\n",
    "def tokenize_fn(line):\n",
    "    return roberta_tokenizer(line['text'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "train_files, test_files = train_test_split(files)\n",
    "raw_dataset = datasets.load_dataset('text', data_files={\n",
    "    \"train\": train_files, \n",
    "    \"test\": test_files\n",
    "})\n",
    "\n",
    "print(f\"{len(raw_dataset['train'])} lines in training dataset\")\n",
    "print(f\"{len(raw_dataset['test'])} lines in testing dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb776b8",
   "metadata": {},
   "source": [
    "## Tokenized dataset\n",
    "\n",
    "WARNING: This cell may take a long time to run depending on the size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16accd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/tokenized\"):\n",
    "    dataset = raw_dataset.map(tokenize_fn, batched=True, remove_columns=['text'], num_proc=8, batch_size=1024)\n",
    "    dataset.save_to_disk(\"data/tokenized\")\n",
    "else: \n",
    "    dataset = datasets.load_from_disk(\"data/tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9db0f8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3069d27",
   "metadata": {},
   "source": [
    "## Custom weights & biases callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd384f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases callback to log evaluation samples\n",
    "class LogPredictionsCallback(WandbCallback):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(data_path, 'rb') as data_file: \n",
    "            self.data = pickle.load(data_file)\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        device = model.device \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=torch.tensor(self.data['input_ids']).to(device),\n",
    "                attention_mask=torch.tensor(self.data['attention_mask']).to(device),\n",
    "                labels=torch.tensor(self.data['labels']).to(device)\n",
    "            )\n",
    "\n",
    "        self.data['input_ids'] = torch.tensor(self.data['input_ids']).detach()\n",
    "        self.data['attention_mask'] = torch.tensor(self.data['attention_mask']).detach()\n",
    "        self.data['labels'] = torch.tensor(self.data['labels']).detach()\n",
    "\n",
    "        mask_pos = torch.where(self.data['input_ids'] == roberta_tokenizer.mask_token_id)\n",
    "\n",
    "        input = torch.clone(self.data['input_ids'])\n",
    "        actual = torch.clone(self.data['input_ids'])\n",
    "        predicted = torch.clone(self.data['input_ids'])\n",
    "\n",
    "        actual[actual == roberta_tokenizer.mask_token_id] = self.data['labels'][mask_pos[0], mask_pos[1]]\n",
    "        predicted[predicted == roberta_tokenizer.mask_token_id] = output.logits[mask_pos[0], mask_pos[1], :].argmax(dim=-1).cpu()\n",
    "\n",
    "        x = [roberta_tokenizer.decode(xi[~torch.isin(xi, torch.tensor([0, 1, 2, 3]))]) for xi in input]\n",
    "        y = roberta_tokenizer.batch_decode(actual, skip_special_tokens=True)\n",
    "        y_hat = roberta_tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
    "\n",
    "        df = pd.DataFrame({\"Input\": x, \"Actual\": y, \"Predicted\": y_hat})\n",
    "        table = self._wandb.Table(dataframe=df)\n",
    "        self._wandb.log({\"sample\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1b3e0",
   "metadata": {},
   "source": [
    "## Model Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_position_embeddings=MAX_LENGTH, \n",
    "    num_attention_heads=NUM_ATTENTION,\n",
    "    num_hidden_layers=NUM_HIDDEN,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    intermediate_size=INTERMEDIATE_SIZE,\n",
    "    hidden_act=HIDDEN_ACT,\n",
    "    hidden_dropout_prob=HIDDEN_DROPOUT_PROB, \n",
    "    attention_probs_dropout_prob=ATTENTION_DROPOUT_PROB,\n",
    "    initializer_range=INIT_RANGE, \n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7753e0",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=roberta_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "callback = LogPredictionsCallback(EVAL_DS_PATH, roberta_tokenizer)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,  \n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset['train'], \n",
    "    eval_dataset=dataset['test']\n",
    ")\n",
    "\n",
    "trainer.add_callback(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a91f2",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./MalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795acee6",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Predictions made from the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"artifacts/run-159ev9rd-sample:v0/sample.table.json\", \"r\"))\n",
    "pd.DataFrame(columns=data['columns'], data=data['data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
