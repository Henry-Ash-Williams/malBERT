{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer, trainers, models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "from transformers import PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "import os \n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_PATH = \"/Volumes/New Volume/malware-detection-dataset/opcodes/processed-data\"\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: os.PathLike, full_path: bool = True) -> List[str]:\n",
    "    all_files = os.listdir(path)\n",
    "    \n",
    "    if full_path:\n",
    "        return [os.path.join(path, file) for file in all_files if file.endswith('.txt') and not file.startswith(\"._\")]\n",
    "    else: \n",
    "        return all_files\n",
    "\n",
    "def get_labels(filenames):\n",
    "    return [1 if \"VirusShare\" in filename else 0 for filename in filenames]\n",
    "\n",
    "paths = get_data(DATA_PATH)\n",
    "labels = get_labels(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeDataset(Dataset): \n",
    "    def __init__(self, paths, labels):\n",
    "        assert len(paths) == len(labels), \"Mismatch between number of files and labels\"\n",
    "        self.paths = paths \n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx <= len(self), \"Index out of range\"\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with open(self.paths[idx], 'r') as file: \n",
    "            content = file.readlines() \n",
    "            \n",
    "        return ' '.join([opcode.rstrip() for opcode in content]), label\n",
    "\n",
    "opcode_dataset = OpcodeDataset(paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./MalBERTa'):\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.WordLevelTrainer(\n",
    "        vocab_size=1293, \n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ], \n",
    "    )\n",
    "    tokenizer.train(paths, trainer)\n",
    "    tokenizer.save('MalBERTa/tokenizer.json')\n",
    "\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=\"MalBERTa/tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )\n",
    "    hf_tokenizer.save_pretrained(\"MalBERTa\")\n",
    "    tokenizer = hf_tokenizer\n",
    "else: \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"MalBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 5552/5552 [00:02<00:00, 1863.22 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 1388/1388 [00:00<00:00, 2638.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def dataset_generator():\n",
    "    for text, label in tqdm(opcode_dataset): \n",
    "        yield {\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "if not os.path.exists('./data/raw'):\n",
    "    dataset = datasets.Dataset.from_generator(dataset_generator)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"data/raw\")\n",
    "else: \n",
    "    dataset = datasets.load_from_disk(\"./data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(314) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(317) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(318) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(321) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(322) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(323) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(324) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(326) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Map (num_proc=8):   0%|          | 0/5552 [00:00<?, ? examples/s]Python(328) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(330) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(332) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(333) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(334) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(337) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(338) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Map (num_proc=8):   0%|          | 0/5552 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m             flattened[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(flattened)\n\u001b[0;32m---> 24\u001b[0m processed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/datasets/dataset_dict.py:941\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    939\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 941\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    962\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m~/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3166\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3160\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3162\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3163\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3164\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3165\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3166\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3167\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3168\u001b[0m     ):\n\u001b[1;32m   3169\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3170\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/programming/python/ai/malbert-test/.venv/lib/python3.10/site-packages/datasets/utils/py_utils.py:713\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    711\u001b[0m             pool_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    712\u001b[0m             \u001b[38;5;66;03m# One of the subprocesses has died. We should not wait forever.\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the subprocesses has abruptly died during map operation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo debug the error, disable multiprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m             )\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing."
     ]
    }
   ],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample['text']\n",
    "    labels = sample['label']\n",
    "    \n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized['input_ids'])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened['label'].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    max_position_embeddings=MAX_LENGTH + 2, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=2048,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_ds = processed_dataset['train'].remove_columns('label')\n",
    "test_ds = processed_dataset['test'].remove_columns('label')\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>push mov in sub in add nop mov inc or push mov...</td>\n",
       "      <td>maskmovdqu vprolvd vmovlps vpternlogq pfpnacc ...</td>\n",
       "      <td>push mov in sub in add nop mov inc or push mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add lea inc in push mov add push lea inc hlt p...</td>\n",
       "      <td>movddup fsubp fsubp phsubd vandnps wrmsr movdd...</td>\n",
       "      <td>add lea inc in push mov add push lea inc hlt p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jmp and inc and inc add jmp and dec and inc ad...</td>\n",
       "      <td>vpternlogq fsubp fsubp fsubp psllw korw vpmadc...</td>\n",
       "      <td>jmp and inc and inc add jmp and dec and inc ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; &lt;mask&gt; add add lea mov &lt;mask&gt; &lt;m...</td>\n",
       "      <td>xtest vcmpltps fsubp vunpcklpd vfmadd213pd fsu...</td>\n",
       "      <td>ret lea mov add add lea mov add add nop sub in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mov add inc &lt;mask&gt; push pshufhw add &lt;mask&gt; mov...</td>\n",
       "      <td>movabs lwpval fsubp vpternlogq vandnps vcmpneq...</td>\n",
       "      <td>mov add inc add push inc add lea mov add inc a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;mask&gt; cmp cmp sbb jne inc add add &lt;mask&gt; dec ...</td>\n",
       "      <td>xtest vcmpss fsubp fsubp vorps fsetpm movddup ...</td>\n",
       "      <td>add cmp cmp sbb jne inc add add je dec xor rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pavgusb test &lt;mask&gt; setne xchg &lt;mask&gt; jmp jae ...</td>\n",
       "      <td>xtest cmpltsd fsubp vpternlogq vandnps wrmsr v...</td>\n",
       "      <td>rol test ror setne xchg shr jmp jae dec lea jb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in sub &lt;mask&gt; add rol inc add &lt;mask&gt; &lt;mask&gt; an...</td>\n",
       "      <td>movdqu fsubp fsubp vunpcklpd vcvttps2qq wrmsr ...</td>\n",
       "      <td>in sub mov add rol inc add mov inc and adc or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and add and in xor sar pextrd jno cld push mov...</td>\n",
       "      <td>maskmovdqu vcmpltps fsubp vpternlogq vandnps v...</td>\n",
       "      <td>and add and in xor sar push jno cld push mov i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp je test sbb add add &lt;mask&gt; add and call ad...</td>\n",
       "      <td>movabs fsubp vcmpunordsd fsubp vandnps korw mu...</td>\n",
       "      <td>cmp je test sbb add add mov add and call add a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input  \\\n",
       "0  push mov in sub in add nop mov inc or push mov...   \n",
       "1  add lea inc in push mov add push lea inc hlt p...   \n",
       "2  jmp and inc and inc add jmp and dec and inc ad...   \n",
       "3  <mask> <mask> <mask> add add lea mov <mask> <m...   \n",
       "4  mov add inc <mask> push pshufhw add <mask> mov...   \n",
       "5  <mask> cmp cmp sbb jne inc add add <mask> dec ...   \n",
       "6  pavgusb test <mask> setne xchg <mask> jmp jae ...   \n",
       "7  in sub <mask> add rol inc add <mask> <mask> an...   \n",
       "8  and add and in xor sar pextrd jno cld push mov...   \n",
       "9  cmp je test sbb add add <mask> add and call ad...   \n",
       "\n",
       "                                           Predicted  \\\n",
       "0  maskmovdqu vprolvd vmovlps vpternlogq pfpnacc ...   \n",
       "1  movddup fsubp fsubp phsubd vandnps wrmsr movdd...   \n",
       "2  vpternlogq fsubp fsubp fsubp psllw korw vpmadc...   \n",
       "3  xtest vcmpltps fsubp vunpcklpd vfmadd213pd fsu...   \n",
       "4  movabs lwpval fsubp vpternlogq vandnps vcmpneq...   \n",
       "5  xtest vcmpss fsubp fsubp vorps fsetpm movddup ...   \n",
       "6  xtest cmpltsd fsubp vpternlogq vandnps wrmsr v...   \n",
       "7  movdqu fsubp fsubp vunpcklpd vcvttps2qq wrmsr ...   \n",
       "8  maskmovdqu vcmpltps fsubp vpternlogq vandnps v...   \n",
       "9  movabs fsubp vcmpunordsd fsubp vandnps korw mu...   \n",
       "\n",
       "                                              Actual  \n",
       "0  push mov in sub in add nop mov inc or push mov...  \n",
       "1  add lea inc in push mov add push lea inc hlt p...  \n",
       "2  jmp and inc and inc add jmp and dec and inc ad...  \n",
       "3  ret lea mov add add lea mov add add nop sub in...  \n",
       "4  mov add inc add push inc add lea mov add inc a...  \n",
       "5  add cmp cmp sbb jne inc add add je dec xor rol...  \n",
       "6  rol test ror setne xchg shr jmp jae dec lea jb...  \n",
       "7  in sub mov add rol inc add mov inc and adc or ...  \n",
       "8  and add and in xor sar push jno cld push mov i...  \n",
       "9  cmp je test sbb add add mov add and call add a...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict(token_ids):\n",
    "    X = data_collator(torch.tensor(token_ids['input_ids']))\n",
    "    preds = trainer.predict(X['input_ids'])\n",
    "    \n",
    "    Y_hat = tokenizer.batch_decode(preds.predictions.argmax(-1))\n",
    "    Y = tokenizer.batch_decode(token_ids['input_ids'])\n",
    "\n",
    "    df = pd.DataFrame(data={\n",
    "        \"Input\": tokenizer.batch_decode(X['input_ids']),\n",
    "        \"Predicted\": Y_hat,\n",
    "        \"Actual\": Y,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "    \n",
    "data = test_ds.select(range(10))\n",
    "predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
