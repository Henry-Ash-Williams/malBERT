{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer, trainers, models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "from transformers import PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "import os \n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_PATH = \"/Volumes/New Volume/malware-detection-dataset/opcodes/processed-data\"\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: os.PathLike, full_path: bool = True) -> List[str]:\n",
    "    all_files = os.listdir(path)\n",
    "    \n",
    "    if full_path:\n",
    "        return [os.path.join(path, file) for file in all_files if file.endswith('.txt') and not file.startswith(\"._\")]\n",
    "    else: \n",
    "        return all_files\n",
    "\n",
    "def get_labels(filenames):\n",
    "    return [1 if \"VirusShare\" in filename else 0 for filename in filenames]\n",
    "\n",
    "paths = get_data(DATA_PATH)\n",
    "labels = get_labels(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeDataset(Dataset): \n",
    "    def __init__(self, paths, labels):\n",
    "        assert len(paths) == len(labels), \"Mismatch between number of files and labels\"\n",
    "        self.paths = paths \n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx <= len(self), \"Index out of range\"\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with open(self.paths[idx], 'r') as file: \n",
    "            content = file.readlines() \n",
    "            \n",
    "        return ' '.join([opcode.rstrip() for opcode in content]), label\n",
    "\n",
    "opcode_dataset = OpcodeDataset(paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./MalBERTa'):\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.WordLevelTrainer(\n",
    "        vocab_size=1293, \n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ], \n",
    "    )\n",
    "    tokenizer.train(paths, trainer)\n",
    "    tokenizer.save('MalBERTa/tokenizer.json')\n",
    "\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=\"MalBERTa/tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )\n",
    "    hf_tokenizer.save_pretrained(\"MalBERTa\")\n",
    "    tokenizer = hf_tokenizer\n",
    "else: \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"MalBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    for text, label in tqdm(opcode_dataset): \n",
    "        yield {\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        }\n",
    "if not os.path.exists('./data/raw'):\n",
    "    dataset = datasets.Dataset.from_generator(dataset_generator)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"data/raw\")\n",
    "else: \n",
    "    dataset = datasets.load_from_disk(\"./data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 5552/5552 [03:59<00:00, 23.21 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 1388/1388 [00:54<00:00, 25.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample['text']\n",
    "    labels = sample['label']\n",
    "    \n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized['input_ids'])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened['label'].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    max_position_embeddings=MAX_LENGTH + 2, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=2048,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_ds = processed_dataset['train'].remove_columns('label')\n",
    "test_ds = processed_dataset['test'].remove_columns('label')\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>push mov in sub in add nop mov inc or push mov...</td>\n",
       "      <td>maskmovdqu vprolvd vmovlps vpternlogq pfpnacc ...</td>\n",
       "      <td>push mov in sub in add nop mov inc or push mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add lea inc in push mov add push lea inc hlt p...</td>\n",
       "      <td>movddup fsubp fsubp phsubd vandnps wrmsr movdd...</td>\n",
       "      <td>add lea inc in push mov add push lea inc hlt p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jmp and inc and inc add jmp and dec and inc ad...</td>\n",
       "      <td>vpternlogq fsubp fsubp fsubp psllw korw vpmadc...</td>\n",
       "      <td>jmp and inc and inc add jmp and dec and inc ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; &lt;mask&gt; add add lea mov &lt;mask&gt; &lt;m...</td>\n",
       "      <td>xtest vcmpltps fsubp vunpcklpd vfmadd213pd fsu...</td>\n",
       "      <td>ret lea mov add add lea mov add add nop sub in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mov add inc &lt;mask&gt; push pshufhw add &lt;mask&gt; mov...</td>\n",
       "      <td>movabs lwpval fsubp vpternlogq vandnps vcmpneq...</td>\n",
       "      <td>mov add inc add push inc add lea mov add inc a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;mask&gt; cmp cmp sbb jne inc add add &lt;mask&gt; dec ...</td>\n",
       "      <td>xtest vcmpss fsubp fsubp vorps fsetpm movddup ...</td>\n",
       "      <td>add cmp cmp sbb jne inc add add je dec xor rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pavgusb test &lt;mask&gt; setne xchg &lt;mask&gt; jmp jae ...</td>\n",
       "      <td>xtest cmpltsd fsubp vpternlogq vandnps wrmsr v...</td>\n",
       "      <td>rol test ror setne xchg shr jmp jae dec lea jb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in sub &lt;mask&gt; add rol inc add &lt;mask&gt; &lt;mask&gt; an...</td>\n",
       "      <td>movdqu fsubp fsubp vunpcklpd vcvttps2qq wrmsr ...</td>\n",
       "      <td>in sub mov add rol inc add mov inc and adc or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and add and in xor sar pextrd jno cld push mov...</td>\n",
       "      <td>maskmovdqu vcmpltps fsubp vpternlogq vandnps v...</td>\n",
       "      <td>and add and in xor sar push jno cld push mov i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp je test sbb add add &lt;mask&gt; add and call ad...</td>\n",
       "      <td>movabs fsubp vcmpunordsd fsubp vandnps korw mu...</td>\n",
       "      <td>cmp je test sbb add add mov add and call add a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input  \\\n",
       "0  push mov in sub in add nop mov inc or push mov...   \n",
       "1  add lea inc in push mov add push lea inc hlt p...   \n",
       "2  jmp and inc and inc add jmp and dec and inc ad...   \n",
       "3  <mask> <mask> <mask> add add lea mov <mask> <m...   \n",
       "4  mov add inc <mask> push pshufhw add <mask> mov...   \n",
       "5  <mask> cmp cmp sbb jne inc add add <mask> dec ...   \n",
       "6  pavgusb test <mask> setne xchg <mask> jmp jae ...   \n",
       "7  in sub <mask> add rol inc add <mask> <mask> an...   \n",
       "8  and add and in xor sar pextrd jno cld push mov...   \n",
       "9  cmp je test sbb add add <mask> add and call ad...   \n",
       "\n",
       "                                           Predicted  \\\n",
       "0  maskmovdqu vprolvd vmovlps vpternlogq pfpnacc ...   \n",
       "1  movddup fsubp fsubp phsubd vandnps wrmsr movdd...   \n",
       "2  vpternlogq fsubp fsubp fsubp psllw korw vpmadc...   \n",
       "3  xtest vcmpltps fsubp vunpcklpd vfmadd213pd fsu...   \n",
       "4  movabs lwpval fsubp vpternlogq vandnps vcmpneq...   \n",
       "5  xtest vcmpss fsubp fsubp vorps fsetpm movddup ...   \n",
       "6  xtest cmpltsd fsubp vpternlogq vandnps wrmsr v...   \n",
       "7  movdqu fsubp fsubp vunpcklpd vcvttps2qq wrmsr ...   \n",
       "8  maskmovdqu vcmpltps fsubp vpternlogq vandnps v...   \n",
       "9  movabs fsubp vcmpunordsd fsubp vandnps korw mu...   \n",
       "\n",
       "                                              Actual  \n",
       "0  push mov in sub in add nop mov inc or push mov...  \n",
       "1  add lea inc in push mov add push lea inc hlt p...  \n",
       "2  jmp and inc and inc add jmp and dec and inc ad...  \n",
       "3  ret lea mov add add lea mov add add nop sub in...  \n",
       "4  mov add inc add push inc add lea mov add inc a...  \n",
       "5  add cmp cmp sbb jne inc add add je dec xor rol...  \n",
       "6  rol test ror setne xchg shr jmp jae dec lea jb...  \n",
       "7  in sub mov add rol inc add mov inc and adc or ...  \n",
       "8  and add and in xor sar push jno cld push mov i...  \n",
       "9  cmp je test sbb add add mov add and call add a...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict(token_ids):\n",
    "    X = data_collator(torch.tensor(token_ids['input_ids']))\n",
    "    preds = trainer.predict(X['input_ids'])\n",
    "    \n",
    "    Y_hat = tokenizer.batch_decode(preds.predictions.argmax(-1))\n",
    "    Y = tokenizer.batch_decode(token_ids['input_ids'])\n",
    "\n",
    "    df = pd.DataFrame(data={\n",
    "        \"Input\": tokenizer.batch_decode(X['input_ids']),\n",
    "        \"Predicted\": Y_hat,\n",
    "        \"Actual\": Y,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "    \n",
    "data = test_ds.select(range(10))\n",
    "predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
