{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer, trainers, models\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import RobertaForMaskedLM, RobertaConfig\n",
    "from transformers import PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "import os \n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6,7\"\n",
    "\n",
    "DATA_PATH = \"/Volumes/New Volume/malware-detection-dataset/opcodes/processed-data\"\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: os.PathLike, full_path: bool = True) -> List[str]:\n",
    "    all_files = os.listdir(path)\n",
    "    \n",
    "    if full_path:\n",
    "        return [os.path.join(path, file) for file in all_files if file.endswith('.txt') and not file.startswith(\"._\")]\n",
    "    else: \n",
    "        return all_files\n",
    "\n",
    "def get_labels(filenames):\n",
    "    return [1 if \"VirusShare\" in filename else 0 for filename in filenames]\n",
    "\n",
    "# paths = get_data(DATA_PATH)\n",
    "# labels = get_labels(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeDataset(Dataset): \n",
    "    def __init__(self, paths, labels):\n",
    "        assert len(paths) == len(labels), \"Mismatch between number of files and labels\"\n",
    "        self.paths = paths \n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx <= len(self), \"Index out of range\"\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        with open(self.paths[idx], 'r') as file: \n",
    "            content = file.readlines() \n",
    "            \n",
    "        return ' '.join([opcode.rstrip() for opcode in content]), label\n",
    "\n",
    "# opcode_dataset = OpcodeDataset(paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./MalBERTa'):\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.WordLevelTrainer(\n",
    "        vocab_size=1293, \n",
    "        special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\",\n",
    "        ], \n",
    "    )\n",
    "    tokenizer.train(paths, trainer)\n",
    "    tokenizer.save('MalBERTa/tokenizer.json')\n",
    "\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=\"MalBERTa/tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )\n",
    "    hf_tokenizer.save_pretrained(\"MalBERTa\")\n",
    "    tokenizer = hf_tokenizer\n",
    "else: \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"MalBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 5552/5552 [00:02<00:00, 1863.22 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 1388/1388 [00:00<00:00, 2638.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def dataset_generator():\n",
    "    for text, label in tqdm(opcode_dataset): \n",
    "        yield {\n",
    "            \"text\": text,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "if not os.path.exists('./data/raw'):\n",
    "    dataset = datasets.Dataset.from_generator(dataset_generator)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"data/raw\")\n",
    "else: \n",
    "    dataset = datasets.load_from_disk(\"./data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sample(sample):\n",
    "    texts = sample['text']\n",
    "    labels = sample['label']\n",
    "    \n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        for i in range(len(tokenized['input_ids'])):\n",
    "            for k in tokenized:\n",
    "                flattened[k].append(tokenized[k][i])\n",
    "            flattened['label'].append(label)\n",
    "\n",
    "    return dict(flattened)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    handle_sample,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    batch_size=64,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    max_position_embeddings=MAX_LENGTH + 2, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=2048,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "train_ds = processed_dataset['train'].remove_columns('label')\n",
    "test_ds = processed_dataset['test'].remove_columns('label')\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict(token_ids):\n",
    "    X = data_collator(torch.tensor(token_ids['input_ids']))\n",
    "    preds = trainer.predict(X['input_ids'])\n",
    "    \n",
    "    Y_hat = tokenizer.batch_decode(preds.predictions.argmax(-1))\n",
    "    Y = tokenizer.batch_decode(token_ids['input_ids'])\n",
    "\n",
    "    df = pd.DataFrame(data={\n",
    "        \"Input\": tokenizer.batch_decode(X['input_ids']),\n",
    "        \"Predicted\": Y_hat,\n",
    "        \"Actual\": Y,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "    \n",
    "data = test_ds.select(range(10))\n",
    "predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MalBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6620, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0293,  0.0339]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./MalBERTa\")\n",
    "data = processed_dataset['train'][0]\n",
    "\n",
    "input_ids = torch.tensor(data['input_ids']).unsqueeze(0)\n",
    "attention_mask = torch.tensor(data['attention_mask']).unsqueeze(0)\n",
    "label = torch.tensor(data['label'])\n",
    "\n",
    "model(input_ids, attention_mask, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score(labels, predictions),\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     11\u001b[0m     }\n\u001b[0;32m---> 13\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[1;32m     14\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./MalBERTa-classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     18\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     19\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m, \n\u001b[1;32m     20\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     21\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     22\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_args, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERTa-classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=256, \n",
    "    per_device_eval_batch_size=256, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier(processed_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
