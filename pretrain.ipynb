{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7632a4",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af556e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 files will be used in training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import pandas as pd \n",
    "from tqdm import TqdmWarning\n",
    "from tokenizers import ByteLevelBPETokenizer \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "import json \n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# Hide all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "\n",
    "# Set up weights & biases \n",
    "os.environ[\"WANDB_PROJECT\"] = \"malbert-hf\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "\n",
    "# Config options \n",
    "MAX_LENGTH = 10         # Max number of tokens in an instruction\n",
    "VOCAB_SIZE = 10000      # Number of tokens \n",
    "SUBSET_SIZE = 1e-3      # Size of dataset as a fraction of the total number of files (0 - 1)\n",
    "\n",
    "# Directory containing files of disassembled executables\n",
    "DATASET_BASE = \"/Volumes/New Volume/malware-detection-dataset/opcodes/disasm\"\n",
    "\n",
    "# Evaluation samples\n",
    "EVAL_DS_PATH = \"./data.pickle\"\n",
    "\n",
    "\n",
    "with open(os.path.join(DATASET_BASE, 'labels.json'), 'r') as dataset_file:\n",
    "    dataset = json.load(dataset_file)\n",
    "files = [os.path.join(DATASET_BASE, name) for name in dataset.keys()]\n",
    "\n",
    "files, _ = train_test_split(files, test_size=1 - SUBSET_SIZE)\n",
    "print(f\"{len(files)} files will be used in training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070739cb",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Train a new tokenizer if we haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940d844",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir MalBERT\n",
    "\n",
    "if not os.path.exists(\"./MalBERT\"):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=files, vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    tokenizer.save_model('MalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556eadc",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Set up the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212fec0",
   "metadata": {},
   "source": [
    "## Un-tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdec662b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfdef1adc5540818d908a48c9e2a271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b240d6a97884e5e9381c4f0ba969102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630086 lines in training dataset\n",
      "129868 lines in testing dataset\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('./MalBERT', max_len=MAX_LENGTH)\n",
    "\n",
    "def tokenize_fn(line):\n",
    "    return roberta_tokenizer(line['text'], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "train_files, test_files = train_test_split(files)\n",
    "\n",
    "raw_dataset = datasets.load_dataset('text', data_files={\n",
    "    \"train\": train_files, \n",
    "    \"test\": test_files\n",
    "})\n",
    "\n",
    "print(f\"{len(raw_dataset['train'])} lines in training dataset\")\n",
    "print(f\"{len(raw_dataset['test'])} lines in testing dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb776b8",
   "metadata": {},
   "source": [
    "## Tokenized dataset\n",
    "\n",
    "WARNING: This cell may take a long time to run depending on the size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16accd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6161223a8f4519aeb55c417ece4cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/630086 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6051e519dd438c89591a1590999c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/129868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27dfbf7caeb44bc936d61c3ecce45db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/630086 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb4643e12c04d4e8a0aa0f0fa1c0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/129868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = raw_dataset.map(tokenize_fn, batched=True, remove_columns=['text'], num_proc=8, batch_size=1024)\n",
    "dataset.save_to_disk(\"data/tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9db0f8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3069d27",
   "metadata": {},
   "source": [
    "## Custom weights & biases callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd384f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biases callback to log evaluation samples\n",
    "class LogPredictionsCallback(WandbCallback):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(data_path, 'rb') as data_file: \n",
    "            self.data = pickle.load(data_file)\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs['model']\n",
    "        device = model.device \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=torch.tensor(self.data['input_ids']).to(device),\n",
    "                attention_mask=torch.tensor(self.data['attention_mask']).to(device),\n",
    "                labels=torch.tensor(self.data['labels']).to(device)\n",
    "            )\n",
    "\n",
    "        self.data['input_ids'] = torch.tensor(self.data['input_ids']).detach()\n",
    "        self.data['attention_mask'] = torch.tensor(self.data['attention_mask']).detach()\n",
    "        self.data['labels'] = torch.tensor(self.data['labels']).detach()\n",
    "\n",
    "        mask_pos = torch.where(self.data['input_ids'] == roberta_tokenizer.mask_token_id)\n",
    "\n",
    "        input = torch.clone(self.data['input_ids'])\n",
    "        actual = torch.clone(self.data['input_ids'])\n",
    "        predicted = torch.clone(self.data['input_ids'])\n",
    "\n",
    "        actual[actual == roberta_tokenizer.mask_token_id] = self.data['labels'][mask_pos[0], mask_pos[1]]\n",
    "        predicted[predicted == roberta_tokenizer.mask_token_id] = output.logits[mask_pos[0], mask_pos[1], :].argmax(dim=-1).cpu()\n",
    "\n",
    "        x = [roberta_tokenizer.decode(xi[~torch.isin(xi, torch.tensor([0, 1, 2, 3]))]) for xi in input]\n",
    "        y = roberta_tokenizer.batch_decode(actual, skip_special_tokens=True)\n",
    "        y_hat = roberta_tokenizer.batch_decode(predicted, skip_special_tokens=True)\n",
    "\n",
    "        df = pd.DataFrame({\"Input\": x, \"Actual\": y, \"Predicted\": y_hat})\n",
    "        table = self._wandb.Table(dataframe=df)\n",
    "        self._wandb.log({\"sample\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1b3e0",
   "metadata": {},
   "source": [
    "## Model Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0811baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_position_embeddings=MAX_LENGTH, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=3,\n",
    "    type_vocab_size=1\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7753e0",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10fd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=roberta_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "callback = LogPredictionsCallback(EVAL_DS_PATH, roberta_tokenizer)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./MalBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,  \n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args, \n",
    "    processing_class=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset['train'], \n",
    "    eval_dataset=dataset['test']\n",
    ")\n",
    "\n",
    "trainer.add_callback(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a91f2",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7a071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlainon\u001b[0m (\u001b[33mhenry-williams\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/henrywilliams/Documents/programming/python/ai/malbert-test/wandb/run-20250417_104559-159ev9rd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/henry-williams/malbert-hf/runs/159ev9rd' target=\"_blank\">./MalBERT</a></strong> to <a href='https://wandb.ai/henry-williams/malbert-hf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/henry-williams/malbert-hf' target=\"_blank\">https://wandb.ai/henry-williams/malbert-hf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/henry-williams/malbert-hf/runs/159ev9rd' target=\"_blank\">https://wandb.ai/henry-williams/malbert-hf/runs/159ev9rd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9846' max='9846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9846/9846 11:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.344000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./MalBERT/checkpoint-9846)... Done. 1.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./MalBERT/checkpoint-9846)... Done. 1.2s\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./MalBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795acee6",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Predictions made from the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e3d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lea edx, [rip&lt;mask&gt; 0x</td>\n",
       "      <td>lea edx, [rip + 0x</td>\n",
       "      <td>lea edx, [rip + 0x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mov word gs:[eax - 0&lt;mask&gt;</td>\n",
       "      <td>mov word gs:[eax - 0x</td>\n",
       "      <td>mov word gs:[eax - 0x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jo 0x&lt;mask&gt;1113</td>\n",
       "      <td>jo 0x140051113</td>\n",
       "      <td>jo 0x140001113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;mask&gt;</td>\n",
       "      <td>das</td>\n",
       "      <td>iretd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;mask&gt; edx, dword [rsp + 0</td>\n",
       "      <td>mov edx, dword [rsp + 0</td>\n",
       "      <td>mov edx, dword [rsp + 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mov qword [rsp063 0x&lt;mask&gt;</td>\n",
       "      <td>mov qword [rsp063 0x10</td>\n",
       "      <td>mov qword [rsp063 0x10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xor al, 0xf&lt;mask&gt;</td>\n",
       "      <td>xor al, 0xf7</td>\n",
       "      <td>xor al, 0xf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jmp qword [rsi +&lt;mask&gt;x3</td>\n",
       "      <td>jmp qword [rsi + 0x3</td>\n",
       "      <td>jmp qword [rsi + 0x3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fld dword [edi&lt;mask&gt; 0x1</td>\n",
       "      <td>fld dword [edi - 0x1</td>\n",
       "      <td>fld dword [edi - 0x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mov rcx, qword [rdi&lt;mask&gt; 8</td>\n",
       "      <td>mov rcx, qword [rdi + 8</td>\n",
       "      <td>mov rcx, qword [rdi + 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Input                   Actual  \\\n",
       "0       lea edx, [rip<mask> 0x       lea edx, [rip + 0x   \n",
       "1   mov word gs:[eax - 0<mask>    mov word gs:[eax - 0x   \n",
       "2              jo 0x<mask>1113           jo 0x140051113   \n",
       "3                       <mask>                      das   \n",
       "4   <mask> edx, dword [rsp + 0  mov edx, dword [rsp + 0   \n",
       "5   mov qword [rsp063 0x<mask>   mov qword [rsp063 0x10   \n",
       "6            xor al, 0xf<mask>             xor al, 0xf7   \n",
       "7     jmp qword [rsi +<mask>x3     jmp qword [rsi + 0x3   \n",
       "8     fld dword [edi<mask> 0x1     fld dword [edi - 0x1   \n",
       "9  mov rcx, qword [rdi<mask> 8  mov rcx, qword [rdi + 8   \n",
       "\n",
       "                 Predicted  \n",
       "0       lea edx, [rip + 0x  \n",
       "1    mov word gs:[eax - 0x  \n",
       "2           jo 0x140001113  \n",
       "3                    iretd  \n",
       "4  mov edx, dword [rsp + 0  \n",
       "5   mov qword [rsp063 0x10  \n",
       "6             xor al, 0xf8  \n",
       "7     jmp qword [rsi + 0x3  \n",
       "8     fld dword [edi - 0x1  \n",
       "9  mov rcx, qword [rdi + 8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"artifacts/run-159ev9rd-sample:v0/sample.table.json\", \"r\"))\n",
    "pd.DataFrame(columns=data['columns'], data=data['data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
